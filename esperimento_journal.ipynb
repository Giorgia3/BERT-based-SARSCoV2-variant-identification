{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giorgia3/BERT-based-SARSCoV2-variant-identification/blob/main/esperimento_journal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuPjc4y1VLP0"
      },
      "source": [
        "# Transformer su dati covid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhhXm5zd_avO"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QjPWoJ1PLhaP"
      },
      "outputs": [],
      "source": [
        "do_training = False\n",
        "do_test = False\n",
        "do_finetuning = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07T2J6IkmupT",
        "outputId": "bab4aba9-6d3e-497b-80b4-5ffcd600720b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W-W3s-dhqf2r",
        "outputId": "a025ac9e-4131-4a86-de01-486a8e271285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.30.2\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.2)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.30.2)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.30.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n",
            "Collecting pysam\n",
            "  Downloading pysam-0.21.0-cp310-cp310-manylinux_2_24_x86_64.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from pysam) (3.0.2)\n",
            "Installing collected packages: pysam\n",
            "Successfully installed pysam-0.21.0\n",
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting numpy==1.23\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "Successfully installed numpy-1.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformers==4.30.2\n",
        "!pip install -q tf-models-official==2.4.0\n",
        "!pip install biopython\n",
        "!pip install pysam\n",
        "!pip install pyyaml==5.4.1\n",
        "!pip install numpy==1.23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UySHGE2zVAxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435ddb54-d354-4bf5-9931-19ec811d0751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import InputExample, InputFeatures, TrainingArguments, Trainer\n",
        "import os.path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import csv\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "from numpy.linalg import matrix_rank\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from enum import Enum\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch import nn\n",
        "import torch.nn.functional\n",
        "import json\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as KB\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from official import nlp\n",
        "import official.nlp.optimization\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap, TABLEAU_COLORS\n",
        "from matplotlib.ticker import FuncFormatter, PercentFormatter\n",
        "%matplotlib inline\n",
        "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
        "# from mpl_toolkits.axes_grid1.colorbar import colorbar\n",
        "from matplotlib import colorbar\n",
        "import matplotlib.cm as cm\n",
        "from cycler import cycler\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from linecache import getline\n",
        "from pathlib import Path\n",
        "import math\n",
        "import gc\n",
        "import json\n",
        "import subprocess\n",
        "import shlex\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from Bio import SeqIO\n",
        "import pysam\n",
        "import pickle\n",
        "from statsmodels.stats.proportion import proportion_confint, proportions_ztest\n",
        "import collections\n",
        "from scipy.stats import chisquare\n",
        "import scipy.stats as stats\n",
        "import plotly.express as px\n",
        "import linecache\n",
        "from os import write\n",
        "from tabulate import tabulate\n",
        "import networkx as nx\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g_-8rTf9XCD",
        "outputId": "8ba4b9b1-e458-4d57-c32a-e108c2ff0297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7e2p3qp_nHV"
      },
      "source": [
        "#### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7yxO55QurbZY"
      },
      "outputs": [],
      "source": [
        "config_dict = {}\n",
        "config_dict['USE_GPU'] = True\n",
        "# USE_TPU = False\n",
        "\n",
        "config_dict['TASK_TYPE'] = 'cone_index_sing_vals_histograms'\n",
        "# options:{'violin_plots',\n",
        "            # 'singularvalues_ratio_analysis',\n",
        "            # 'attention_flow',\n",
        "            # 'simple_test',\n",
        "            # 'attention_analysis',\n",
        "            # 'clustering',\n",
        "            # 'one_vs_all_classification',\n",
        "            # 'eigenvalues_analysis',\n",
        "            # 'von_neumann_entropy_attentions',\n",
        "            # 'distance_cones_analysis',\n",
        "            # 'shannon_entropy_attentions',\n",
        "            # 'cone_index_sing_vals_histograms',}\n",
        "\n",
        "config_dict['SPIKE_REGION_ANALYSIS'] = True\n",
        "\n",
        "config_dict['REDUCE_N_INPUT_SAMPLES'] = True\n",
        "config_dict['MAX_N_SAMPLES_PER_CLASS'] = 40000\n",
        "\n",
        "config_dict['K'] = 12\n",
        "config_dict['STRIDE'] = 9\n",
        "config_dict['ADD_KMER_TOKENS_TO_VOCAB'] = True\n",
        "config_dict['MIN_N_OCCUR_KMER'] = None\n",
        "\n",
        "config_dict['ALIGNMENT'] = True\n",
        "config_dict['SPLIT_DATA_IN_CHUNKS'] = False\n",
        "if config_dict['SPLIT_DATA_IN_CHUNKS'] or config_dict['ALIGNMENT']:\n",
        "    config_dict['CHUNK_LEN'] = 150\n",
        "    config_dict['CHUNK_STRIDE'] = 150\n",
        "\n",
        "config_dict['TRAIN_BATCH_SIZE'] = 4    # recommended: 32\n",
        "config_dict['EVAL_BATCH_SIZE'] = 4    # recommended: 32\n",
        "config_dict['EPOCHS'] = 2              # recommended: 2-4\n",
        "config_dict['LR'] = 2e-5            #2e-5, #1e-3 # 2e-4\n",
        "\n",
        "# maximum n. of tokens to be considered for each sequence: (CHUNK_LEN-K)/STRIDE +1 (max value supported by Bert-Base: 512)\n",
        "if config_dict['SPLIT_DATA_IN_CHUNKS']:\n",
        "    config_dict['MAX_LENGTH'] = math.ceil((config_dict['CHUNK_LEN']-config_dict['K'])/config_dict['STRIDE'] +1)\n",
        "else:\n",
        "    config_dict['MAX_LENGTH'] = 512\n",
        "\n",
        "# attention threshold\n",
        "config_dict['THETA'] = 0.3\n",
        "\n",
        "# MLP for OvsAC\n",
        "config_dict['POSITIVE_CLASS_MLP'] = 'mu'\n",
        "config_dict['RELEVANT_TOKENS_MLP'] = ['211-214']\n",
        "config_dict['INPUT_DIM_MLP'] = len(config_dict['RELEVANT_TOKENS_MLP'])*768\n",
        "config_dict['OUTPUT_DIM_MLP'] = 2\n",
        "config_dict['TRAIN_BATCH_SIZE_MLP'] = 64\n",
        "config_dict['EVAL_BATCH_SIZE_MLP'] = 64\n",
        "config_dict['EPOCHS_MLP'] = 5\n",
        "config_dict['HIDDEN_UNITS_1_MLP'] = 32              # recommended: 2-4\n",
        "config_dict['HIDDEN_UNITS_2_MLP'] = 32              # recommended: 2-4\n",
        "config_dict['LR_MLP'] = 2e-5            #2e-5, #1e-3 # 2e-4\n",
        "\n",
        "# WEA\n",
        "all_layers_heads = []\n",
        "for head in range(0,12):\n",
        "    for layer in range(0,12):\n",
        "        all_layers_heads.append([layer,head])\n",
        "if config_dict['TASK_TYPE'] == 'distance_cones_analysis' :\n",
        "    all_layers_heads1 = []\n",
        "    for head in range(0,12):\n",
        "        for layer in range(0,12):\n",
        "            all_layers_heads1.append([layer,head])\n",
        "    dc_layer_heads = all_layers_heads1\n",
        "    # dc_layer_heads = []\n",
        "    # for head in range(0,12):\n",
        "    #     dc_layer_heads.append([1-1, head])\n",
        "    config_dict['SELECTED_LAYER_HEAD_LIST'] = dc_layer_heads # distance cones:[[1-1,5-1]]\n",
        "else:\n",
        "    config_dict['SELECTED_LAYER_HEAD_LIST'] = all_layers_heads #[[1-1,4-1], [12-1,9-1], [11-1, 5-1], [1-1, 5-1], [5-1, 10-1]] #all_layers_heads\n",
        "config_dict['SELECTED_CLASS'] = \"omicron\"\n",
        "config_dict['BASES'] = ['A','C','G','T','N', 'W', 'S', 'M', 'K', 'R', 'Y']\n",
        "\n",
        "def generate_config_str():\n",
        "    str_conf = '--------------------------------------------------------------\\n'\n",
        "    str_conf += \"Configuration:\\n\"\n",
        "    str_conf += \"==============\\n\"\n",
        "    str_conf += '\\n'\n",
        "    for key, value in config_dict.items():\n",
        "        str_conf += f\"\\t{key} = {str(value)}\\n\"\n",
        "    str_conf += '\\n'\n",
        "    str_conf += '--------------------------------------------------------------'\n",
        "    return str_conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7GeO5bL5Jhnl"
      },
      "outputs": [],
      "source": [
        "# spike gene coordinates (1-based): https://www.ncbi.nlm.nih.gov/nuccore/NC_045512.2?report=gbwithparts&log$=seqview\n",
        "spike_gene_start = 21563 -1\n",
        "spike_gene_end = 25384 -1\n",
        "# domain_coordinates_1based = {\n",
        "#     \"SP\" : [1, 13],\n",
        "#     \"S1\" : [14, 685],\n",
        "#     \"S2\" : [686, 1273],\n",
        "#     \"NTD\" : [14, 303],\n",
        "#     \"RBD\" : [319, 541],\n",
        "#     \"FP\" : [788, 806],\n",
        "#     \"S2'\" : [816, 1273],\n",
        "#     \"IFP\" : [891, 906],\n",
        "#     \"HR1\" : [942, 990],\n",
        "#     \"HR2\" : [1163, 1202],\n",
        "#     \"TM\" : [1214, 1234],\n",
        "#     \"CT\" : [1235, 1255],\n",
        "# }\n",
        "domain_coordinates_1based = {\n",
        "    \"SP\" : [1, 13],\n",
        "    \"S1\" : [14, 685],\n",
        "    \"S2\" : [686, 1273],\n",
        "    \"NTD\" : [14, 305],\n",
        "    \"no domain 1\": [306, 318],\n",
        "    \"RBD\" : [319, 541],\n",
        "    \"no domain 2\": [542, 787],\n",
        "    \"FP\" : [788, 806],\n",
        "    \"no domain 3\": [807, 911],\n",
        "    \"HR1\" : [912, 984],\n",
        "    \"no domain 4\": [985, 1162],\n",
        "    \"HR2\" : [1163, 1213],\n",
        "    \"TM\" : [1213, 1237],\n",
        "    \"CT\" : [1237, 1273],\n",
        "}\n",
        "\n",
        "def convert_domain_coords_in_token_indices(domain):\n",
        "    domain_coords_0based = np.asarray(domain_coordinates_1based[domain]) - 1;\n",
        "    domain_start_base_idx = domain_coords_0based[0] * 3\n",
        "    domain_end_base_idx = domain_coords_0based[1] * 3 - 1\n",
        "\n",
        "    bins = [[i*config_dict['STRIDE']-config_dict['STRIDE'], i*config_dict['STRIDE']-config_dict['STRIDE']+config_dict['K']-1] for i in range(config_dict['MAX_LENGTH'])]\n",
        "    bins[0][1] = -1 # cls\n",
        "\n",
        "    for i,bin in enumerate(bins):\n",
        "        if domain_start_base_idx >= bin[0] and domain_start_base_idx <= bin[1]:\n",
        "            domain_start_token_idx = i\n",
        "            break\n",
        "    for i,bin in enumerate(bins):\n",
        "        if domain_end_base_idx >= bin[0] and domain_end_base_idx <= bin[1]:\n",
        "            domain_end_token_idx = i\n",
        "    return domain_start_token_idx, domain_end_token_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZG2pCKjfs5sU"
      },
      "outputs": [],
      "source": [
        "colormaps = ['Greys', 'Greens', 'GnBu', 'Blues', 'Purples', 'PuRd', 'Reds', 'Oranges', 'YlOrBr',\n",
        "            'YlOrRd', 'OrRd', 'RdPu', 'BuPu',\n",
        "            'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
        "colormaps_layers = ['Greys', 'Greens', 'YlGn', 'PuBuGn', 'GnBu', 'Blues',\n",
        "             'Purples', 'RdPu', 'PuRd', 'Oranges', 'OrRd', 'Reds', 'YlOrRd', 'BuPu',\n",
        "            'YlGnBu','BuGn', 'PuBu', 'YlOrBr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5-ylETBpdQlF"
      },
      "outputs": [],
      "source": [
        "n_layers = 12\n",
        "n_heads = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9ACgOidWfE9d"
      },
      "outputs": [],
      "source": [
        "n_tokens_per_seq = math.ceil((((spike_gene_end-spike_gene_start)-config_dict['K'])/config_dict['STRIDE']) +1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NISnUnjVBboo"
      },
      "source": [
        "#### Directories and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3SQlM1w0aqoy"
      },
      "outputs": [],
      "source": [
        "main_dir = '/content/drive/MyDrive/tesi'\n",
        "\n",
        "# Input data:\n",
        "datasets_dir = '/content/drive/MyDrive/tesi/datasets'\n",
        "variant_files = {}\n",
        "for filename in os.listdir(Path(datasets_dir) / 'original'):\n",
        "    variant_files[os.path.splitext(filename)[0]] = Path(datasets_dir) / 'original' / filename\n",
        "if config_dict['SPIKE_REGION_ANALYSIS']:\n",
        "    reference_seq_file = Path(datasets_dir) / 'reference.fasta'\n",
        "    if not os.path.exists(reference_seq_file):\n",
        "        raise FileNotFoundError(f'File {reference_seq_file} not found in {datasets_dir} folder.')\n",
        "\n",
        "# Experiment dir:\n",
        "if config_dict['SPLIT_DATA_IN_CHUNKS']:\n",
        "    experiment_dir = Path(main_dir) / f\"cl{config_dict['CHUNK_LEN']}_cs{config_dict['CHUNK_STRIDE']}_k{config_dict['K']}_s{config_dict['STRIDE']}\"\n",
        "else:\n",
        "    experiment_dir = Path(main_dir) / f\"k{config_dict['K']}_s{config_dict['STRIDE']}\"\n",
        "\n",
        "# Preprocessed data:\n",
        "preprocessed_data_dir = Path(experiment_dir) / 'preprocessed_data'\n",
        "if not os.path.exists(preprocessed_data_dir):\n",
        "    os.makedirs(preprocessed_data_dir)\n",
        "    print(f\"Directory '{preprocessed_data_dir}' created\")\n",
        "if config_dict['SPIKE_REGION_ANALYSIS']:\n",
        "    reformatted_seqs_file = Path(preprocessed_data_dir) / 'reformatted_seqs.fasta'\n",
        "    duplicated_seqs_file = Path(preprocessed_data_dir) / 'duplicated_seqs.fasta'\n",
        "    aligned_seqs_file = Path(preprocessed_data_dir) / 'aligned.sam'\n",
        "    spike_seqs_file = Path(preprocessed_data_dir) / 'spike_seqs.csv'\n",
        "    cigars_file = Path(preprocessed_data_dir) / 'cigars.csv'\n",
        "    input_seqs_file = spike_seqs_file\n",
        "else:\n",
        "    reformatted_seqs_file = Path(preprocessed_data_dir) / 'reformatted_seqs.csv'\n",
        "    input_seqs_file = reformatted_seqs_file\n",
        "seqs_index_file = Path(preprocessed_data_dir) / 'seqs_index.csv'\n",
        "ids_dict_file = Path(preprocessed_data_dir) / 'ids_dict.csv'\n",
        "trainvaltest_splits_dir = Path(preprocessed_data_dir) / 'trainvaltest_splits'\n",
        "train_file = Path(trainvaltest_splits_dir) / 'train.csv'\n",
        "val_file = Path(trainvaltest_splits_dir) / 'val.csv'\n",
        "test_file = Path(trainvaltest_splits_dir) / 'test.csv'\n",
        "trainvaltest_sizes_file = Path(trainvaltest_splits_dir) / 'trainvaltest_sizes.csv'\n",
        "token_count_file = Path(preprocessed_data_dir) / 'token_count.csv'\n",
        "\n",
        "# Outputs dir:\n",
        "outputs_dir = Path(experiment_dir) / 'outputs'\n",
        "if not os.path.exists(outputs_dir):\n",
        "    os.makedirs(outputs_dir)\n",
        "    print(f\"Directory '{outputs_dir}' created\")\n",
        "log_dir = Path(outputs_dir) / 'log'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "    print(f\"Directory '{log_dir}' created\")\n",
        "tokens_histograms_dir = Path(outputs_dir) / 'tokens_histograms'\n",
        "if not os.path.exists(tokens_histograms_dir):\n",
        "    os.makedirs(tokens_histograms_dir)\n",
        "    print(f\"Directory '{tokens_histograms_dir}' created\")\n",
        "final_val_outputs_file = Path(outputs_dir) / 'final_val_outputs'\n",
        "final_test_outputs_file = Path(outputs_dir) / 'final_test_outputs'\n",
        "model_file_finetuned = Path(outputs_dir) / 'model_finetuned'\n",
        "model_file = Path(outputs_dir) / 'model'\n",
        "tokenizer_file = Path(outputs_dir) / 'tokenizer'\n",
        "training_stats_file = Path(outputs_dir) / 'training_stats'\n",
        "test_accuracies_file = Path(outputs_dir) / 'test_accuracies'\n",
        "final_data_test_file = Path(outputs_dir) / 'final_data_test_file'\n",
        "\n",
        "# attention matrices\n",
        "attention_matrices_dir = Path(outputs_dir) / 'attention_matrices'\n",
        "if not os.path.exists(attention_matrices_dir):\n",
        "    os.makedirs(attention_matrices_dir)\n",
        "    print(f\"Directory '{attention_matrices_dir}' created\")\n",
        "proportion_attn_domains_dir = Path(attention_matrices_dir) / \"proportion_attn_domains\"\n",
        "if not os.path.exists(proportion_attn_domains_dir):\n",
        "    os.makedirs(proportion_attn_domains_dir)\n",
        "    print(f\"Directory '{proportion_attn_domains_dir}' created\")\n",
        "\n",
        "#mathematical interpretation outputs\n",
        "math_interpret_dir = Path(outputs_dir) / \"mathematical_interpretation\"\n",
        "if not os.path.exists(math_interpret_dir):\n",
        "    os.makedirs(math_interpret_dir)\n",
        "    print(f\"Directory '{math_interpret_dir}' created\")\n",
        "CLS_embeddings_dir = Path(math_interpret_dir) / \"Y_outputs\" / \"CLS_embeddings\"\n",
        "if not os.path.exists(CLS_embeddings_dir):\n",
        "    os.makedirs(CLS_embeddings_dir)\n",
        "    print(f\"Directory '{CLS_embeddings_dir}' created\")\n",
        "\n",
        "# biological interpretation output\n",
        "log_dir_bio = Path(outputs_dir) / \"biological_interpretation\" / \"log\"\n",
        "if not os.path.exists(log_dir_bio):\n",
        "    os.makedirs(log_dir_bio)\n",
        "    print(f\"Directory '{log_dir_bio}' created\")\n",
        "\n",
        "# clustering output\n",
        "clustering_dir = Path(outputs_dir) / \"clustering\"\n",
        "if not os.path.exists(clustering_dir):\n",
        "    os.makedirs(clustering_dir)\n",
        "    print(f\"Directory '{clustering_dir}' created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8iK6he4kTgp_"
      },
      "outputs": [],
      "source": [
        "config_dict['CLASS_LABELS'] = {}\n",
        "i = 0\n",
        "for variant_name in variant_files:\n",
        "    config_dict['CLASS_LABELS'][variant_name] = i\n",
        "    i += 1\n",
        "config_dict['N_CLASSES'] = i\n",
        "inv_class_labels_dict = {v:k for k, v in config_dict['CLASS_LABELS'].items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAl_Nz-Nw-bO",
        "outputId": "e195bd77-5fa4-49d8-e71e-f2201be6f1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------\n",
            "Configuration:\n",
            "==============\n",
            "\n",
            "\tUSE_GPU = True\n",
            "\tTASK_TYPE = cone_index_sing_vals_histograms\n",
            "\tSPIKE_REGION_ANALYSIS = True\n",
            "\tREDUCE_N_INPUT_SAMPLES = True\n",
            "\tMAX_N_SAMPLES_PER_CLASS = 40000\n",
            "\tK = 12\n",
            "\tSTRIDE = 9\n",
            "\tADD_KMER_TOKENS_TO_VOCAB = True\n",
            "\tMIN_N_OCCUR_KMER = None\n",
            "\tALIGNMENT = True\n",
            "\tSPLIT_DATA_IN_CHUNKS = False\n",
            "\tCHUNK_LEN = 150\n",
            "\tCHUNK_STRIDE = 150\n",
            "\tTRAIN_BATCH_SIZE = 4\n",
            "\tEVAL_BATCH_SIZE = 4\n",
            "\tEPOCHS = 2\n",
            "\tLR = 2e-05\n",
            "\tMAX_LENGTH = 512\n",
            "\tTHETA = 0.3\n",
            "\tPOSITIVE_CLASS_MLP = mu\n",
            "\tRELEVANT_TOKENS_MLP = ['211-214']\n",
            "\tINPUT_DIM_MLP = 768\n",
            "\tOUTPUT_DIM_MLP = 2\n",
            "\tTRAIN_BATCH_SIZE_MLP = 64\n",
            "\tEVAL_BATCH_SIZE_MLP = 64\n",
            "\tEPOCHS_MLP = 5\n",
            "\tHIDDEN_UNITS_1_MLP = 32\n",
            "\tHIDDEN_UNITS_2_MLP = 32\n",
            "\tLR_MLP = 2e-05\n",
            "\tSELECTED_LAYER_HEAD_LIST = [[0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0], [10, 0], [11, 0], [0, 1], [1, 1], [2, 1], [3, 1], [4, 1], [5, 1], [6, 1], [7, 1], [8, 1], [9, 1], [10, 1], [11, 1], [0, 2], [1, 2], [2, 2], [3, 2], [4, 2], [5, 2], [6, 2], [7, 2], [8, 2], [9, 2], [10, 2], [11, 2], [0, 3], [1, 3], [2, 3], [3, 3], [4, 3], [5, 3], [6, 3], [7, 3], [8, 3], [9, 3], [10, 3], [11, 3], [0, 4], [1, 4], [2, 4], [3, 4], [4, 4], [5, 4], [6, 4], [7, 4], [8, 4], [9, 4], [10, 4], [11, 4], [0, 5], [1, 5], [2, 5], [3, 5], [4, 5], [5, 5], [6, 5], [7, 5], [8, 5], [9, 5], [10, 5], [11, 5], [0, 6], [1, 6], [2, 6], [3, 6], [4, 6], [5, 6], [6, 6], [7, 6], [8, 6], [9, 6], [10, 6], [11, 6], [0, 7], [1, 7], [2, 7], [3, 7], [4, 7], [5, 7], [6, 7], [7, 7], [8, 7], [9, 7], [10, 7], [11, 7], [0, 8], [1, 8], [2, 8], [3, 8], [4, 8], [5, 8], [6, 8], [7, 8], [8, 8], [9, 8], [10, 8], [11, 8], [0, 9], [1, 9], [2, 9], [3, 9], [4, 9], [5, 9], [6, 9], [7, 9], [8, 9], [9, 9], [10, 9], [11, 9], [0, 10], [1, 10], [2, 10], [3, 10], [4, 10], [5, 10], [6, 10], [7, 10], [8, 10], [9, 10], [10, 10], [11, 10], [0, 11], [1, 11], [2, 11], [3, 11], [4, 11], [5, 11], [6, 11], [7, 11], [8, 11], [9, 11], [10, 11], [11, 11]]\n",
            "\tSELECTED_CLASS = omicron\n",
            "\tBASES = ['A', 'C', 'G', 'T', 'N', 'W', 'S', 'M', 'K', 'R', 'Y']\n",
            "\tCLASS_LABELS = {'gh': 0, 'lambda': 1, 'mu': 2, 'alpha': 3, 'beta': 4, 'delta': 5, 'gamma': 6, 'omicron': 7}\n",
            "\tN_CLASSES = 8\n",
            "\n",
            "--------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "n_log_files = len(os.listdir(log_dir))\n",
        "if n_log_files > 0:\n",
        "    log_file = Path(log_dir) / f'log({n_log_files}).txt'\n",
        "else:\n",
        "    log_file = Path(log_dir) / f'log.txt'\n",
        "\n",
        "with open(log_file, 'w') as log_fp:\n",
        "    log_fp.write(generate_config_str())\n",
        "    print(generate_config_str())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d49fkRvRBqVx"
      },
      "source": [
        "#### Tokenizer and Bert model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "62528069eafb4a3ebab007c2336f191c",
            "a5af04a567ce46a4bbb1221859e02606",
            "d1501459dc9447b6bde94a8787f8d2b4",
            "9404ca4525914ab1ba1e8d3fbcf38fdf",
            "3763841c7c7a4423abce8d6f9422859d",
            "2bad7f9042234a46be8d1bf4a6a48ce9",
            "ded6037026014b129d66794be49215ec",
            "aa2952c078c946bda7c777c71832b81a",
            "374d63e815da463693d682480d7330a9",
            "d9655efd73eb4075b3d022541f27c752",
            "3a3c227272ef4f17add45b22fd5c150f",
            "87beeb9bb46849dc84c96d437fe01e1f",
            "90949c703fd145d289ab9a2a65cb2e38",
            "abc575db4a6c4a059fe049ac6b2f0068",
            "70cba967949343ca8212fb7871bc00e7",
            "0314e07920094c5b9d54d20f06465c68",
            "ba71b3f2eaea49a698ad2da3fda8bbba",
            "c9327eb469a6438b8762b43cefcfdf6e",
            "a3d8c9ab43e6426c970c4b4481eb0fb1",
            "b35be6fa41b649e28fc896c9a780623c",
            "358ce13f267f41669ff59754652c497a",
            "4e7baa4e005541b587c80e51fced9871",
            "aefdf3e259984e67ba533399c025a82f",
            "75fa909d80134cedae0650bb30e67343",
            "348397f18a924890bd5c46ea096eac71",
            "f3f1773b9cf0452a984ca05f105aaefe",
            "be9e04b9f433436eabd83065b022686f",
            "4250f994d6704ba5b1cce670910213fb",
            "a32e4f9f34094459922dce0f77c402c3",
            "e7562f30ffbe4e08b7ed71f9197ddc40",
            "2082d8728c61488989bef409df7623e1",
            "33b6bef13ca54977ae7c536fcb227e98",
            "9f2ec9ef728248afb04ee78178bf6d96",
            "b79625aa919c4911a8dbd8031a610f9b",
            "e0f1a0c2b87f4ea0b85077352630c402",
            "fda825da18154a6b8ed86d3efab8b7c0",
            "3a4c089868b6434b9d4c60ab134a1d28",
            "64854179264644a2a7fc7b174913f228",
            "d4c1feb958b3405481cb99dbe6fa8011",
            "ca40ac2376034c29a080138817dfcbe6",
            "a83268fc86304dc7baa5766f386543cc",
            "5dab372e1788448d9726988d1274c7c6",
            "e42654f417974163855a8d93ed78fe15",
            "9e4ed3604134497982de13511e26d04b"
          ]
        },
        "id": "k0lESROX9zqI",
        "outputId": "ec61efd5-aabf-4738-90bb-171169620bce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62528069eafb4a3ebab007c2336f191c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87beeb9bb46849dc84c96d437fe01e1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aefdf3e259984e67ba533399c025a82f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b79625aa919c4911a8dbd8031a610f9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)\n",
        "\n",
        "# supervised BERT classifier:\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=config_dict['N_CLASSES'],\n",
        "    output_attentions = True, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOOTHxRydWym",
        "outputId": "7402a0c6-94ef-49c8-b10f-0d311d24d195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime type: None.\n"
          ]
        }
      ],
      "source": [
        "if config_dict['USE_GPU'] and (do_training or do_test or config_dict['TASK_TYPE']=='one_vs_all_classification'):\n",
        "    # Get the GPU device name.\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "\n",
        "    # The device name should look like the following:\n",
        "    if device_name == '/device:GPU:0':\n",
        "        print('Found GPU at: {}'.format(device_name))\n",
        "    else:\n",
        "        raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "    # If there's a GPU available...\n",
        "    if torch.cuda.is_available():\n",
        "        # Tell PyTorch to use the GPU.\n",
        "        device = torch.device(\"cuda\")\n",
        "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    # If not...\n",
        "    else:\n",
        "        print('No GPU available, using the CPU instead.')\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "# elif USE_TPU:\n",
        "#     try:\n",
        "#         tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "#         print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "#     except ValueError:\n",
        "#         raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "else:\n",
        "    print('Runtime type: None.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsnAUTbvYGxm"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFI-Z-3RjDA7"
      },
      "source": [
        "#### Reformat input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ixiHm3Kaj8P7"
      },
      "outputs": [],
      "source": [
        "ids_dict = {}\n",
        "\n",
        "def read_fasta(fp):\n",
        "    \"\"\"\n",
        "    Read fasta file, yield id and sequence.\n",
        "    \"\"\"\n",
        "    id, seq = None, []\n",
        "    for line in fp:\n",
        "        line = line.rstrip()\n",
        "        if line.startswith(\">\"):\n",
        "            if id: yield (id, ''.join(seq))\n",
        "            id, seq = line, []\n",
        "        else:\n",
        "            seq.append(line)\n",
        "    if id: yield (id, ''.join(seq))\n",
        "\n",
        "def split_fixed_len_chunks(seq, chunk_len, chunk_stride):\n",
        "    \"\"\"\n",
        "    Split sequence in fixed-length chunks with stride. Pad shorter chunks with 'N'.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(seq), chunk_stride):\n",
        "        window = seq[i:i+chunk_len]\n",
        "        window += 'N' * (chunk_len - len(window))\n",
        "        chunks.append(window)\n",
        "    return chunks\n",
        "\n",
        "def reformat_fasta(input_file, output_file, duplicates_file, label, log_fp_trainval):\n",
        "    \"\"\"\n",
        "    Read fasta file, possibily split each sequence into chunks of length CHUNK_LEN and\n",
        "    reformat: '[label], [id], [base1], [base2], ..., [baseN]'\n",
        "    \"\"\"\n",
        "\n",
        "    index = []\n",
        "    seq_len_info = {}\n",
        "    seq_len_info['max'] = 0\n",
        "    seq_len_info['min'] = None\n",
        "    seq_len_info['sum'] = 0\n",
        "    seq_len_info['avg'] = 0\n",
        "    seq_len_info['n_seq'] = 0\n",
        "    seq_len_info['n_dup'] = 0\n",
        "\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    with open(input_file) as in_fp, open(output_file, 'a') as out_file, open(duplicates_file, 'a') as dup_file:\n",
        "\n",
        "        for seq_id, seq in tqdm(read_fasta(in_fp)):\n",
        "\n",
        "            #remove duplicates\n",
        "            if seq_id in ids_dict:\n",
        "                s = str(label) + ',' + str(seq_id) + ',' + str(0) + ',' + ''.join(seq) + '\\n'\n",
        "                dup_file.write(s)\n",
        "                seq_len_info['n_dup'] += 1\n",
        "                continue\n",
        "\n",
        "            # convert patient id from string to int\n",
        "            ids_dict[seq_id] = [len(ids_dict)+1, label]\n",
        "            id = ids_dict[seq_id][0]\n",
        "\n",
        "            if config_dict['SPLIT_DATA_IN_CHUNKS'] or config_dict['ALIGNMENT']:\n",
        "                seq_len_info['n_seq'] += 1\n",
        "\n",
        "                # split sequence in chunks\n",
        "                chunks = split_fixed_len_chunks(seq, config_dict['CHUNK_LEN'], config_dict['CHUNK_STRIDE'])\n",
        "\n",
        "                # reformat chunk info and write on file\n",
        "                # (NB: each chunk is identified by its sequence id and its position in the sequence)\n",
        "                if config_dict['ALIGNMENT']:\n",
        "                    seq_records = []\n",
        "                    for pos, c in enumerate(chunks):\n",
        "                        # FASTA format\n",
        "                        header = str(label) + ',' + str(id) + ',' + str(pos)\n",
        "                        s = Seq(''.join(c))\n",
        "                        s_record = SeqRecord(s, id=header, description=\"\")\n",
        "                        seq_records.append(s_record)\n",
        "                    SeqIO.write(seq_records, out_file, \"fasta\")\n",
        "                else:\n",
        "                    for pos, c in enumerate(chunks):\n",
        "                        # CSV format\n",
        "                        s = str(label) + ',' + str(id) + ',' + str(pos) + ',' + ''.join(c) + '\\n'\n",
        "                        out_file.write(s)\n",
        "\n",
        "                # update length statistics\n",
        "                index.append([id, label])\n",
        "                seq_len_info['sum'] += len(chunks)\n",
        "                if len(chunks) > seq_len_info['max']:\n",
        "                    seq_len_info['max'] = len(chunks)\n",
        "                if seq_len_info['min'] == None or len(chunks) < seq_len_info['min']:\n",
        "                    seq_len_info['min'] = len(chunks)\n",
        "\n",
        "            else:\n",
        "                # reformat sequence info and write on file\n",
        "                s = str(label) + ',' + str(id) + ',' + str(0) + ',' + ''.join(seq) + '\\n'\n",
        "                out_file.write(s)\n",
        "                index.append([id, label])\n",
        "\n",
        "                # update length statistics\n",
        "                seq_len_info['sum'] += len(seq)\n",
        "                seq_len_info['n_seq'] += 1\n",
        "                if len(seq) > seq_len_info['max']:\n",
        "                    seq_len_info['max'] = len(seq)\n",
        "                if seq_len_info['min'] == None or len(seq) < seq_len_info['min']:\n",
        "                    seq_len_info['min'] = len(seq)\n",
        "\n",
        "            # stop earlier to consider only MAX_N_SAMPLES_PER_CLASS samples per class\n",
        "            count += 1\n",
        "            if config_dict['REDUCE_N_INPUT_SAMPLES'] and count >= config_dict['MAX_N_SAMPLES_PER_CLASS']:\n",
        "                break\n",
        "\n",
        "        # print length statistics\n",
        "        seq_len_info['avg'] = seq_len_info['sum'] / seq_len_info['n_seq']\n",
        "        print(f\"\\tN. sequences: {seq_len_info['n_seq']}\")\n",
        "        print(f\"\\tN. duplicated sequences: {seq_len_info['n_dup']}\")\n",
        "        print(f\"\\tTot. n. chunks: {seq_len_info['sum']}\")\n",
        "        print(f\"\\tMin. {'n. chunks per sequence' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {seq_len_info['min']}\")\n",
        "        print(f\"\\tMax. {'n. chunks per sequence' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {seq_len_info['max']}\")\n",
        "        print(f\"\\tAvg. {'n. chunks per sequence' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {seq_len_info['avg']}\")\n",
        "        print()\n",
        "        log_fp_trainval.write(f\"\\tN. sequences: {seq_len_info['n_seq']}\\n\")\n",
        "        log_fp_trainval.write(f\"\\tN. duplicated sequences: {seq_len_info['n_dup']}\\n\")\n",
        "        log_fp_trainval.write(f\"\\tTot. n. chunks: {seq_len_info['sum']}\\n\")\n",
        "        log_fp_trainval.write(f\"\\tMin. {'n. chunks per sequence' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {seq_len_info['min']}\\n\")\n",
        "        log_fp_trainval.write(f\"\\tMax. {'n. chunks per sequence' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {seq_len_info['max']}\\n\")\n",
        "        log_fp_trainval.write(f\"\\tAvg. {'n. chunks per sequence' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {seq_len_info['avg']}\\n\\n\")\n",
        "\n",
        "    with open(ids_dict_file, 'w') as ids_dict_fp:\n",
        "        ids_dict_fp.write(f\"seq_id,id,label\\n\")\n",
        "        for seq_id, id_label_list in ids_dict.items():\n",
        "            ids_dict_fp.write(f\"{seq_id},{id_label_list[0]},{id_label_list[1]}\\n\")\n",
        "\n",
        "    return index, seq_len_info\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNyp3nmTyUpu",
        "outputId": "e0bd721b-9872-4490-c0c9-45d110d7bf82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data already reformatted in /content/drive/MyDrive/tesi/k12_s9/preprocessed_data/reformatted_seqs.fasta file. See /content/drive/MyDrive/tesi/k12_s9/outputs/log/log(136).txt for data info.\n"
          ]
        }
      ],
      "source": [
        "# check if all input paths exist\n",
        "for _, input_file in variant_files.items():\n",
        "    if not os.path.exists(input_file):\n",
        "        raise FileNotFoundError(f\"File {input_file} not found\")\n",
        "\n",
        "\n",
        "# if reformatted data already available continue, else reformat\n",
        "if os.path.exists(reformatted_seqs_file):\n",
        "    print(f\"Data already reformatted in {reformatted_seqs_file} file. See {log_file} for data info.\")\n",
        "else:\n",
        "    with open(seqs_index_file, 'w') as seqs_index_fp, open(log_file, 'w') as log_fp:\n",
        "        tot_n_seq = 0\n",
        "        tot_sum_len = 0\n",
        "        tot_max_len_list = []\n",
        "        tot_min_len_list = []\n",
        "\n",
        "        log_fp.write(f\"Sequences info:\\n\")\n",
        "        log_fp.write(f\"===============\\n\")\n",
        "\n",
        "        # read each variant input file and reformat, possibly splitting sequences into chunks\n",
        "        for variant_name, input_file in variant_files.items():\n",
        "            print(f\"Variant name: {variant_name} | Label: {config_dict['CLASS_LABELS'][variant_name]} | File: {input_file}\")\n",
        "            log_fp.write(f\"Variant name: {variant_name} | Label: {config_dict['CLASS_LABELS'][variant_name]} | File: {input_file}\\n\")\n",
        "\n",
        "            variant_seqs_index, variant_seq_len_info = reformat_fasta(input_file, reformatted_seqs_file, duplicated_seqs_file, config_dict['CLASS_LABELS'][variant_name], log_fp)\n",
        "\n",
        "            seqs_index_csvwriter = csv.writer(seqs_index_fp)\n",
        "            seqs_index_csvwriter.writerows(variant_seqs_index)\n",
        "\n",
        "            tot_max_len_list.append(variant_seq_len_info['max'])\n",
        "            tot_min_len_list.append(variant_seq_len_info['min'])\n",
        "            tot_sum_len += variant_seq_len_info['sum']\n",
        "            tot_n_seq += variant_seq_len_info['n_seq']\n",
        "\n",
        "        tot_max_len = max(tot_max_len_list)\n",
        "        tot_min_len = min(tot_min_len_list)\n",
        "\n",
        "        print(f'Tot info:')\n",
        "        print(f'\\tN. seqences: {tot_n_seq}')\n",
        "        print(f'\\tTot n. chunks: {tot_sum_len}')\n",
        "        print(f\"\\tMin {'n. chunks' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {tot_min_len}\")\n",
        "        print(f\"\\tMax {'n. chunks' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {tot_max_len}\")\n",
        "        print(f\"\\tAvg {'n. chunks' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {tot_sum_len / tot_n_seq}\")\n",
        "        log_fp.write(f'Tot info:\\n')\n",
        "        log_fp.write(f'\\tN. seqences: {tot_n_seq}\\n')\n",
        "        log_fp.write(f\"\\tMin {'n. chunks' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {min(tot_min_len_list)}\\n\")\n",
        "        log_fp.write(f\"\\tMax {'n. chunks' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {max(tot_max_len_list)}\\n\")\n",
        "        log_fp.write(f\"\\tAvg {'n. chunks' if config_dict['SPLIT_DATA_IN_CHUNKS'] else 'len'}: {tot_sum_len / tot_n_seq}\\n\")\n",
        "        log_fp.write('--------------------------------------------------------------\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wp0QxyBaHNm"
      },
      "source": [
        "#### Get test data used for the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGylGmJNBwIi",
        "outputId": "514cafab-fbc5-4d39-9f4e-45d32cfee36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "144920it [00:00, 270272.38it/s]\n",
            "9881it [00:00, 14087.93it/s]\n",
            "92it [00:00, 250.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100922it [00:00, 113086.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "lambda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "433919it [00:02, 185930.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143\n",
            "mu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2658357it [00:11, 238327.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "506\n",
            "alpha\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10161950it [00:36, 279063.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1300\n",
            "beta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9722973it [00:34, 280200.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1803\n",
            "delta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8972918it [00:39, 229352.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2251\n",
            "gamma\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10848836it [00:57, 187935.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1526\n",
            "omicron\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11273361it [00:43, 261457.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with open(ids_dict_file, 'r') as idsdictfp, open(test_file, 'r') as testfilefp, open(Path(preprocessed_data_dir)/'elenco_ids_test.txt', 'w') as elenco_ids_test_fp:\n",
        "    idsdictaltro = {}\n",
        "    csvreader_ids = csv.reader(idsdictfp, delimiter=',')\n",
        "    csvreader_testfp = csv.reader(testfilefp, delimiter=',')\n",
        "    ids_list_filter = []\n",
        "    for line in tqdm(csvreader_ids):\n",
        "        idsdictaltro[line[1]] = line[0]\n",
        "    for line in tqdm(csvreader_testfp):\n",
        "        elenco_ids_test_fp.write(f'{idsdictaltro[line[1]]}\\n')\n",
        "        ids_list_filter.append(idsdictaltro[line[1]])\n",
        "\n",
        "duplicated_seqs_list_filter = []\n",
        "with open(duplicated_seqs_file, 'r') as dupseqsfp:\n",
        "    csvreader_dupseqsfp = csv.reader(dupseqsfp, delimiter=',')\n",
        "    for line in tqdm(csvreader_dupseqsfp):\n",
        "        duplicated_seqs_list_filter.append(line[1])\n",
        "\n",
        "\n",
        "# output_file = Path(preprocessed_data_dir)/ 'test_filtered' / f'complete.fasta'\n",
        "for variant_name, input_file in variant_files.items():\n",
        "    output_file = Path(preprocessed_data_dir)/ 'test_filtered' / f'{variant_name}.fasta'\n",
        "    print(variant_name)\n",
        "    with open(input_file) as in_fp, open(output_file, 'a') as out_fp:\n",
        "        i=0\n",
        "        id, seq = None, []\n",
        "        flag_keep = True\n",
        "        list_dup_seqs_already_inserted = []\n",
        "        for line in tqdm(in_fp):\n",
        "            line = line.rstrip()\n",
        "            if line.startswith(\">\"):\n",
        "                if line in ids_list_filter:\n",
        "                    if line in duplicated_seqs_list_filter:\n",
        "                        if line not in list_dup_seqs_already_inserted:\n",
        "                            i+=1\n",
        "                            flag_keep = True\n",
        "                            list_dup_seqs_already_inserted.append(line)\n",
        "                        else:\n",
        "                            flag_keep = False\n",
        "                    else:\n",
        "                        i+=1\n",
        "                        flag_keep = True\n",
        "                else:\n",
        "                    flag_keep = False\n",
        "            if flag_keep:\n",
        "                out_fp.write(f'{line}\\n')\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzEy9Q_Di9cL"
      },
      "source": [
        "#### Sequence alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Taj4S7I2Ha"
      },
      "source": [
        "Fetch aligned chunks overlapping with spike gene and concatenate on file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L9dZl9UNI0mE"
      },
      "outputs": [],
      "source": [
        "def fetch_aligned_spike_seqs(aligned_seqs_file, spike_seqs_file, class_num):\n",
        "    if not os.path.exists(aligned_seqs_file):\n",
        "        print(f\"Error: file {aligned_seqs_file} not found.\")\n",
        "    print(f'Processing aligned sequences of class {class_num}.')\n",
        "\n",
        "    aligned_seqs_dict = {}\n",
        "    samfile = pysam.AlignmentFile(aligned_seqs_file, \"r\")\n",
        "    i=0\n",
        "    n_mapped = 0\n",
        "    n_reverse = 0\n",
        "    n_supplementary = 0\n",
        "    n_secondary = 0\n",
        "    n_unmapped = 0\n",
        "    n_tot_flagged = 0\n",
        "\n",
        "    # parse sam file\n",
        "    for aligned_segment in tqdm(samfile.fetch()):\n",
        "\n",
        "        #check if read correctly aligned or reverse\n",
        "        if aligned_segment.flag == 0 or aligned_segment.is_reverse:\n",
        "\n",
        "            qname = aligned_segment.query_name.split(',')\n",
        "            label = qname[0]\n",
        "            id = qname[1]\n",
        "\n",
        "            # select only sequences of current class\n",
        "            if label == str(class_num):\n",
        "\n",
        "                aligned_chunk = aligned_segment.query_alignment_sequence\n",
        "                aligned_pos_start = aligned_segment.reference_start # 0-based leftmost coordinate\n",
        "                aligned_pos_end = aligned_segment.reference_end -1 # reference_end points to one past the last aligned residue\n",
        "                aligned_reference_positions = aligned_segment.get_reference_positions(full_length=True) # a list of reference positions that this read aligns to.\n",
        "                                                                    # If full_length is set, None values will be included for any soft-clipped or unaligned positions within the read.\n",
        "                                                                    # The returned list will thus be of the same length as the read\n",
        "                cigar = aligned_segment.cigarstring\n",
        "\n",
        "                n_mapped += 1\n",
        "                # check if read mapped over a portion of the spike gene\n",
        "                if (aligned_pos_start < spike_gene_start and aligned_pos_end > spike_gene_start)\\\n",
        "                    or (aligned_pos_start > spike_gene_start and aligned_pos_end < spike_gene_end) \\\n",
        "                    or (aligned_pos_start < spike_gene_end and aligned_pos_end > spike_gene_end):\n",
        "\n",
        "                    # print(f'{qname} {aligned_pos_start} {aligned_pos_end}')\n",
        "                    # i +=1\n",
        "\n",
        "                    # save read info in dict, key is id\n",
        "                    if id not in aligned_seqs_dict:\n",
        "                        aligned_seqs_dict[id] = {'label':label,\n",
        "                                                'pos_start_list':[],\n",
        "                                                'pos_end_list':[],\n",
        "                                                'reference_positions_list':[],\n",
        "                                                'aligned_chunk_spike_list':[],\n",
        "                                                'cigar_list':[]}\n",
        "                    aligned_seqs_dict[id]['cigar_list'].append(cigar)\n",
        "\n",
        "                    # reverse complement if chunk has been aligned as reversed\n",
        "                    if aligned_segment.is_reverse:\n",
        "                        n_reverse += 1\n",
        "                        aligned_chunk = str(Seq(aligned_chunk).reverse_complement())\n",
        "\n",
        "                    # truncate chunks with bases outside of the spike gene\n",
        "                    # 1st case: first bases of read mapped outside of spike region\n",
        "                    if aligned_pos_start < spike_gene_start and aligned_pos_end > spike_gene_start:\n",
        "                        # truncate aligned chunk\n",
        "                        aligned_chunk_spike = aligned_chunk[spike_gene_start-aligned_pos_start:]\n",
        "                        # modify start position\n",
        "                        aligned_pos_start = spike_gene_start\n",
        "                        # truncate reference positions of aligment\n",
        "                        for idx, pos in enumerate(aligned_reference_positions):\n",
        "                            if pos != None and pos >= spike_gene_start:\n",
        "                                aligned_reference_positions_spike = aligned_reference_positions[idx:]\n",
        "                                break\n",
        "\n",
        "                    # 2nd case: last bases of read mapped outside of spike region\n",
        "                    elif aligned_pos_start < spike_gene_end and aligned_pos_end > spike_gene_end:\n",
        "                        # truncate aligned chunk\n",
        "                        aligned_chunk_spike = aligned_chunk[:spike_gene_end-aligned_pos_start+1]\n",
        "                        # modify end position\n",
        "                        aligned_pos_end = spike_gene_end\n",
        "                        # truncate reference positions of aligment\n",
        "                        for idx, pos in enumerate(aligned_reference_positions):\n",
        "                            if pos != None and pos > spike_gene_end:\n",
        "                                aligned_reference_positions_spike = aligned_reference_positions[:idx]\n",
        "                                break\n",
        "\n",
        "                    # 3rd case: read completely mapped inside spike region\n",
        "                    else:\n",
        "                        aligned_chunk_spike = aligned_chunk\n",
        "                        aligned_reference_positions_spike = aligned_reference_positions\n",
        "\n",
        "                    aligned_seqs_dict[id]['aligned_chunk_spike_list'].append(aligned_chunk_spike)\n",
        "                    aligned_seqs_dict[id]['reference_positions_list'].append(aligned_reference_positions_spike)\n",
        "                    relative_aligned_pos_start = aligned_pos_start - spike_gene_start\n",
        "                    aligned_seqs_dict[id]['pos_start_list'].append(relative_aligned_pos_start)\n",
        "                    relative_aligned_pos_end = aligned_pos_end - spike_gene_start\n",
        "                    aligned_seqs_dict[id]['pos_end_list'].append(relative_aligned_pos_end)\n",
        "\n",
        "                    # if (relative_aligned_pos_end-relative_aligned_pos_start+1) != len(aligned_chunk_spike):\n",
        "                    #     print(f\"Attention: {relative_aligned_pos_end}-{relative_aligned_pos_start}+1={relative_aligned_pos_end-relative_aligned_pos_start} != {len(aligned_chunk_spike)}\")\n",
        "                    #     print(aligned_segment.get_reference_positions(full_length=True))\n",
        "                    # else:\n",
        "                    #     print('OK')\n",
        "\n",
        "        # check if this is a supplementary alignment\n",
        "        if aligned_segment.is_supplementary:\n",
        "            n_supplementary += 1\n",
        "        # check if not primary alignment\n",
        "        if aligned_segment.is_secondary:\n",
        "            n_secondary += 1\n",
        "        # check if read itself is unmapped\n",
        "        if aligned_segment.is_unmapped:\n",
        "            n_unmapped += 1\n",
        "        if aligned_segment.flag != 0:\n",
        "            n_tot_flagged += 1\n",
        "\n",
        "        # if i>10:\n",
        "        #     break\n",
        "\n",
        "    samfile.close()\n",
        "\n",
        "    # write sequences index on file\n",
        "    with open(seqs_index_file, 'a') as seqs_index_fp:\n",
        "        seqs_index = [[int(id), aligned_seqs_dict[id]['label']] for id in aligned_seqs_dict]\n",
        "        seqs_index_csvwriter = csv.writer(seqs_index_fp)\n",
        "        seqs_index_csvwriter.writerows(seqs_index)\n",
        "\n",
        "    print(f'Skipped chunks and actions for class {class_num}:')\n",
        "    print(f'\\tn_reverse={n_reverse} (reverse complemented)')\n",
        "    print(f'\\tn_supplementary={n_supplementary} (skipped)')\n",
        "    print(f'\\tn_secondary={n_secondary} (skipped)')\n",
        "    print(f'\\tn_unmapped={n_unmapped} (skipped)')\n",
        "    print(f'\\tn_tot_flagged={n_tot_flagged}')\n",
        "\n",
        "    return aligned_seqs_dict #, seqs_index\n",
        "\n",
        "def concat_spike_seqs_on_file(aligned_seqs_dict, cigars_file, reference_seq_file):\n",
        "\n",
        "    # get reference sequence\n",
        "    with open(reference_seq_file, 'r') as reference_seq_fp:\n",
        "        for _, reference_seq in read_fasta(reference_seq_fp):\n",
        "            break\n",
        "        reference_seq_spike = reference_seq[spike_gene_start:spike_gene_end+1]\n",
        "\n",
        "    with open(spike_seqs_file, 'a') as spike_seqs_fp, open(cigars_file, 'a') as cigars_fp:\n",
        "\n",
        "        # concatenated_alignments = reference_seq.copy()\n",
        "\n",
        "        # get data of each patient\n",
        "        for id in tqdm(aligned_seqs_dict):\n",
        "            pos_start_list = aligned_seqs_dict[id]['pos_start_list']\n",
        "            pos_end_list = aligned_seqs_dict[id]['pos_end_list']\n",
        "            reference_positions_list = aligned_seqs_dict[id]['reference_positions_list']\n",
        "            aligned_chunk_spike_list = aligned_seqs_dict[id]['aligned_chunk_spike_list']\n",
        "            cigar_list = aligned_seqs_dict[id]['cigar_list']\n",
        "\n",
        "            # sort list of alignments of that id based on the start position\n",
        "            pos_start_list_sorted, pos_end_list_sorted, aligned_chunk_spike_list_sorted, cigar_list_sorted = \\\n",
        "                (list(l) for l in zip(*sorted(zip(pos_start_list, pos_end_list, aligned_chunk_spike_list, cigar_list),\n",
        "                                          key=lambda x: x[0])))\n",
        "            pos_start_list_sorted = pos_start_list\n",
        "            pos_end_list_sorted = pos_end_list\n",
        "            aligned_chunk_spike_list_sorted = aligned_chunk_spike_list\n",
        "\n",
        "            concatenated_alignment = []\n",
        "            prev_pos_end = None\n",
        "            is_start = True\n",
        "            for pos_start, pos_end, reference_positions, aligned_chunk_spike in zip(pos_start_list_sorted, pos_end_list_sorted, reference_positions_list, aligned_chunk_spike_list_sorted):\n",
        "                read_seq = []\n",
        "\n",
        "                # check if first bases of reference are unmapped\n",
        "                if is_start and pos_start > 0:\n",
        "                    # copy reference bases\n",
        "                    concatenated_alignment.extend(reference_seq_spike[:pos_start])\n",
        "\n",
        "                # check if there is a gap of unmapped bases between previous read and current read\n",
        "                if not is_start and pos_start - prev_pos_end > 1:\n",
        "                    concatenated_alignment.extend(reference_seq_spike[prev_pos_end:pos_start])\n",
        "\n",
        "                is_start = False\n",
        "\n",
        "                concatenated_alignment.extend(aligned_chunk_spike)\n",
        "\n",
        "                prev_pos_end = pos_end\n",
        "\n",
        "            # check if the last bases of reference are unmapped\n",
        "            if spike_gene_end-prev_pos_end > 0:\n",
        "                concatenated_alignment.extend(reference_seq_spike[prev_pos_end:spike_gene_end+1])\n",
        "\n",
        "            s = str(aligned_seqs_dict[id]['label']) + ',' + str(id) + ',' + str(0) + ',' + ''.join(concatenated_alignment) + '\\n'\n",
        "            spike_seqs_fp.write(s)\n",
        "\n",
        "            # TODO write cigars file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqiQRiyDv8iz",
        "outputId": "067fa5f7-bbde-488c-fb70-b6d5568ac0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned spike gene sequences already available in /content/drive/MyDrive/tesi/k12_s9/preprocessed_data/spike_seqs.csv file.\n"
          ]
        }
      ],
      "source": [
        "if config_dict['SPIKE_REGION_ANALYSIS']:\n",
        "    if os.path.exists(spike_seqs_file):\n",
        "        print(f\"Aligned spike gene sequences already available in {spike_seqs_file} file.\")\n",
        "    else:\n",
        "        for class_num in range(config_dict['N_CLASSES']):\n",
        "            aligned_seqs_dict = fetch_aligned_spike_seqs(aligned_seqs_file, spike_seqs_file, class_num)\n",
        "            concat_spike_seqs_on_file(aligned_seqs_dict, cigars_file, reference_seq_file)\n",
        "            del aligned_seqs_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFehJEJyB-yY"
      },
      "source": [
        "Read the index of sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw7K3YkMRs0o",
        "outputId": "63cb98e4-1738-41c8-ac0c-6c115becdace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "49403it [00:00, 137092.41it/s]\n"
          ]
        }
      ],
      "source": [
        "def read_seqs_index(seqs_index_file):\n",
        "    seqs_index = []\n",
        "    with open(seqs_index_file, 'r') as seqs_index_fp:\n",
        "        seqs_index_csvreader = csv.reader(seqs_index_fp, delimiter=',')\n",
        "        for line in tqdm(seqs_index_csvreader):\n",
        "            seqs_index.append([int(line[0]), int(line[1])])\n",
        "    return seqs_index\n",
        "\n",
        "seqs_index = read_seqs_index(seqs_index_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-JF9oLnHOwC"
      },
      "source": [
        "####Check duplicated sequences per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf5wj24MHnmb",
        "outputId": "184c2fe6-d568-4b59-ebf6-6e7b2f904c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gh | n. duplicated seqs = 0/120 (0.00%) | n. duplicated ids = 0/120 (0.00%)\n",
            "lambda | n. duplicated seqs = 0/713 (0.00%) | n. duplicated ids = 0/713 (0.00%)\n",
            "mu | n. duplicated seqs = 0/2532 (0.00%) | n. duplicated ids = 0/2532 (0.00%)\n",
            "alpha | n. duplicated seqs = 0/6500 (0.00%) | n. duplicated ids = 0/6500 (0.00%)\n",
            "beta | n. duplicated seqs = 0/9017 (0.00%) | n. duplicated ids = 0/9017 (0.00%)\n",
            "delta | n. duplicated seqs = 0/11254 (0.00%) | n. duplicated ids = 0/11254 (0.00%)\n",
            "gamma | n. duplicated seqs = 0/7629 (0.00%) | n. duplicated ids = 0/7629 (0.00%)\n",
            "omicron | n. duplicated seqs = 0/11638 (0.00%) | n. duplicated ids = 0/11638 (0.00%)\n",
            "Tot n. duplicates = 0/49403\t(0.00%)\n",
            "\n",
            "No total duplicates to remove.\n"
          ]
        }
      ],
      "source": [
        "# def find_duplicated_seqs(filepath, seqs_index_dict, type_dataset, remove_dups=False):\n",
        "def find_duplicated_seqs(filepath, seqs_index_dict, remove_dups=False):\n",
        "    # if type_dataset not in ['train', 'val', 'test']:\n",
        "    #     raise Exception('Not valid type_dataset: select train, val or test')\n",
        "    title = f'total duplicates'\n",
        "    tot_dups = 0\n",
        "    tot_seqs = 0\n",
        "    duplicates_line_num = []\n",
        "    with open(log_file, 'a') as log_fp:\n",
        "        log_fp.write(f\"{title}\\n\")\n",
        "        log_fp.write(f\"==============================\\n\")\n",
        "\n",
        "        with open(filepath, 'r') as fp:\n",
        "            for class_lab, class_seq_nums in seqs_index_dict.items():\n",
        "                class_seqs = []\n",
        "                class_ids = []\n",
        "                class_line_nums = []\n",
        "                csv_reader = csv.reader(fp, delimiter=',')\n",
        "                for n_line, line in enumerate(csv_reader):\n",
        "                    label = line[0]\n",
        "                    if label == str(class_lab):\n",
        "                        seq = line[3]\n",
        "                        id = line[1]\n",
        "                        class_seqs.append(seq)\n",
        "                        class_ids.append(id)\n",
        "                        class_line_nums.append(n_line)\n",
        "                        # class_lines.append(line)\n",
        "                fp.seek(0)\n",
        "                class_seqs_df = pd.DataFrame({'seqs': class_seqs,\n",
        "                                            'ids' : class_ids,\n",
        "                                            'line_nums' : class_line_nums})\n",
        "                del class_seqs\n",
        "                del class_ids\n",
        "                del class_line_nums\n",
        "                dup_seqs = class_seqs_df['seqs'].duplicated()\n",
        "                dup_ids = class_seqs_df['ids'].duplicated()\n",
        "                dup_mask = dup_seqs | dup_ids\n",
        "                duplicates_line_num.extend(class_seqs_df[dup_mask == True]['line_nums'])\n",
        "                tot_dups += dup_mask.sum()\n",
        "                tot_seqs += len(dup_mask)\n",
        "\n",
        "                log_dups = f'{inv_class_labels_dict[class_lab]} | n. duplicated seqs = {dup_seqs.sum()}/{len(dup_seqs)} ({dup_seqs.sum()/len(dup_seqs)*100:.2f}%) | n. duplicated ids = {dup_ids.sum()}/{len(dup_ids)} ({dup_ids.sum()/len(dup_ids)*100:.2f}%)'\n",
        "                log_fp.write(f\"{log_dups}\\n\")\n",
        "                print(log_dups)\n",
        "\n",
        "            log_dups = f'Tot n. duplicates = {tot_dups}/{tot_seqs}\\t({tot_dups/tot_seqs*100:.2f}%)'\n",
        "            log_fp.write(f\"{log_dups}\\n\\n\")\n",
        "            print(f\"{log_dups}\\n\")\n",
        "\n",
        "        if remove_dups:\n",
        "            if tot_dups != 0:\n",
        "                print(f\"Removing {title}...\")\n",
        "                dirname = os.path.dirname(filepath)\n",
        "                filename = os.path.splitext(os.path.basename(filepath))[0]\n",
        "                filepath_no_dups = Path(dirname) / f'{filename}_no_dups.csv'\n",
        "                with open(filepath, 'r') as fp, open(filepath_no_dups, 'w') as fp_no_dups, open(seqs_index_file, 'w') as seqs_index_fp:\n",
        "                    fp_no_dups_csvwriter = csv.writer(fp_no_dups)\n",
        "                    seqs_index_csvwriter = csv.writer(seqs_index_fp)\n",
        "                    csv_reader = csv.reader(fp, delimiter=',')\n",
        "                    for n_line, line in enumerate(csv_reader):\n",
        "                        if n_line not in duplicates_line_num:\n",
        "                            fp_no_dups_csvwriter.writerow(line)\n",
        "                            index = [int(line[1]), int(line[0])]\n",
        "                            seqs_index_csvwriter.writerow(index)\n",
        "                os.rename(filepath, f'{filepath}_with_dups.csv')\n",
        "                os.rename(filepath_no_dups, filepath)\n",
        "                print('Done.\\n')\n",
        "                log_fp.write(f\"Removed {title}\\n\\n\")\n",
        "\n",
        "                # #update data sizes\n",
        "                # sizes_info[f'total_data_size_seqs'] = sum(1 for line in open(test_file))\n",
        "                # with open(trainvaltest_sizes_file, 'w') as trainvaltest_sizes_fp:\n",
        "                #     for key in sizes_info.keys():\n",
        "                #         trainvaltest_sizes_fp.write(f\"{key},{sizes_info[key]}\\n\")\n",
        "\n",
        "            else:\n",
        "                print(f\"No {title} to remove.\")\n",
        "\n",
        "seqs_index_dict = {}\n",
        "\n",
        "for seq_n, seq_lab in seqs_index:\n",
        "    if seq_lab not in seqs_index_dict:\n",
        "        seqs_index_dict[seq_lab] = []\n",
        "    seqs_index_dict[seq_lab].append(seq_n)\n",
        "\n",
        "find_duplicated_seqs(input_seqs_file, seqs_index_dict, remove_dups=True)\n",
        "\n",
        "# find_duplicated_seqs(train_file, seqs_index_dict, type_dataset=\"train\")\n",
        "# find_duplicated_seqs(val_file, seqs_index_dict, type_dataset=\"val\")\n",
        "# find_duplicated_seqs(test_file, seqs_index_dict, type_dataset=\"test\", remove_dups=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "6PRVoYLE6GS2",
        "outputId": "1b6eb7de-28d0-4e31-a5cf-36bffb8601e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#print dups info\n",
        "df_dups = pd.read_csv(Path(preprocessed_data_dir) / 'dups_info.txt', delimiter=',', header=0)\n",
        "df_dups['N. not duplicated'] = df_dups['tot'] - df_dups['dups']\n",
        "df_dups.rename(columns={'dups': 'N. duplicates', 'class': 'Class'}, inplace=True)\n",
        "df_dups_plot = df_dups.drop(columns=['percent','tot'])\n",
        "df_dups_plot.plot(x='Class', kind='bar', stacked=True, title='Stacked Bar Graph by dataframe', ylabel='N. sequences')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yUZGqf9sXBJG"
      },
      "outputs": [],
      "source": [
        "# sizes_info[f'test_data_size_seqs'] = sum(1 for line in open(test_file))\n",
        "# print(sizes_info)\n",
        "# with open(trainvaltest_sizes_file, 'w') as trainvaltest_sizes_fp:\n",
        "#     for key in sizes_info.keys():\n",
        "#         trainvaltest_sizes_fp.write(f\"{key},{sizes_info[key]}\\n\")\n",
        "\n",
        "# with open(input_seqs_file, 'r') as fp, open(seqs_index_file, 'w') as seqs_index_fp:\n",
        "#                     seqs_index_csvwriter = csv.writer(seqs_index_fp)\n",
        "#                     csv_reader = csv.reader(fp, delimiter=',')\n",
        "#                     for n_line, line in enumerate(csv_reader):\n",
        "#                         index = [int(line[1]), int(line[0])]\n",
        "#                         seqs_index_csvwriter.writerow(index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arlnCFxwaVlk"
      },
      "source": [
        "####Split data in train and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ID2TyHCGsj"
      },
      "source": [
        "Split in train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkrjyi9DTjY6",
        "outputId": "a97724d7-996d-4f73-a6e3-b23642b882de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "49403it [00:00, 840862.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data already split in train-val-test datasets in /content/drive/MyDrive/tesi/k12_s9/preprocessed_data/trainvaltest_splits file. See /content/drive/MyDrive/tesi/k12_s9/outputs/log/log(136).txt for data info.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3it [00:00, 10.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train (70%): 36360 sequences\n",
            "Val (10%): 3162 sequences\n",
            "Test (20%): 9881 sequences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def split_train_val_test_seqs(seqs_index):\n",
        "    \"\"\"\n",
        "    Split input data in training, validation and testing datasets according to precomputed splits on ids\n",
        "    (NB: even if we work with chunks, all chunks of the same sequence are grouped in the same dataset since we split on sequence ids)\n",
        "    \"\"\"\n",
        "    ids = [s[0] for s in seqs_index] # X : [id1,...,idN]\n",
        "    y = [s[1] for s in seqs_index] # y : [label1,...,labelN]\n",
        "\n",
        "    # split the sequence ids into training, validation and testing splits\n",
        "    ids_train_val, ids_test, y_train_val, y_test = train_test_split(ids, y, test_size=0.20, random_state=42, stratify=y, shuffle=True)\n",
        "    ids_train, ids_val, y_train, y_val = train_test_split(ids_train_val, y_train_val, test_size=0.08, random_state=42, stratify=y_train_val, shuffle=True)\n",
        "\n",
        "    sizes_info = {}\n",
        "    sizes_info['train_data_size_seqs'] = len(ids_train)\n",
        "    sizes_info['val_data_size_seqs'] =len(ids_val)\n",
        "    sizes_info['test_data_size_seqs'] =len(ids_test)\n",
        "\n",
        "    # copy train, val, test sequences into separate files\n",
        "    with open(input_seqs_file, 'r') as in_fp,\\\n",
        "            open(train_file, 'w') as train_fp,\\\n",
        "            open(val_file, 'w') as val_fp,\\\n",
        "            open(test_file, 'w') as test_fp:\n",
        "        csvreader = csv.reader(in_fp, delimiter=',')\n",
        "        for line in tqdm(csvreader):\n",
        "            id = int(line[1])\n",
        "            if id in ids_train:\n",
        "                train_fp.write(','.join(line)+'\\n')\n",
        "            elif id in ids_val:\n",
        "                val_fp.write(','.join(line)+'\\n')\n",
        "            elif id in ids_test:\n",
        "                test_fp.write(','.join(line)+'\\n')\n",
        "            else:\n",
        "                print(f\"Error: id {id} not found in train, val, test splits\")\n",
        "                exit\n",
        "\n",
        "    return ids_train, ids_val, ids_test, sizes_info\n",
        "\n",
        "\n",
        "sizes_info = {}\n",
        "\n",
        "seqs_index = read_seqs_index(seqs_index_file)\n",
        "\n",
        "# if training, validation and testing datasets already available continue, else compute splits\n",
        "if os.path.exists(trainvaltest_splits_dir):\n",
        "    print(f\"Data already split in train-val-test datasets in {trainvaltest_splits_dir} file. See {log_file} for data info.\")\n",
        "    with open(trainvaltest_sizes_file, 'r') as trainvaltest_sizes_fp:\n",
        "        csvreader = csv.reader(trainvaltest_sizes_fp, delimiter=',')\n",
        "        for line in tqdm(csvreader):\n",
        "            sizes_info[line[0]] = int(line[1])\n",
        "else:\n",
        "    os.makedirs(trainvaltest_splits_dir)\n",
        "    print(f\"Directory '{trainvaltest_splits_dir}' created\")\n",
        "    ids_train, ids_val, ids_test, sizes_info  = split_train_val_test_seqs(seqs_index)\n",
        "    with open(trainvaltest_sizes_file, 'w') as trainvaltest_sizes_fp:\n",
        "        for key in sizes_info.keys():\n",
        "            trainvaltest_sizes_fp.write(f\"{key},{sizes_info[key]}\\n\")\n",
        "\n",
        "with open(log_file, 'a') as log_fp:\n",
        "    print(f\"\\nTrain (70%): {sizes_info['train_data_size_seqs']} sequences\")\n",
        "    print(f\"Val (10%): {sizes_info['val_data_size_seqs']} sequences\")\n",
        "    print(f\"Test (20%): {sizes_info['test_data_size_seqs']} sequences\")\n",
        "    log_fp.write(f\"\\nTrain - val - test split info:\\n\")\n",
        "    log_fp.write(f\"==============================\\n\")\n",
        "    log_fp.write(f\"Train (70%): {sizes_info['train_data_size_seqs']} sequences\\n\")\n",
        "    log_fp.write(f\"Val (10%): {sizes_info['val_data_size_seqs']} sequences\\n\")\n",
        "    log_fp.write(f\"Test (20%): {sizes_info['test_data_size_seqs']} sequences\\n\")\n",
        "    log_fp.write('--------------------------------------------------------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CQA2FIVH52C"
      },
      "source": [
        "### Tokenization and data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8luDn_1WCZl_"
      },
      "source": [
        "#### Add most frequent tokens to Bert vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVJ0OooVJcCh"
      },
      "source": [
        "Create dictionary of token ids and BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8Ak2KlXd87Au"
      },
      "outputs": [],
      "source": [
        "def seq2kmer(seq, k, stride, string_format=False):\n",
        "    \"\"\"\n",
        "    Convert original sequence to kmers with stride.\n",
        "    Pad shorter sequences with 'N'.\n",
        "\n",
        "    Arguments:\n",
        "    seq -- str, original sequence.\n",
        "    k -- int, kmer of length k specified.\n",
        "\n",
        "    Returns:\n",
        "    kmers -- str, kmers separated by space\n",
        "    \"\"\"\n",
        "    list_seq = list(seq)\n",
        "    not_divisible = (len(list_seq)%k != 0)\n",
        "    while not_divisible:\n",
        "        list_seq.append('N')\n",
        "        not_divisible = (len(list_seq)%k != 0)\n",
        "    kmers = [list_seq[x:x+k] for x in range(0, len(list_seq)-k+1, stride)]\n",
        "\n",
        "    if string_format:\n",
        "        return [''.join(kmer).lower() for kmer in kmers]\n",
        "\n",
        "    return kmers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ5yoP0Xc4SU",
        "outputId": "d06ce082-3c77-4414-8578-f091c5f355fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count file already exists.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# compute the number of occurrencies of each token in the training dataset:\n",
        "token_count = {}\n",
        "# if token counts already computed load them from file\n",
        "if os.path.exists(token_count_file):\n",
        "    print(\"Token count file already exists.\")\n",
        "    with open(token_count_file) as token_count_fp:\n",
        "        csvreader = csv.reader(token_count_fp, delimiter=',')\n",
        "        for line in tqdm(csvreader):\n",
        "            token_count[line[0]] = int(line[1])\n",
        "else:\n",
        "    with open(train_file) as train_fp, open(token_count_file, 'w') as token_count_fp:\n",
        "        train_reader = csv.reader(train_fp, delimiter=',')\n",
        "        # loop on training data\n",
        "        for line in tqdm(train_reader):\n",
        "            label = line[0]\n",
        "            id = line[1]\n",
        "            pos = line[2]\n",
        "            seq = line[3]\n",
        "            # split sequence/chunk in kmers\n",
        "            tokens = seq2kmer(seq, config_dict['K'], config_dict['STRIDE'], string_format=True)\n",
        "            # update token count\n",
        "            for count,tok in enumerate(tokens):\n",
        "                # if count > config_dict['MAX_LENGTH']-2: #consider only tokens <MAX_LENGTH\n",
        "                #     break\n",
        "                if tok in token_count:\n",
        "                    token_count[tok] += 1\n",
        "                else:\n",
        "                    token_count[tok] = 1\n",
        "        # save token count on file\n",
        "        csvwriter = csv.writer(token_count_fp, delimiter=',')\n",
        "        for key, value in token_count.items():\n",
        "            csvwriter.writerow([key, value])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW2TSCE1Io9k"
      },
      "source": [
        "Plot histogram of k-mer token occurrencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u9VlD4QYIkwX",
        "outputId": "3be4eae3-0eb0-4314-fab0-3bf5ed973db6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0826a9b9d5fd>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_count_ordered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Num. occurrences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_count_ordered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_histograms_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"k{config_dict['K']}_s{config_dict['STRIDE']}.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot sorted token count histogram to find elbow, i.e. number of tokens which contribute to most of token occurrences\n",
        "token_count_ordered = {k: v for k, v in sorted(token_count.items(), key=lambda item: item[1], reverse=True)}\n",
        "fig = plt.figure(figsize=(20,10))\n",
        "plt.step(range(len(token_count_ordered.keys())), list(token_count_ordered.values()),color='b')\n",
        "fig.suptitle(f\"Ordered token occurrencies (k={config_dict['K']}, stride={config_dict['STRIDE']})\", y=0.95, fontsize=20)\n",
        "plt.xlabel(f'Tokens (tot num: {len(token_count_ordered)})', fontsize=16)\n",
        "plt.xticks(np.arange(0, len(token_count_ordered), 100), rotation=90)\n",
        "plt.ylabel('Num. occurrences', fontsize=16)\n",
        "plt.yticks(np.arange(0, max(list(token_count_ordered.values())), 1000))\n",
        "fig_path = Path(tokens_histograms_dir) / f\"k{config_dict['K']}_s{config_dict['STRIDE']}.jpg\"\n",
        "fig.savefig(fig_path)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCKjjyKs3mD2"
      },
      "outputs": [],
      "source": [
        "# select minimum n. occurrences at elbow to determine which are the most frequent tokens\n",
        "print(f\"Min. n. occurrencies for kmers to be added as tokens to BERT vocabulary:\")\n",
        "config_dict['MIN_N_OCCUR_KMER'] = 0\n",
        "\n",
        "with open(log_file, 'a') as log_fp:\n",
        "    print(f\"MIN_N_OCCUR_KMER = {config_dict['MIN_N_OCCUR_KMER']}\")\n",
        "    log_fp.write(f\"Min. n. occurrencies for kmers to be added as tokens to BERT vocabulary:\\n\")\n",
        "    log_fp.write(f\"========================================================================\\n\")\n",
        "    log_fp.write(f\"MIN_N_OCCUR_KMER = {config_dict['MIN_N_OCCUR_KMER']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txG6pEE0lvde"
      },
      "source": [
        "Add most frequent kmer tokens to BERT vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBvct6rYlgP3"
      },
      "outputs": [],
      "source": [
        "if config_dict['ADD_KMER_TOKENS_TO_VOCAB']:\n",
        "    # extract most frequent tokens based on min n. of occurrences\n",
        "    frequent_tokens_in_train_data = [k for k,v in token_count.items() if v>config_dict['MIN_N_OCCUR_KMER']]\n",
        "    print(f\"Num. frequent (>{config_dict['MIN_N_OCCUR_KMER']} occurrences) tokens in train dataset: {len(frequent_tokens_in_train_data)}\")\n",
        "\n",
        "    # add most frequent tokens to BERT vocabulary\n",
        "    print('Adding most frequent new tokens to Bert tokenizer...')\n",
        "    num_added_toks = tokenizer.add_tokens(frequent_tokens_in_train_data)\n",
        "    print(f'Num. of new tokens added: {num_added_toks}')\n",
        "    with open(log_file, 'a') as log_fp:\n",
        "        log_fp.write(f\"Num. of new frequent tokens added to BERT vocabulary: {num_added_toks}\\n\")\n",
        "        log_fp.write('--------------------------------------------------------------\\n')\n",
        "\n",
        "    # resize BERT token embeddings\n",
        "    print('Resizing token embeddings of AutoModelForSequenceClassification...')\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJwKrFxDqUPS"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(tokenizer_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-_ek6Bf8PK0"
      },
      "outputs": [],
      "source": [
        "# Tell pytorch to run this model on the GPU.\n",
        "if do_training:\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4luOFo_x4-pN"
      },
      "source": [
        "Encoder for tokenized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPq7v7bZyzZh"
      },
      "outputs": [],
      "source": [
        "def token_encode(seq):\n",
        "    encoded_seq = {}\n",
        "\n",
        "    #   (1) Tokenize the sentence.\n",
        "    seq_tokens = seq2kmer(seq, config_dict['K'], config_dict['STRIDE'])\n",
        "\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    seq_input_ids = []\n",
        "    for token in seq_tokens:\n",
        "        token_str = ''.join(token).lower()\n",
        "        seq_input_ids.append(tokenizer.convert_tokens_to_ids(token_str))\n",
        "        # print(f'{token_str} || {tokenizer.convert_tokens_to_ids(token_str)}')\n",
        "    # print(seq_input_ids)\n",
        "\n",
        "    #   (5) Truncate the sentence to `MAX_LENGTH`\n",
        "    #       NB: -2 because we must consider also [CLS] and [SEP] tokens\n",
        "    if len(seq_tokens) > config_dict['MAX_LENGTH']-2:\n",
        "        seq_input_ids = seq_input_ids[:config_dict['MAX_LENGTH']-2]\n",
        "\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    seq_input_ids.insert(0, tokenizer.convert_tokens_to_ids('[CLS]'))\n",
        "\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    seq_input_ids.append(tokenizer.convert_tokens_to_ids('[SEP]'))\n",
        "\n",
        "    #   Pad if sentence is too short\n",
        "    if len(seq_tokens) < config_dict['MAX_LENGTH']:\n",
        "        length_seq = len(seq_input_ids)\n",
        "        seq_input_ids.extend([tokenizer.convert_tokens_to_ids('[PAD]')]*(config_dict['MAX_LENGTH'] - length_seq))\n",
        "    # print(seq_tokens)\n",
        "    # print(seq_input_ids)\n",
        "\n",
        "\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    seq_attention_mask = [int(id!=tokenizer.convert_tokens_to_ids('[PAD]')) for id in seq_input_ids] #map(lambda x: 0 if x==token_ids_dict['[PAD]'] else 1, seq_input_ids)\n",
        "    # print(seq_attention_mask)\n",
        "    # print()\n",
        "\n",
        "\n",
        "    encoded_seq['input_ids'] = seq_input_ids\n",
        "    encoded_seq['attention_mask'] = seq_attention_mask\n",
        "\n",
        "    return encoded_seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjacS5SEl9IU"
      },
      "source": [
        "####Data generator from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBJ5WbKItLeW"
      },
      "outputs": [],
      "source": [
        "class DatasetGenerator(Dataset):\n",
        "\n",
        "    def __init__(self, input_reader, input_fp, metadata):\n",
        "        self.input_reader = input_reader\n",
        "        self.metadata = metadata\n",
        "        self.input_fp = input_fp\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.metadata['len']\n",
        "\n",
        "    def __getitem__(self, idx):  #iterate lazily\n",
        "        line = getline(self.input_fp.name, idx+1 ).split(',') #idx+1 because getline considers 1 as index of first line in file, while idx starts from 0\n",
        "        #line = next(itertools.islice(self.input_fp, idx, idx+1)).split(',')\n",
        "        #print(line[:20])\n",
        "        try:\n",
        "            label = float(line[0])\n",
        "        except Exception as e:\n",
        "            print(f'line error: {line}')\n",
        "            exit()\n",
        "        seq_id = int(line[1])\n",
        "        pos = int(line[2])\n",
        "        seq = line[3]\n",
        "\n",
        "        # Tokenize sequence and map the tokens to thier word IDs.\n",
        "        example = InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                text_a = seq,\n",
        "                                text_b = None,\n",
        "                                label = label)\n",
        "\n",
        "        encoded_seq = token_encode(example.text_a)\n",
        "\n",
        "        features = InputFeatures(input_ids=encoded_seq['input_ids'],\n",
        "                                    attention_mask=encoded_seq['attention_mask'],\n",
        "                                    label=example.label)\n",
        "        ids = features.input_ids\n",
        "        mask = features.attention_mask\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(features.label, dtype=torch.float),\n",
        "            'seq_ids': torch.tensor(seq_id, dtype=torch.int),\n",
        "            'positions': torch.tensor(pos, dtype=torch.int)\n",
        "        }\n",
        "\n",
        "class DatasetGenerator_InputEmbeddings(Dataset):\n",
        "\n",
        "    def __init__(self, input_fp, metadata):\n",
        "        self.metadata = metadata\n",
        "        self.input_fp = input_fp\n",
        "        self.selected_variant = config_dict['POSITIVE_CLASS_MLP']\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.metadata['len']\n",
        "\n",
        "    def __getitem__(self, idx):  #iterate lazily\n",
        "        line = getline(self.input_fp.name, idx+1 ).split(',') #idx+1 because getline considers 1 as index of first line in file, while idx starts from 0\n",
        "        #line = next(itertools.islice(self.input_fp, idx, idx+1)).split(',')\n",
        "        #print(line[:20])\n",
        "        label_tmp = float(line[0])\n",
        "        if label_tmp == config_dict['CLASS_LABELS'][self.selected_variant]:\n",
        "            label = float(1)\n",
        "        else:\n",
        "            label = float(0)\n",
        "        seq_id = int(line[1])\n",
        "        pos = int(line[2])\n",
        "        seq = line[3]\n",
        "\n",
        "        # Tokenize sequence and map the tokens to thier word IDs.\n",
        "        example = InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                text_a = seq,\n",
        "                                text_b = None,\n",
        "                                label = label)\n",
        "\n",
        "        encoded_seq = token_encode(example.text_a)\n",
        "\n",
        "        features = InputFeatures(input_ids=encoded_seq['input_ids'],\n",
        "                                    attention_mask=encoded_seq['attention_mask'],\n",
        "                                    label=example.label)\n",
        "        ids = features.input_ids\n",
        "        mask = features.attention_mask\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(features.label, dtype=torch.float),\n",
        "            'seq_ids': torch.tensor(seq_id, dtype=torch.int),\n",
        "            'positions': torch.tensor(pos, dtype=torch.int)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8B3bjtS4LO"
      },
      "source": [
        "### Supervised: BERT classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjMdI11yAMmG"
      },
      "source": [
        "####Training functions (fonti: [esempio1](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb), [esempio2](https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=6O_NbXFGMukX))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjgcRrvJIa5-"
      },
      "outputs": [],
      "source": [
        "class ClearMemory(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def setup_training(train_data_size, log_fp):\n",
        "    # Set up epochs and steps\n",
        "    steps_per_epoch = int(train_data_size / config_dict['TRAIN_BATCH_SIZE'])\n",
        "    num_train_steps = steps_per_epoch * config_dict['EPOCHS']\n",
        "    warmup_steps = int(config_dict['EPOCHS'] * train_data_size * 0.1 / config_dict['TRAIN_BATCH_SIZE'])\n",
        "\n",
        "    # creates an optimizer and learning rate scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = config_dict['LR'], # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = num_train_steps)\n",
        "    # optimizer = nlp.optimization.create_optimizer(\n",
        "    #     2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "    # Set the seed value all over the place to make this reproducible.\n",
        "    seed_val = 42\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    log_fp.write(\"\\nTraining setup:\\n\")\n",
        "    log_fp.write(\"===============\\n\")\n",
        "    log_fp.write(f\"\\tsteps_per_epoch = {steps_per_epoch}\\n\")\n",
        "    log_fp.write(f\"\\tnum_train_steps = {num_train_steps}\\n\")\n",
        "    log_fp.write(f\"\\twarmup_steps = {warmup_steps}\\n\")\n",
        "    log_fp.write(f\"\\toptimizer = {optimizer}\\n\")\n",
        "    log_fp.write(f\"\\tscheduler = {scheduler}\\n\")\n",
        "    log_fp.write(f\"\\tseed_val = {seed_val}\\n\")\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    if config_dict['N_CLASSES'] > 2:\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        return criterion(outputs.logits, targets.to(torch.long))\n",
        "    else:\n",
        "        return outputs.loss\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK9-HCP7IhuQ"
      },
      "source": [
        "Training and validation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17o-i3hgM3sA"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, optimizer, scheduler, log_fp):\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "    train_steps_loss = []\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader, 0):\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.float)\n",
        "        seq_ids = batch['seq_ids'].to(device, dtype = torch.int)\n",
        "        y_onehot = torch.nn.functional.one_hot(targets.long(), num_classes=config_dict['N_CLASSES'])\n",
        "        y_onehot = y_onehot.float()\n",
        "\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward`\n",
        "        # function and pass down the arguments. The `forward` function is\n",
        "        # documented here:\n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        outputs = model(ids,\n",
        "                       token_type_ids=None,\n",
        "                       attention_mask=mask,\n",
        "                       labels=y_onehot,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # Save hidden states of last layer for clustering\n",
        "        # hidden_states = Tuple of torch.FloatTensor (one for the output of the embeddings\n",
        "        # + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
        "        # last_hidden_states = outputs.hidden_states[12]\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5}  of  {:>5}.    Elapsed: {:}.     Loss:  {:}.'.format(step, len(train_dataloader), elapsed, loss.item()))\n",
        "            log_fp.write(('  Batch {:>5}  of  {:>5}.    Elapsed: {:}.     Loss:  {:}.\\n'.format(step, len(train_dataloader), elapsed, loss.item())))\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += float(loss.item())\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            train_steps_loss.append(float(loss.item()))\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "    log_fp.write(\"  Average training loss: {0:.2f}\\n\".format(avg_train_loss))\n",
        "    log_fp.write(\"  Training epoch took: {:}\\n\".format(training_time))\n",
        "\n",
        "    return avg_train_loss, training_time, train_steps_loss\n",
        "\n",
        "\n",
        "def validation(val_dataloader, log_fp):\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    final_data = {'seq_ids' : [],\n",
        "                  'positions' : [],\n",
        "                  'targets' : [],\n",
        "                  'outputs' : [],\n",
        "                  'last_hidden_states_dict' : {}\n",
        "                  }\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.float)\n",
        "        seq_ids = batch['seq_ids'].to('cpu').numpy()\n",
        "        positions = batch['positions'].to('cpu').numpy()\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            y_onehot = torch.nn.functional.one_hot(targets.long(), num_classes=config_dict['N_CLASSES'])\n",
        "            y_onehot = y_onehot.float()\n",
        "            outputs = model(ids,\n",
        "                            attention_mask=mask,\n",
        "                            labels=y_onehot,\n",
        "                            return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the\n",
        "        # output values prior to applying an activation function like the\n",
        "        # softmax.\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += float(loss.item())\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = targets.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        final_data['targets'].extend(y_onehot.cpu().detach().numpy().tolist())\n",
        "        if config_dict['N_CLASSES'] > 2:\n",
        "            final_data['outputs'].extend(torch.softmax(outputs.logits, dim=1).cpu().detach().numpy().tolist())\n",
        "        else:\n",
        "            final_data['outputs'].extend(torch.sigmoid(outputs.logits).cpu().detach().numpy().tolist())\n",
        "        final_data['seq_ids'].extend(seq_ids)\n",
        "        final_data['positions'].extend(positions)\n",
        "\n",
        "        # Save hidden states of last layer for clustering\n",
        "        # hidden_states = Tuple of torch.FloatTensor (one for the output of the embeddings\n",
        "        # + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
        "        last_hidden_states = outputs.hidden_states[12].cpu().detach().numpy()\n",
        "        for i, seq_id in enumerate(seq_ids):\n",
        "            final_data['last_hidden_states_dict']['seq_id'] = last_hidden_states[i]\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    log_fp.write(\"  Validation Accuracy: {0:.2f}\\n\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    log_fp.write(\"  Validation Loss: {0:.2f}\\n\".format(avg_val_loss))\n",
        "    log_fp.write(\"  Validation took: {:}\\n\".format(validation_time))\n",
        "\n",
        "    return final_data, avg_val_loss, avg_val_accuracy, validation_time\n",
        "\n",
        "\n",
        "def supervisedBertClassifierFinetune(train_dataloader, val_dataloader, train_data_size, log_fp):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    optimizer, scheduler = setup_training(train_data_size, log_fp)\n",
        "\n",
        "    training_stats = []\n",
        "    train_steps_loss = []\n",
        "    total_t0 = time.time()\n",
        "    log_fp.write(\"\\nTraining and validation:\\n\")\n",
        "    log_fp.write(\"==========================\\n\")\n",
        "\n",
        "    for epoch in range(config_dict['EPOCHS']):\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, config_dict['EPOCHS']))\n",
        "        print('Training...')\n",
        "        log_fp.write('\\n======== Epoch {:} / {:} ========\\n'.format(epoch + 1, config_dict['EPOCHS']))\n",
        "        log_fp.write(\"\\nTraining:\\n\")\n",
        "        avg_train_loss, training_time, train_steps_loss_epoch = train(train_dataloader, optimizer, scheduler, log_fp)\n",
        "        train_steps_loss.extend(train_steps_loss_epoch)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Validation...\")\n",
        "        log_fp.write(\"\\nValidation:\\n\")\n",
        "        final_data, avg_val_loss, avg_val_accuracy, validation_time = validation(val_dataloader, log_fp)\n",
        "\n",
        "        training_stats_epoch = {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time,\n",
        "            'final_data':final_data\n",
        "        }\n",
        "        training_stats.append(training_stats_epoch)\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "    log_fp.write(\"Total training took {:} (h:mm:ss)\\n\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "    # return train hidden_states from last epoch\n",
        "    return training_stats, train_steps_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCYb9yvaE1BL"
      },
      "source": [
        "#### Finetuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAReZQwFVHo0"
      },
      "outputs": [],
      "source": [
        "if do_training and do_finetuning:\n",
        "    with open(train_file) as train_fp, open(val_file) as val_fp, open(log_file, 'a') as log_fp:\n",
        "\n",
        "        train_reader = csv.reader(train_fp, delimiter=',')\n",
        "        train_metadata = {'len':sizes_info['train_data_size_seqs']}\n",
        "        X_train_generator = DatasetGenerator(train_reader, train_fp, train_metadata)\n",
        "        train_dataloader = DataLoader(\n",
        "                X_train_generator,  # The training samples.\n",
        "                sampler = RandomSampler(X_train_generator), # Select batches randomly\n",
        "                batch_size = config_dict['TRAIN_BATCH_SIZE'], # Trains with this batch size.\n",
        "                num_workers = 0\n",
        "        )\n",
        "\n",
        "        val_reader = csv.reader(val_fp, delimiter=',')\n",
        "        val_metadata = {'len':sizes_info['val_data_size_seqs']}\n",
        "        X_val_generator = DatasetGenerator(val_reader, val_fp, val_metadata) #.batch(EVAL_BATCH_SIZE)\n",
        "\n",
        "\n",
        "        validation_dataloader = DataLoader(\n",
        "                X_val_generator, # The validation samples.\n",
        "                sampler = SequentialSampler(X_val_generator), # Pull out batches sequentially.\n",
        "                batch_size = config_dict['EVAL_BATCH_SIZE'], # Evaluate with this batch size.\n",
        "                num_workers = 0\n",
        "        )\n",
        "\n",
        "        training_stats, train_steps_loss = supervisedBertClassifierFinetune(train_dataloader, validation_dataloader, sizes_info['train_data_size_seqs'], log_fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8qTYoZYl7xy"
      },
      "source": [
        "#### Save model and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNMO8WVnyqAE"
      },
      "outputs": [],
      "source": [
        "# save or load model for inference:\n",
        "if do_training or do_test or config_dict['TASK_TYPE']=='one_vs_all_classification':\n",
        "    if do_finetuning:\n",
        "        mod_f = model_file_finetuned\n",
        "    else:\n",
        "        mod_f = model_file\n",
        "    if os.path.exists(mod_f):\n",
        "        # model.load_state_dict(torch.load(mod_f, map_location=torch.device('cpu')))\n",
        "        # device = torch.device(\"cpu\")\n",
        "        model.load_state_dict(torch.load(mod_f))\n",
        "    else:\n",
        "        torch.save(model.state_dict(), mod_f)\n",
        "\n",
        "if do_test or config_dict['TASK_TYPE']=='one_vs_all_classification':\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmIVTEGD_y19"
      },
      "outputs": [],
      "source": [
        "# save or load last epoch validation data:\n",
        "if os.path.exists(final_val_outputs_file):\n",
        "    final_data = pickle.load(open(final_val_outputs_file, 'rb'))\n",
        "else:\n",
        "    final_data = training_stats[-1]['final_data']\n",
        "    # save output data on file for furure computation:\n",
        "    pickle.dump(final_data, open(final_val_outputs_file, 'wb'))\n",
        "    # pickle.dump(str(final_data), open(final_val_outputs_file, 'wb'))\n",
        "\n",
        "\n",
        "# save or load last epoch validation data:\n",
        "if os.path.exists(training_stats_file):\n",
        "    training_stats = pickle.load(open(training_stats_file, 'rb'))\n",
        "else:\n",
        "    # save output data on file for furure computation:\n",
        "    pickle.dump(training_stats, open(training_stats_file, 'wb'))\n",
        "    # pickle.dump(str(final_data), open(final_val_outputs_file, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51iQ6z5tfd2G"
      },
      "source": [
        "#### Validation statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJy0DrxgkwVN"
      },
      "source": [
        "Cluster last hidden states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1ELwLhBkvst"
      },
      "outputs": [],
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "# import matplotlib.pyplot as plt # graphs plotting\n",
        "# import matplotlib.cm as cm\n",
        "# import seaborn as sns\n",
        "\n",
        "# print(\"Starting t-SNE\")\n",
        "\n",
        "# X_embedded = TSNE(n_components = 2, perplexity = 30, random_state = 1).fit_transform()\n",
        "\n",
        "# print(\"Writting File!!!\")\n",
        "\n",
        "# write_path_11 = \"/alina-data1/sarwan/IEEE_BigData/Dataset/t_sne_plot_2_dim.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eXM0LRUmVAR"
      },
      "source": [
        "Statistics and plots functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQrkHYfqpnjk"
      },
      "outputs": [],
      "source": [
        "from pandas._config import config\n",
        "def show_train_stats_and_plots(training_stats, train_steps_loss):\n",
        "    print('Results:')\n",
        "    # Display floats with two decimal places.\n",
        "    pd.set_option('precision', 2)\n",
        "\n",
        "    # Create a DataFrame from our training statistics.\n",
        "    df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "    # Use the 'epoch' as the row index.\n",
        "    df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "    # A hack to force the column headers to wrap.\n",
        "    #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "    # Display the table.\n",
        "    print(df_stats)\n",
        "\n",
        "    # Use plot styling from seaborn.\n",
        "    sns.set(style='darkgrid')\n",
        "\n",
        "    # Increase the plot size and font size.\n",
        "    sns.set(font_scale=1.5)\n",
        "    fig = plt.figure(1, figsize=(12,6))\n",
        "    #plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "    # Plot the learning curve.\n",
        "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "    # Label the plot.\n",
        "    plt.title(\"Average Training & Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "    plt.show()\n",
        "    fig_path = Path(outputs_dir) / 'trainval.jpg'\n",
        "    fig.savefig(fig_path)\n",
        "\n",
        "    fig2 = plt.figure(2, figsize=(30,6))\n",
        "    plt.plot(train_steps_loss, 'b', label=\"Training steps loss (every 40 steps)\")\n",
        "\n",
        "    # Label the plot.\n",
        "    plt.title(\"Training steps loss\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "    fig_path2 = Path(outputs_dir) / 'train_steps.jpg'\n",
        "    fig2.savefig(fig_path2)\n",
        "\n",
        "\n",
        "def confusion_matrix_plot(targets_labels, outputs_labels, path, taskname = \"\"):\n",
        "    confusion_matr = metrics.confusion_matrix(targets_labels, outputs_labels)\n",
        "    plt.figure()\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    sns.heatmap(confusion_matr, annot=True, fmt=\"d\", cmap=\"viridis\", annot_kws={\"fontsize\":20})\n",
        "    plt.title(f'{taskname} Confusion Matrix', fontsize=25)\n",
        "    plt.ylabel('True label', fontsize=21)\n",
        "    plt.xlabel('Predicted label', fontsize=21)\n",
        "    curr_xticks, curr_xlabels = plt.xticks()\n",
        "    plt.xticks(curr_xticks, labels=[inv_class_labels_dict[int(t.get_text())] for t in curr_xlabels], rotation=90, fontsize=20)\n",
        "    curr_yticks, curr_ylabels = plt.yticks()\n",
        "    plt.yticks(curr_yticks, labels=[inv_class_labels_dict[int(t.get_text())] for t in curr_ylabels], rotation=0, fontsize=20)\n",
        "    cax = plt.gcf().axes[-1]\n",
        "    cax.tick_params(labelsize=20)\n",
        "    fig_path = Path(path) / f\"{taskname.replace(' ', '_')}_confusion_matrix.jpg\"\n",
        "    fig = plt.gcf()\n",
        "    fig.savefig(fig_path)\n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "\n",
        "\n",
        "def plot_PRC(y, y_score, y_pred, path):\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.gca()\n",
        "    ax.set_xlim([-0.1, 1.1])\n",
        "    ax.set_ylim([-0.1, 1.1])\n",
        "    ax.grid(True)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.legend(loc='lower right')\n",
        "    ax.set_title(f\"Precision-Recall Curves\")\n",
        "    # metrics.PrecisionRecallDisplay.from_predictions(y, y_score, ax=ax, )\n",
        "    # precision = dict()\n",
        "    # recall = dict()\n",
        "    # average_precision = dict()\n",
        "    # for i in range(config_dict['N_CLASSES']):\n",
        "    #     precision[i], recall[i], _ = metrics.precision_recall_curve(y[:],\n",
        "    #                                                         y_score[:, i])\n",
        "    #     average_precision[i] = metrics.average_precision_score(y[:], y_score[:, i])\n",
        "\n",
        "    # A \"micro-average\": quantifying score on all classes jointly\n",
        "    # precision[\"micro\"], recall[\"micro\"], _ = metrics.precision_recall_curve(y,\n",
        "    #     y_score)\n",
        "    # average_precision[\"micro\"] = metrics.average_precision_score(y, y_score,\n",
        "    #                                                     average=\"micro\")\n",
        "    # print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
        "    #     .format(average_precision[\"micro\"]))\n",
        "    # display = metrics.PrecisionRecallDisplay(\n",
        "    #     recall=recall[\"micro\"],\n",
        "    #     precision=precision[\"micro\"],\n",
        "    #     average_precision=average_precision[\"micro\"])\n",
        "    # display.plot()\n",
        "    # _ = display.ax_.set_title(f\"Precision-Recall curve micro-averaged over all classes (AP={average_precision['micro']})\")\n",
        "    def multiclass_prc(y_test, y_score, y_pred, average=\"micro\"):\n",
        "        lb = LabelBinarizer()\n",
        "        lb.fit(y_test)\n",
        "        y_test = lb.transform(y_test)\n",
        "        y_pred = lb.transform(y_pred)\n",
        "\n",
        "        for (idx, c_label) in enumerate(config_dict['CLASS_LABELS'].keys()):\n",
        "            display = metrics.PrecisionRecallDisplay.from_predictions(y_test[:,idx].astype(int), y_pred[:,idx], name=c_label)\n",
        "            display.plot(ax=ax)\n",
        "        # ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
        "        return metrics.average_precision_score(y_test, y_pred, average=average)\n",
        "    print('AP score:', multiclass_prc(y, y_score, y_pred))\n",
        "    ax.set_xlabel('Recall')\n",
        "    ax.set_ylabel('Precision')\n",
        "    # fig.show()\n",
        "    fig.savefig(Path(path) / 'prc.png')\n",
        "\n",
        "def ROC_curve_plot(targets_labels, outputs_labels, path, taskname = \"\", figsize=(17, 6)):\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    labels_dict = {class_label:[[],[]] for class_label in config_dict['CLASS_LABELS'].values()}\n",
        "    for i in range(len(targets_labels)):\n",
        "        labels_dict[targets_labels[i]][0].append(targets_labels[i])\n",
        "        labels_dict[targets_labels[i]][1].append(outputs_labels[i])\n",
        "\n",
        "    for class_label, tgt_out_labels in labels_dict.items():\n",
        "        print(\"CLASSSSS\")\n",
        "        print(class_label)\n",
        "        print(tgt_out_labels[0])\n",
        "        print(tgt_out_labels[1])\n",
        "        fpr[class_label], tpr[class_label], _ = metrics.roc_curve(tgt_out_labels[0], tgt_out_labels[1], pos_label=class_label)\n",
        "        roc_auc[class_label] = metrics.auc(fpr[class_label], tpr[class_label])\n",
        "\n",
        "    # roc for each class\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('Receiver operating characteristic example')\n",
        "    for class_label in labels_dict.keys():\n",
        "        ax.plot(fpr[class_label], tpr[class_label], label='ROC curve (AUC = %0.2f) for class %i' % (roc_auc[class_label], class_label))\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.grid(alpha=.4)\n",
        "    sns.despine()\n",
        "    plt.show()\n",
        "\n",
        "def final_statistics(target_labels, output_labels, output_logits, log_file, taskname, logits=None, target_names=None):\n",
        "    accuracy = metrics.accuracy_score(target_labels, output_labels)\n",
        "    f1_score_micro = metrics.f1_score(target_labels, output_labels, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(target_labels, output_labels, average='macro')\n",
        "    if target_names==None:\n",
        "        target_names = config_dict['CLASS_LABELS']\n",
        "    specificity = {}\n",
        "    for l_name, l in target_names.items():\n",
        "        prec,recall,_,_ = metrics.precision_recall_fscore_support(target_labels==l,\n",
        "                                                        output_labels==l,\n",
        "                                                        pos_label=True,average=None)\n",
        "        specificity[l_name] = recall[0]\n",
        "    classification_report = metrics.classification_report(target_labels, output_labels, target_names=target_names.keys())\n",
        "    confusion_matr = metrics.confusion_matrix(target_labels, output_labels)\n",
        "    print(f'{taskname}:')\n",
        "    print(f\"===============================\")\n",
        "    print(f\"Accuracy Score = {accuracy}\")\n",
        "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
        "    print(f\"Specificity:\\n{json.dumps(specificity, indent=4)}\\n\")\n",
        "    print(f\"Classification report:\\n{str(classification_report)}\")\n",
        "    confusion_matrix_plot(target_labels, output_labels, outputs_dir, taskname)\n",
        "    # plot_PRC(target_labels, output_logits, output_labels, outputs_dir)\n",
        "    with open(log_file, 'a') as log_fp:\n",
        "        log_fp.write(f\"\\n{taskname}:\\n\")\n",
        "        log_fp.write(f\"===============================\\n\")\n",
        "        log_fp.write(f\"Accuracy Score = {accuracy}\\n\")\n",
        "        log_fp.write(f\"F1 Score (Micro) = {f1_score_micro}\\n\")\n",
        "        log_fp.write(f\"F1 Score (Macro) = {f1_score_macro}\\n\")\n",
        "        log_fp.write(f\"Classification report:\\n{classification_report}\\n\")\n",
        "        log_fp.write(f\"Specificity:\\n{json.dumps(specificity, indent=4)}\\n\")\n",
        "        log_fp.write(f\"Confusion matrix:\\n{confusion_matr}\\n\")\n",
        "    if logits:\n",
        "        logits_pos_score = [x[target_labels[i]] for i,x in enumerate(logits)]\n",
        "        ROC_curve_plot(target_labels, logits_pos_score, outputs_dir, taskname)\n",
        "\n",
        "def find_best_positions(final_data, min_score=0.8, threshold=0.5):\n",
        "    best_positions = None\n",
        "\n",
        "    outputs_labels=np.argmax(final_data['outputs'],axis=1)\n",
        "    targets_labels=np.argmax(final_data['targets'],axis=1)\n",
        "\n",
        "    # analisys according to position in sequence\n",
        "    final_data['select_pred'] = []\n",
        "    final_data['count_pred'] = []\n",
        "    for target,pred,pred_score in zip(targets_labels,outputs_labels, final_data['outputs']):\n",
        "        # select only predictions which are correct and with score > min_score (*)\n",
        "        final_data['select_pred'].append(int(target == pred and pred_score[np.argmax(pred_score)]>min_score))\n",
        "        final_data['count_pred'].append(1)\n",
        "    final_data_df = pd.DataFrame(final_data).drop(columns=['outputs', 'targets','seq_ids'])\n",
        "    grouped_df = final_data_df.groupby(['positions']).agg({'select_pred': 'sum', 'count_pred': 'count'})\n",
        "\n",
        "    # for each position, calculate percentage of chunks which satisfy conditions (see (*))\n",
        "    grouped_df['percents'] = grouped_df['select_pred'] / grouped_df['count_pred']\n",
        "    # select only positions which have a percentage of chunks which satisfy conditions (see (*)) > threshold\n",
        "    best_positions = [i for i, perc in enumerate(np.asarray(grouped_df['percents'])) if perc > threshold]\n",
        "\n",
        "    print(f\"Best positions: {best_positions}\")\n",
        "    with open(log_file, 'a') as log_fp:\n",
        "        log_fp.write(f\"\\nBest positions (i.e. those with >{threshold*100}% of chunks with correct prediction and score>{min_score}):\\n\")\n",
        "        log_fp.write(f\"=======================================================================================\\n\")\n",
        "        log_fp.write(f\"{best_positions}\\n\")\n",
        "\n",
        "    plot = grouped_df['percents'].plot.bar(figsize=(20,10))\n",
        "    fig = plot.get_figure()\n",
        "    fig.suptitle(f\"For each position, percentage of chunks with correct prediction and score>{min_score}\", y=0.95, fontsize=20)\n",
        "    plt.yticks(np.arange(0, 1, 0.1))\n",
        "    plt.grid()\n",
        "    plt.axhline(y=threshold, color='r', linestyle='-')\n",
        "    fig_path = Path(outputs_dir) / f\"percent_pos.jpg\"\n",
        "    fig.savefig(fig_path)\n",
        "    plt.show()\n",
        "\n",
        "    return best_positions\n",
        "\n",
        "def per_sample_result_computation(final_data, best_positions, min_score=0.8, taskname='', filter_positions=True, filter_score=True):\n",
        "    targets_dict = {}\n",
        "    preds_dict = {}\n",
        "    for id,target in zip(final_data['seq_ids'],final_data['targets']):\n",
        "        if id not in targets_dict:\n",
        "            targets_dict[id] = np.argmax(target)\n",
        "        if id not in preds_dict:\n",
        "            preds_dict[id] = {'counts':[], 'outputs_label':[], 'prediction': []}\n",
        "\n",
        "\n",
        "    final_data['select_pred'] = []\n",
        "    final_data['outputs_label'] = []\n",
        "    for pos,pred in zip(final_data['positions'],final_data['outputs']):\n",
        "        # select prediction only if it is in best position and if prediction is certain (score>min_score) (*)\n",
        "        if filter_positions:\n",
        "            if filter_score:\n",
        "                final_data['select_pred'].append(int(pos in best_positions and pred[np.argmax(pred)]>min_score))\n",
        "            else:\n",
        "                final_data['select_pred'].append(int(pos in best_positions))\n",
        "        else:\n",
        "            if filter_score:\n",
        "                final_data['select_pred'].append(int(pred[np.argmax(pred)]>min_score))\n",
        "            else:\n",
        "                final_data['select_pred'].append(int(pred[np.argmax(pred)]>0))\n",
        "        final_data['outputs_label'].append(np.argmax(pred))\n",
        "\n",
        "    final_data_df = pd.DataFrame(final_data)\n",
        "    # filter only selected predictions according to conditions (see (*))\n",
        "    filtered_data_df = final_data_df[final_data_df['select_pred'] > 0]\n",
        "    filtered_data_df = filtered_data_df[['seq_ids','outputs_label']]\n",
        "    grouped_sample_data_df = pd.DataFrame(filtered_data_df).groupby(['seq_ids','outputs_label']).size().reset_index(name='counts')\n",
        "    grouped_sample_data_dict = grouped_sample_data_df.to_dict('list')\n",
        "\n",
        "    for seq_id,output_l,count in zip(grouped_sample_data_dict['seq_ids'], grouped_sample_data_dict['outputs_label'], grouped_sample_data_dict['counts']):\n",
        "        preds_dict[seq_id]['outputs_label'].append(output_l)\n",
        "        preds_dict[seq_id]['counts'].append(count)\n",
        "\n",
        "\n",
        "    for seq_id in preds_dict.keys():\n",
        "        counts_sorted = preds_dict[seq_id]['counts'].copy()\n",
        "        counts_sorted.sort(reverse=True)\n",
        "        if len(preds_dict[seq_id]['counts'])==0 or (len(counts_sorted)>1 and all(element == counts_sorted[0] for element in counts_sorted)):\n",
        "            preds_dict[seq_id]['prediction'] = 'uncertain'\n",
        "        else:\n",
        "            majority_class_index = np.argmax(preds_dict[seq_id]['counts'])\n",
        "            preds_dict[seq_id]['prediction'] = preds_dict[seq_id]['outputs_label'][majority_class_index]\n",
        "\n",
        "    correct_pred_count = 0\n",
        "    uncertain_pred_count = 0\n",
        "    tot_count = 0\n",
        "    target_labels = []\n",
        "    output_labels = []\n",
        "\n",
        "\n",
        "    with open(log_file, 'a') as log_fp:\n",
        "        for seq_id in targets_dict.keys():\n",
        "            tot_count += 1\n",
        "            if preds_dict[seq_id]['prediction'] == 'uncertain':\n",
        "                uncertain_pred_count += 1\n",
        "            else:\n",
        "                target_labels.append(targets_dict[seq_id])\n",
        "                output_labels.append(preds_dict[seq_id]['prediction'])\n",
        "                if targets_dict[seq_id] == preds_dict[seq_id]['prediction']:\n",
        "                    correct_pred_count += 1\n",
        "            print(f\"seq: {seq_id}\\t target: {targets_dict[seq_id]}\\t predicted: {preds_dict[seq_id]['prediction']}\")\n",
        "            log_fp.write(f\"seq: {seq_id}\\t target: {targets_dict[seq_id]}\\t predicted: {preds_dict[seq_id]['prediction']}\\n\")\n",
        "\n",
        "    final_statistics(target_labels, output_labels, log_file, f\"{taskname} grouped by samples, filter_positions={filter_positions}, filter_score={filter_score}, min_score={min_score}\")\n",
        "\n",
        "    print(f\"Correct predictions: {correct_pred_count}/{tot_count} -> {correct_pred_count/tot_count}\")\n",
        "    print(f\"Wrong predictions: {tot_count-correct_pred_count-uncertain_pred_count}/{tot_count} -> {(tot_count-correct_pred_count-uncertain_pred_count)/tot_count}\")\n",
        "    print(f\"Uncertain predictions: {uncertain_pred_count}/{tot_count} -> {uncertain_pred_count/tot_count}\")\n",
        "\n",
        "    with open(log_file, 'a') as log_fp:\n",
        "        log_fp.write(f\"Correct predictions: {correct_pred_count}/{tot_count} -> {correct_pred_count/tot_count}\\n\")\n",
        "        log_fp.write(f\"Wrong predictions: {tot_count-correct_pred_count-uncertain_pred_count}/{tot_count} -> {(tot_count-correct_pred_count-uncertain_pred_count)/tot_count}\\n\")\n",
        "        log_fp.write(f\"Uncertain predictions: {uncertain_pred_count}/{tot_count} -> {uncertain_pred_count/tot_count}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFshYJ6cdQzn"
      },
      "outputs": [],
      "source": [
        "if do_training:\n",
        "    show_train_stats_and_plots(training_stats, train_steps_loss)\n",
        "\n",
        "    output_labels=np.argmax(final_data['outputs'],axis=1)\n",
        "    target_labels=np.argmax(final_data['targets'],axis=1)\n",
        "    final_statistics(target_labels, output_labels, final_data['outputs'], log_file, 'Global Validation')\n",
        "\n",
        "    if config_dict['SPLIT_DATA_IN_CHUNKS']:\n",
        "        #best_positions = find_best_positions(final_data, min_score=0.8, threshold=0.75)\n",
        "        #per_sample_result_computation(final_data, best_positions, taskname=\"Validation\")\n",
        "        per_sample_result_computation(final_data, best_positions, taskname=\"Validation\", filter_positions=True, filter_score=True, min_score=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJWuuDabaE7k"
      },
      "outputs": [],
      "source": [
        "# show_train_stats_and_plots(training_stats, train_steps_loss)\n",
        "\n",
        "# output_labels=np.argmax(final_data['outputs'],axis=1)\n",
        "# target_labels=np.argmax(final_data['targets'],axis=1)\n",
        "# output_logits = final_data['outputs']\n",
        "# final_statistics(target_labels, output_labels, output_logits, log_file, 'Validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFgDDhbkVBfP"
      },
      "source": [
        "####Attention matrices functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNMscXlkVANP"
      },
      "outputs": [],
      "source": [
        "def get_attentions(attentions, sample_idx_in_batch, layer=0, attention_head=0, sum=False):\n",
        "  '''\n",
        "  get the particular output for a particular layer and attention head\n",
        "  layer -> 0 to 11\n",
        "  attention_head -> 0 to 11\n",
        "  '''\n",
        "  if sum:\n",
        "    #avg over all attention heads in a layer\n",
        "    return attentions[layer][sample_idx_in_batch].sum(dim=0).cpu().detach().numpy()\n",
        "\n",
        "  #return values for a particular attention head inside a specific layer\n",
        "  return attentions[layer][sample_idx_in_batch][attention_head].cpu().detach().numpy()\n",
        "\n",
        "\n",
        "def min_max_scale(X, range=(0, 1)):\n",
        "    mi, ma = range\n",
        "    X_std = (X - X.min()) / (X.max() - X.min())\n",
        "    X_scaled = X_std * (ma - mi) + mi\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def calculate_weight_domain_proportion_by_head(attentions_sum, domain, proportion, theta=0):\n",
        "    if domain not in domain_coordinates_1based:\n",
        "        raise ValueError(\"plt_attentions_domain: domain must be one of %r.\" % domain_coordinates_1based)\n",
        "\n",
        "    domain_start_idx, domain_end_idx = convert_domain_coords_in_token_indices(domain)\n",
        "\n",
        "    # proportion of attention weights in the selected domain from each head that connects all amino acids to those in the selected domain\n",
        "    #weight_total_by_head = torch.zeros((n_layers, n_heads), dtype=torch.double)\n",
        "    if proportion == \"attention\":\n",
        "        # Update total attention_analysis weights per head. Sum over from_index (dim 2), to_index (dim 3)\n",
        "        weight_total_by_head = attentions_sum.sum((2, 3))\n",
        "        # Update weighted sum of feature values per head\n",
        "        weight_domain_sum_by_head = attentions_sum[:, :, :, domain_start_idx:domain_end_idx+1].sum((2, 3))\n",
        "    elif proportion == \"high-confidence-attention\":\n",
        "        attentions_sum_total_masked = ma.masked_where(attentions_sum <= theta, attentions_sum)\n",
        "        ma.set_fill_value(attentions_sum_total_masked, 0)\n",
        "        weight_total_by_head = attentions_sum_total_masked.sum((2, 3))\n",
        "        attentions_sum_domain = attentions_sum[:, :, :, domain_start_idx:domain_end_idx+1]\n",
        "        attentions_sum_domain_masked = ma.masked_where(attentions_sum_domain <= theta, attentions_sum_domain)\n",
        "        ma.set_fill_value(attentions_sum_domain_masked, 0)\n",
        "        weight_domain_sum_by_head = attentions_sum_domain_masked.sum((2, 3))\n",
        "    elif proportion == \"high-attention-tokens-count\":\n",
        "        mask_high_attn_total = attentions_sum > theta\n",
        "        weight_total_by_head = mask_high_attn_total.long().sum((2, 3))\n",
        "        mask_high_attn_domain = attentions_sum[:, :, :, domain_start_idx:domain_end_idx+1] > theta\n",
        "        weight_domain_sum_by_head = mask_high_attn_domain.long().sum((2, 3))\n",
        "    else:\n",
        "        raise ValueError(\"plt_attentions_domain: proportion must be one of %r.\" % [\"attention\", \"high-attention-tokens\"])\n",
        "\n",
        "    return weight_domain_sum_by_head / weight_total_by_head, weight_domain_sum_by_head, weight_total_by_head\n",
        "\n",
        "\n",
        "def plt_attentions_domain(attentions_sum, domain, dir_path, filename, proportion, theta=0, min_total=0, fig_size=(130,100), title=None):\n",
        "    if domain not in domain_coordinates_1based:\n",
        "        raise ValueError(\"plt_attentions_domain: domain must be one of %r.\" % domain_coordinates_1based)\n",
        "\n",
        "    weight_domain_proportion_by_head, weight_sum, weight_total = calculate_weight_domain_proportion_by_head(attentions_sum, domain, proportion, theta=theta)\n",
        "\n",
        "    exclude_mask = np.array(weight_total) < min_total\n",
        "    masked_weight_domain_proportion = np.ma.masked_array(weight_domain_proportion_by_head, mask=exclude_mask)\n",
        "    layer_max = np.asarray(masked_weight_domain_proportion).max(-1)\n",
        "\n",
        "    fig = plt.figure(figsize=(5, 3.5)) #plt.figure(figsize=(3, 2.2))\n",
        "    fig.suptitle(title, fontsize=13, fontweight='bold')\n",
        "    ax1 = plt.subplot2grid((100, 85), (0, 0), colspan=65, rowspan=99)  # Heatmap\n",
        "    ax2 = plt.subplot2grid((100, 85), (12, 70), colspan=15, rowspan=75)  # Barchart\n",
        "\n",
        "    xtick_labels = [str(i) if i % 2 == 0 else '' for i in range(1, n_heads + 1)]\n",
        "    ytick_labels = [str(i) if i % 2 == 0 else '' for i in range(1, n_layers + 1)]\n",
        "    heatmap = sns.heatmap((masked_weight_domain_proportion * 100).tolist(), center=0.0, ax=ax1,\n",
        "                          square=True, cbar=False, linewidth=0.1, linecolor='#D0D0D0',\n",
        "                          cmap=LinearSegmentedColormap.from_list('rg', [\"#F14100\", \"white\", \"#5a3dc4\"], N=256),\n",
        "                          mask=exclude_mask,\n",
        "                          xticklabels=xtick_labels,\n",
        "                          yticklabels=ytick_labels)\n",
        "    for _, spine in heatmap.spines.items():\n",
        "        spine.set_visible(True)\n",
        "        spine.set_edgecolor('#D0D0D0')\n",
        "        spine.set_linewidth(0.1)\n",
        "    plt.setp(heatmap.get_yticklabels(), fontsize=11)\n",
        "    plt.setp(heatmap.get_xticklabels(), fontsize=11)\n",
        "    heatmap.tick_params(axis='x', pad=1, length=2)\n",
        "    heatmap.tick_params(axis='y', pad=.5, length=2)\n",
        "    heatmap.yaxis.labelpad = 3\n",
        "    heatmap.invert_yaxis()\n",
        "    heatmap.set_facecolor('#E7E6E6')\n",
        "    # split axes of heatmap to put colorbar\n",
        "    ax_divider = make_axes_locatable(ax1)\n",
        "    cax = ax_divider.append_axes('left', size='7%', pad='33%')\n",
        "    # # make colorbar for heatmap.\n",
        "    # # Heatmap returns an axes obj but you need to get a mappable obj (get_children)\n",
        "    cbar = colorbar(ax1.get_children()[0], cax=cax, orientation='vertical', format='%.0f%%')\n",
        "    cax.yaxis.set_ticks_position('left')\n",
        "    cbar.solids.set_edgecolor(\"face\")\n",
        "    cbar.ax.tick_params(labelsize=11, length=4, pad=2)\n",
        "    ax1.set_title('% Attention', size=12)\n",
        "    ax1.set_xlabel('Head', size=11)\n",
        "    ax1.set_ylabel('Layer', size=11)\n",
        "    for _, spine in ax1.spines.items():\n",
        "        spine.set_visible(True)\n",
        "    ax2.set_title('Max', size=12)\n",
        "    bp = sns.barplot(x=layer_max * 100, ax=ax2, y=list(range(layer_max.shape[0])), color=\"#5a3dc4\", orient=\"h\",\n",
        "                     edgecolor=\"none\")\n",
        "    formatter = FuncFormatter(lambda y, pos: '0' if (y == 0) else \"%d%%\" % (y))\n",
        "    ax2.xaxis.set_major_formatter(formatter)\n",
        "    plt.setp(bp.get_xticklabels(), fontsize=11)\n",
        "    bp.tick_params(axis='x', pad=1, length=3)\n",
        "    ax2.invert_yaxis()\n",
        "    ax2.set_yticklabels([])\n",
        "    ax2.spines['top'].set_visible(False)\n",
        "    ax2.spines['right'].set_visible(False)\n",
        "    ax2.spines['left'].set_visible(False)\n",
        "    ax2.xaxis.set_ticks_position('bottom')\n",
        "    ax2.axvline(0, linewidth=.85, color='black')\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig_path = Path(dir_path) / f'{filename}.jpg'\n",
        "    fig.savefig(fig_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def top_heads_attention_proportions(attentions_all_layers, proportion=\"attention\", theta=0, min_total=0):\n",
        "    max_attentive_heads_pvalues = []\n",
        "    inv_class_labels_dict = {v:k for k, v in config_dict['CLASS_LABELS'].items()}\n",
        "    if theta == None:\n",
        "        raise ValueError(\"top_heads_attention_proportions: theta: got None, expected float\")\n",
        "\n",
        "    top_heads_attention_pct = {}\n",
        "\n",
        "    for target_label in attentions_all_layers.keys():\n",
        "        attentions_sum = attentions_all_layers[target_label]\n",
        "        if inv_class_labels_dict[target_label] not in top_heads_attention_pct.keys():\n",
        "            top_heads_attention_pct[inv_class_labels_dict[target_label]] = {}\n",
        "        for domain in domain_coordinates_1based.keys():\n",
        "            domain_start_idx, domain_end_idx = convert_domain_coords_in_token_indices(domain)\n",
        "\n",
        "            # (1) the proportion of high-confidence attention arcs (αi,j > θ) for which f(i,j)=1 for top head\n",
        "            weight_domain_proportion_by_head, num_pos, num_total = calculate_weight_domain_proportion_by_head(attentions_sum, domain, proportion='high-attention-tokens-count', theta=theta)\n",
        "            exclude_mask = np.array(num_total) < min_total\n",
        "            masked_weight_domain_proportion_by_head = np.ma.masked_array(weight_domain_proportion_by_head, mask=exclude_mask)\n",
        "            top_head = np.unravel_index(np.argmax(masked_weight_domain_proportion_by_head, axis=None), masked_weight_domain_proportion_by_head.shape)\n",
        "            high_confidence_pct = masked_weight_domain_proportion_by_head[top_head[0],top_head[1]].item() * 100\n",
        "            num_pos = num_pos[top_head[0],top_head[1]].item()\n",
        "            num_total = num_total[top_head[0],top_head[1]].item()\n",
        "\n",
        "            # # (2) background: the proportion of all possible pairs i, j for which f(i,j)=1\n",
        "            if proportion == \"attention\" or proportion == \"high-confidence-attention\":\n",
        "                weight_domain_count_by_head = attentions_sum[:, :, :, domain_start_idx:domain_end_idx+1].sum((2, 3))\n",
        "                weight_total_count_by_head = attentions_sum.sum((2, 3))\n",
        "                num_pos_background = weight_domain_count_by_head[top_head[0],top_head[1]].item()\n",
        "                num_total_background = weight_total_count_by_head[top_head[0],top_head[1]].item()\n",
        "                background_pct = num_pos_background / num_total_background * 100\n",
        "            elif proportion == \"high-attention-tokens-count\":\n",
        "                mask_domain = torch.zeros(attentions_sum.shape, dtype=torch.bool)\n",
        "                mask_domain[:, :, :, domain_start_idx:domain_end_idx+1] = 1\n",
        "                weight_domain_count_by_head = mask_domain.long().sum((2, 3))\n",
        "                mask_total = torch.ones(attentions_sum.shape, dtype=torch.bool)\n",
        "                weight_total_count_by_head = mask_total.long().sum((2, 3))\n",
        "            else:\n",
        "                raise ValueError(\"plt_attentions_domain: proportion must be one of %r.\" % [\"attention\", \"high-attention-tokens\"])\n",
        "\n",
        "            # p-value for statistical significance\n",
        "            stat, p_value = proportions_ztest(np.asarray([num_pos_background, num_pos]), np.asarray([num_total_background, num_total]))\n",
        "            m = masked_weight_domain_proportion_by_head.shape[0]*masked_weight_domain_proportion_by_head.shape[1]\n",
        "            alpha = 0.05 / m    # Bonferroni adjustment\n",
        "            statistical_significance = p_value < alpha\n",
        "            max_attentive_heads_pvalues.append([inv_class_labels_dict[target_label], domain, f'{top_head[0]+1}-{top_head[1]+1}', high_confidence_pct, background_pct, f'{p_value:.25f}', statistical_significance])\n",
        "\n",
        "            top_heads_attention_pct[inv_class_labels_dict[target_label]][domain] = high_confidence_pct\n",
        "\n",
        "    df_top_heads_attention_pct = pd.DataFrame(top_heads_attention_pct)\n",
        "    df_top_heads_attention_pct['MEDIAN'] = df_top_heads_attention_pct.median(axis=1)\n",
        "    df_max_attentive_heads_pvalues = pd.DataFrame(max_attentive_heads_pvalues, columns=['Class', 'Domain', 'Top head', 'Attn %', 'Background %', 'p-value', f'p<{alpha:.8f}'])\n",
        "\n",
        "    return df_top_heads_attention_pct.sort_values(\"MEDIAN\", axis=0, ascending=False), df_max_attentive_heads_pvalues\n",
        "\n",
        "def show_attentions_for_each_domain(attentions_all_layers, dir_path, proportion=\"attention\"):\n",
        "    inv_class_labels_dict = {v:k for k, v in config_dict['CLASS_LABELS'].items()}\n",
        "    if proportion == 'high-attention-tokens-count':\n",
        "        theta = 0.3\n",
        "    else:\n",
        "        theta = None\n",
        "\n",
        "    for target_label in attentions_all_layers.keys():\n",
        "        for domain in domain_coordinates_1based.keys():\n",
        "            plt_attentions_domain(attentions_all_layers[target_label],\n",
        "                            domain,\n",
        "                            dir_path,\n",
        "                            filename=f\"{domain}_{inv_class_labels_dict[int(target_label)]}\",\n",
        "                            proportion=proportion,\n",
        "                            theta=theta,\n",
        "                            title=f\"Domain: {domain}, Class: {inv_class_labels_dict[int(target_label)]}\")\n",
        "\n",
        "\n",
        "def plt_attentions(mat, tick_labels, dir_path, filename=\"attention_matrix\", theta=0, fig_size=(130,100), annot=False, cmap=sns.color_palette(\"viridis_r\"), title=None):\n",
        "    '''\n",
        "    plot the NxN matrix passed as a heat map\n",
        "\n",
        "    mat: square matrix to visualize\n",
        "    tick_labels: labels for xticks and yticks (the tokens in our case)\n",
        "    '''\n",
        "\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f\"Directory '{dir_path}' created\")\n",
        "\n",
        "    #percentile_threshold = np.percentile(mat, percentile)\n",
        "    # mask_att_below_thresh = mat < mat.max() * threshold_ratio\n",
        "    # mask_att_above_thresh = mat >= mat.max() * threshold_ratio\n",
        "    mask_att_below_thresh = mat < theta\n",
        "    mask_att_above_thresh = mat >= theta\n",
        "\n",
        "    xs, ys = np.where(mask_att_above_thresh == True)\n",
        "    high_attention_positions = []\n",
        "    for x,y in zip(xs,ys):\n",
        "        high_attention_positions.append([tick_labels[x],tick_labels[y],mat[x,y]])\n",
        "    sorted_high_attention_positions = pd.DataFrame(high_attention_positions, columns=['Row','Col','Attn. Score']).sort_values(by=['Attn. Score'], ascending=False)\n",
        "    with open(f\"{Path(dir_path) / filename}.txt\", 'a') as fp:\n",
        "        fp.write(title+'\\n')\n",
        "        fp.write(sorted_high_attention_positions.to_string(index=False))\n",
        "        fp.write('\\n')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=fig_size)\n",
        "    ax = sns.heatmap(mat,\n",
        "                     annot=annot,\n",
        "                     yticklabels=tick_labels,\n",
        "                     xticklabels=tick_labels,\n",
        "                     cmap=cmap,\n",
        "                     mask=mask_att_below_thresh)\n",
        "    ax.xaxis.set_ticks_position('top')\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=8)\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
        "    xtick_above_threshold = [np.any(mask_att_above_thresh[:,i]) for i in range(0, len(mask_att_above_thresh))]\n",
        "    ytick_above_threshold = [np.any(mask_att_above_thresh[i,:]) for i in range(0, len(mask_att_above_thresh))]\n",
        "    for (is_above_threshold, ticklbl) in zip(xtick_above_threshold, ax.xaxis.get_ticklabels()):\n",
        "        if is_above_threshold:\n",
        "            # ticklbl.set_weight(\"bold\")\n",
        "            ticklbl.set(color = 'black', backgroundcolor='yellow', weight='bold', alpha=0.5)\n",
        "        # ticklbl.set_color('blue' if is_above_threshold else 'black')\n",
        "        # ticklbl.set_backgroundcolor('blue' if is_above_threshold else '0')\n",
        "        # ticklbl.set(color = 'white' if is_above_threshold else 'black', backgroundcolor='blue' if is_above_threshold else 'white')\n",
        "    for (is_above_threshold, ticklbl) in zip(ytick_above_threshold, ax.yaxis.get_ticklabels()):\n",
        "        if is_above_threshold:\n",
        "            ticklbl.set(color = 'black', backgroundcolor='yellow', weight='bold', alpha=0.5)\n",
        "            # ticklbl.set_weight(\"bold\")\n",
        "    if title:\n",
        "        ax.set_title(title, fontsize=80)\n",
        "    else:\n",
        "        ax.set_title('Attention matrix', fontsize=80)\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=80)\n",
        "\n",
        "    # # padding_idx = n_tokens_per_seq + 2   # +2 to consider also 'CLS' and 'SEP' tokens\n",
        "    # padding_idx = tick_labels.index(next(i for i in tick_labels if i.endswith('_[PAD]')))\n",
        "    # ax.hlines([padding_idx], *ax.get_xlim(), linestyles='dashed')\n",
        "    # ax.vlines([padding_idx], *ax.get_ylim(), linestyles='dashed')\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # n_figures = len(os.listdir(dir_path))\n",
        "    # if n_figures > 0:\n",
        "    #     fig_path = Path(dir_path) / f'{filename}({n_figures}).jpg'\n",
        "    # else:\n",
        "    fig_path = Path(dir_path) / f'{filename}.jpg'\n",
        "    fig.savefig(fig_path, bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "\n",
        "def plot_overlaid_attentions(dict_attentions, tick_labels, dir_path, threshold_ratio=0.5, fig_size=(80,80), annot=False, title=None, cmap=colormaps):\n",
        "    '''\n",
        "    Plot heatmap with all attention matrices of last layer of different classes\n",
        "    superimposed with different colours\n",
        "\n",
        "    dict_attentions: square matrices to visualize\n",
        "    path = path to save\n",
        "    tick_labels: labels for xticks and yticks (the tokens in our case)\n",
        "    '''\n",
        "    fig, ax = plt.subplots(figsize=fig_size)\n",
        "    i = 0\n",
        "    for label, mat in dict_attentions.items():\n",
        "        #ax = sns.heatmap(mat, annot=annot, cmap=cmap[label])\n",
        "        mask_att_below_thresh = mat < mat.max() * threshold_ratio\n",
        "        ax = sns.heatmap(mat,\n",
        "                         annot=annot,\n",
        "                         mask=mask_att_below_thresh,\n",
        "                         cmap=cmap[int(label)],\n",
        "                         alpha=1-i/len(dict_attentions.keys()), #alpha=0.9,\n",
        "                         xticklabels=1, yticklabels=1,\n",
        "                         square = True,\n",
        "                         cbar = False,\n",
        "                        #  cbar_kws={\"shrink\":0.10, \"label\": f\"{int(label)}\"}\n",
        "                         )\n",
        "        i += 1\n",
        "\n",
        "    ax.xaxis.set_ticks_position('top')\n",
        "    ax.set_xticklabels(tick_labels, rotation=90)\n",
        "    ax.set_yticklabels(tick_labels, rotation=0)\n",
        "    # ax.xaxis.set_ticks_position('top')\n",
        "    # ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "    # mask_att_above_thresh = mat > mat.max() / 2\n",
        "\n",
        "    # padding_idx = n_tokens_per_seq + 2   # +2 to consider also 'CLS' and 'SEP' tokens\n",
        "    padding_idx = next(i for i in tick_labels if i.endswith('[PAD]'))\n",
        "    ax.hlines([padding_idx], *ax.get_xlim(), linestyles='dashed')\n",
        "    ax.vlines([padding_idx], *ax.get_ylim(), linestyles='dashed')\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title, fontsize=80)\n",
        "    else:\n",
        "        ax.set_title('Attention matrix', fontsize=80)\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f\"Directory '{dir_path}' created\")\n",
        "\n",
        "    fig_path = Path(dir_path) / f'{title}.jpg'\n",
        "    fig.savefig(fig_path, bbox_inches='tight', pad_inches=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUv_tTHm1ky"
      },
      "source": [
        "####Test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKbXNLltak08"
      },
      "outputs": [],
      "source": [
        "def distance_cone(mat):\n",
        "    # norm_per_row = np.linalg.norm(mat, axis=1)\n",
        "    normalized_rows_mat =  mat/np.linalg.norm(mat, ord=2, axis=1, keepdims=True)\n",
        "    # norm_per_row = np.linalg.norm(norm, ord=2, axis=1, keepdims=True) #math.sqrt(sum([i**2 for i in mat[0]]))\n",
        "    # print(f'NORMA RIGhe = {norm_per_row}')\n",
        "    # norm = [i/norm_per_row for i in mat[0]]\n",
        "    # print(norm)\n",
        "\n",
        "    # #print(root_sum_lambda_i)\n",
        "    # norm_lambda = [abs(lambda_i)/root_sum_lambda_i for lambda_i in eigvals]\n",
        "    # # print(norm_per_row)\n",
        "    # normalized_rows_mat = []\n",
        "    # for row in range(mat.shape[0]):\n",
        "    #     normalized_rows_mat.append(mat[row, :] / norm_per_row[row])\n",
        "    summed_rows_array = np.sum(normalized_rows_mat, axis=0)\n",
        "    norm_summed_rows_mat = np.linalg.norm(summed_rows_array)\n",
        "    return norm_summed_rows_mat\n",
        "\n",
        "def singular_values(mat):\n",
        "    return abs(np.linalg.svd(mat, compute_uv=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F16BM5fz0E_K"
      },
      "outputs": [],
      "source": [
        "def mat_save_csv(mat, name, dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "        print(f\"Directory '{dirpath}' created\")\n",
        "    np.savetxt(Path(dirpath) / f'{name}.csv', mat, delimiter=\",\")\n",
        "\n",
        "def chi2_test(multihead_output):\n",
        "    pass\n",
        "    #n_bins = 28\n",
        "    # for row in range(multihead_output.shape[0]):\n",
        "    #     # print(f'min={min(multihead_output[row,:])} | max={max(multihead_output[row,:])}')\n",
        "    #     # print(multihead_output[row,:])\n",
        "    #     bins = pd.cut(multihead_output[row,:], bins=n_bins)\n",
        "    #     bin_counts = pd.value_counts(bins)\n",
        "    #     print(bins.index.tolist())\n",
        "\n",
        "    #     mean_val = []\n",
        "    #     sum_bins = 0\n",
        "    #     for i,bin_count in enumerate(bin_counts):\n",
        "    #         bin_interval = bin_counts.index.tolist()[i]\n",
        "    #         mv = (bin_interval.right-bin_interval.left)/2\n",
        "    #         mean_val.append(mv)\n",
        "    #         n = bin_count*mv\n",
        "    #         sum_bins += n\n",
        "    #     empirical_mean = sum_bins / 768\n",
        "    #     mean_row = np.mean(multihead_output[row,:])\n",
        "\n",
        "    #     sum_quadr_res = 0\n",
        "    #     for i,bin_count in enumerate(bin_counts):\n",
        "    #         mv = mean_val[i]\n",
        "    #         sum_quadr_res += ((mv - empirical_mean)**2) * bin_count\n",
        "    #     empirical_stddev = math.sqrt(1/(768) * sum_quadr_res)\n",
        "    #     stddev_row = np.std(multihead_output[row,:])\n",
        "\n",
        "    #     exp_freq = []\n",
        "    #     for i,bin_count in enumerate(bin_counts):\n",
        "    #         exp_freq.append(1/(math.sqrt(2*math.pi) * empirical_stddev) * math.exp(-(mean_val[i]-empirical_mean)**2/(2*empirical_stddev**2)))\n",
        "\n",
        "    #     chisq_test_stat, p_value = chisquare(bin_counts/768, exp_freq, n_bins-1)\n",
        "    #     print(f'row {row} | chisq_test_stat ={chisq_test_stat} | p-value = {p_value}')\n",
        "\n",
        "    #     # print(f'empirical_mean = {empirical_mean} | empirical_stddev = {empirical_stddev}')\n",
        "    #     print(f'mean = {mean_row} | stddev = {stddev_row}')\n",
        "    #     print(mean_val)\n",
        "    #     plt.bar(mean_val, bin_counts/768, width=0.2)\n",
        "    #     # plt.hist(multihead_output[row,:]/768, bins=n_bins)\n",
        "    #     mu = mean_row\n",
        "    #     variance = stddev_row**2\n",
        "    #     sigma = stddev_row\n",
        "    #     x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
        "    #     # plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
        "    #     # mu = mean_row\n",
        "    #     # variance = stddev_row**2\n",
        "    #     # sigma = stddev_row\n",
        "    #     # x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
        "    #     def f(y):\n",
        "    #         return [1/(math.sqrt(2*math.pi) * stddev_row) * math.exp(-(yi-mean_row)**2/(2*stddev_row**2)) for yi in y]\n",
        "    #     plt.plot(x, f(x), color='black')\n",
        "    #     plt.show()\n",
        "\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8bMPFCUiZPd"
      },
      "outputs": [],
      "source": [
        "mat = np.random.rand(n_layers, n_heads)\n",
        "df_cols = [f\"H{x+1}\" for x in range(n_heads)]\n",
        "df_idx = [f\"L{x+1}\" for x in range(n_layers)]\n",
        "df = pd.DataFrame(mat, columns=df_cols)\n",
        "# df.set_index(\"Layer\\Head\", inplace = True)\n",
        "df.insert(0, \"Layer\\Head\", df_idx)\n",
        "# pd.options.display.float_format = '${:,:.2e}'.format\n",
        "print(tabulate(df, headers=df.columns, showindex=False, stralign='right', floatfmt=\".2e\"))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddxfLUbaDMHo"
      },
      "outputs": [],
      "source": [
        "def find_min_n_elem_sum_greater_thresh(arr, threshold):\n",
        "    # We could get rid of worse case O(n^2) behavior that basically never happens\n",
        "    # by selecting the median here deterministically, but in practice, the constant\n",
        "    # factor on the algorithm will be much worse.\n",
        "    len_arr = list(range(len(arr)))\n",
        "    p = random.choice(len_arr)\n",
        "    left_arr = arr[:p]\n",
        "    right_arr = arr[p+1:]\n",
        "    # assume p is in neither left_arr nor right_arr\n",
        "    right_sum = sum(right_arr)\n",
        "    if right_sum + arr[p] >= threshold:\n",
        "        if right_sum < threshold:\n",
        "            # solved it, p forms the cut off\n",
        "            return len(right_arr) + 1\n",
        "        # took too much, at least we eliminated left_arr and p\n",
        "        return find_min_n_elem_sum_greater_thresh(right_arr, threshold)\n",
        "    else:\n",
        "        # didn't take enough yet, include all elements from and eliminate right_arr and p\n",
        "        return len(right_arr) + 1 + find_min_n_elem_sum_greater_thresh(left_arr, threshold - right_sum - arr[p])\n",
        "\n",
        "def compute_size_of_metastable_state_k(mat, threshold):\n",
        "    return np.mean([find_min_n_elem_sum_greater_thresh(row, threshold) for row in mat])\n",
        "\n",
        "def count_n_class_samples(class_name):\n",
        "    n_class_samples = 0\n",
        "    with open(train_file) as train_fp, open(token_count_file, 'w') as token_count_fp:\n",
        "        train_reader = csv.reader(train_fp, delimiter=',')\n",
        "        # loop on training data\n",
        "        for line in tqdm(train_reader):\n",
        "            label = line[0]\n",
        "            if label == class_name:\n",
        "                n_class_samples += 1\n",
        "\n",
        "    return n_class_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWdpqY7Blnyv"
      },
      "outputs": [],
      "source": [
        "def compute_Von_Neumann_entropy_eigvals(mat, head_lay):\n",
        "    eigvals = np.linalg.eigvals(mat)\n",
        "\n",
        "    # print(f'{head_lay} {len(eigvals) - np.count_nonzero(eigvals)}/{len(eigvals)}')\n",
        "    # print(f'{head_lay} {eigvals}')\n",
        "\n",
        "    # dir_path = math_interpret_dir\n",
        "    # fig = plt.figure(figsize=(8,8))\n",
        "    # plt.hist(eigvals)\n",
        "    # plt.title(f\"Histogram of eigenvalues of the attention matrix for {head_lay}\")\n",
        "    # plt.grid(linestyle = '--')\n",
        "    # plt.yticks(list(plt.yticks()[0]) + [1])\n",
        "    # plt.show()\n",
        "    # dit_path_head = Path(dir_path) / 'hist_sing_val' / head_lay\n",
        "    # if not os.path.exists(dit_path_head):\n",
        "    #     os.makedirs(dit_path_head)\n",
        "    # fig_path = Path(dit_path_head) / f'head_lay.jpg'\n",
        "    # fig.savefig(fig_path, bbox_inches='tight')\n",
        "    # plt.close()\n",
        "\n",
        "    root_sum_lambda_i = math.sqrt(sum([abs(lambda_i)**2 for lambda_i in eigvals]))\n",
        "    #print(root_sum_lambda_i)\n",
        "    norm_lambda = [abs(lambda_i)/root_sum_lambda_i for lambda_i in eigvals]\n",
        "    #print(norm_lambda)\n",
        "    return -sum([abs(lambda_i_norm) * np.log(abs(lambda_i_norm)) for lambda_i_norm in norm_lambda if lambda_i_norm!=float(0)])\n",
        "\n",
        "    # root_sum_lambda_i = math.sqrt(sum([abs(lambda_i)**2 for lambda_i in eigvals]))\n",
        "    # #print(root_sum_lambda_i)\n",
        "    # norm_lambda = [lambda_i/root_sum_lambda_i for lambda_i in eigvals]\n",
        "    # #print(norm_lambda)\n",
        "    # return -sum([lambda_i_norm * np.log(lambda_i_norm) for lambda_i_norm in norm_lambda if lambda_i_norm!=float(0)])\n",
        "\n",
        "def compute_Shannon_entropy(mat, head_lay):\n",
        "\n",
        "    bins = pd.cut(mat.ravel(), bins=20)\n",
        "    shannon_entropy=0\n",
        "    tot_count = len(mat.ravel())\n",
        "    pjs = pd.value_counts(bins) / tot_count\n",
        "    for pj in pjs:\n",
        "        if pj != 0:\n",
        "            shannon_entropy += pj*np.log(pj)\n",
        "\n",
        "    return -shannon_entropy\n",
        "\n",
        "def supervisedBertClassifierTest(model_test, test_dataloader, log_fp_test, theta=0, selected_layer_head_list=None, selected_class=None, dir_path=None, title=None):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('Calculating predictions...')\n",
        "    log_fp_test.write(\"\\nTest:\\n\")\n",
        "    log_fp_test.write(\"=====\\n\")\n",
        "    # Put model in evaluation mode\n",
        "    model_test.eval()\n",
        "\n",
        "    if config_dict['TASK_TYPE'] =='simple_test':\n",
        "        # timing\n",
        "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "        timings = []\n",
        "\n",
        "        # GPU-warmup\n",
        "        rand_idx = [np.random.randint(1,len(tokenizer.get_added_vocab())) for _ in range(512)]\n",
        "        dummy_input = {\n",
        "                'ids': torch.tensor([\n",
        "                                    np.take(np.asarray(list(tokenizer.get_added_vocab().values())), rand_idx, axis=0),\n",
        "                                    np.take(np.asarray(list(tokenizer.get_added_vocab().values())), rand_idx, axis=0)\n",
        "                                    ],\n",
        "                                    dtype=torch.long),\n",
        "                'mask': torch.ones(2,config_dict['MAX_LENGTH'], dtype=torch.long),\n",
        "                'targets': torch.randn(2,config_dict['MAX_LENGTH'], dtype=torch.float),\n",
        "                'seq_ids': torch.randint(2,config_dict['MAX_LENGTH'], (1,config_dict['MAX_LENGTH'])),\n",
        "                'positions': torch.randint(2,config_dict['MAX_LENGTH'], (1,config_dict['MAX_LENGTH']))\n",
        "            }\n",
        "        for _ in range(10):\n",
        "            _ = model_test(dummy_input['ids'].to(device, dtype = torch.long),\n",
        "                        attention_mask=dummy_input['mask'].to(device, dtype = torch.long),\n",
        "                        return_dict=False)\n",
        "\n",
        "    test_accuracies = []\n",
        "    total_test_accuracy = 0\n",
        "\n",
        "    final_data_test = {'seq_ids' : [],\n",
        "                  'positions' : [],\n",
        "                  'targets' : [],\n",
        "                  'outputs' : [],\n",
        "                  'logits' : [],\n",
        "                  'output_embeddings' : [],\n",
        "                  'input_embeddings' : [],\n",
        "                  }\n",
        "    attentions = {}\n",
        "    attentions_last_layer = {} # 1 heatmap per class, with attention scores of last layer of all samples\n",
        "    attentions_all_layers = {} # 1 list of attention matrices per class, with attention scores of all layers of 1 sample\n",
        "    attentions_all_layers_thresh = {}\n",
        "    count_class_samples = {}\n",
        "    repr_token_base_positions_axis = {}\n",
        "    violin_plots_data = {}\n",
        "    timings=None\n",
        "\n",
        "    distance_cones={}\n",
        "    distance_cones_1_sample={}\n",
        "    count_layer = {k:0 for k in range(-1, 12)}\n",
        "\n",
        "    # cone_index_sing_vals_histograms:\n",
        "    number_of_omicron_samples = count_n_class_samples(f\"{config_dict['CLASS_LABELS']['omicron']}\")\n",
        "    random_indices = [random.randint(0, number_of_omicron_samples) for _ in range(30)]\n",
        "    cone_indices_at_softmax = []\n",
        "    sing_vals_at_softmax = []\n",
        "\n",
        "    layer_query_weight_names = [f'bert.encoder.layer.{l}.attention.self.query.weight' for l in range(n_layers)]\n",
        "    layer_key_weight_names = [f'bert.encoder.layer.{l}.attention.self.key.weight' for l in range(n_layers)]\n",
        "    layer_value_weight_names = [f'bert.encoder.layer.{l}.attention.self.value.weight' for l in range(n_layers)]\n",
        "    model_params = dict(model_test.named_parameters())\n",
        "\n",
        "    for _,label in config_dict['CLASS_LABELS'].items():\n",
        "        count_class_samples[int(label)] = 0\n",
        "    # attention_matrices_count = {}\n",
        "    # for _, label in config_dict['CLASS_LABELS'].items():\n",
        "    #     attention_matrices_count[int(label)] = 0\n",
        "    # max_n_attention_matrices = 6\n",
        "\n",
        "    # Predict\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5}  of  {:>5}.'.format(step, len(test_dataloader)))\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.float)\n",
        "        seq_ids = batch['seq_ids'].to('cpu').numpy()\n",
        "        positions = batch['positions'].to('cpu').numpy()\n",
        "\n",
        "        if (config_dict['TASK_TYPE'] == 'singularvalues_ratio_analysis' or config_dict['TASK_TYPE'] == 'violin_plots') and config_dict['CLASS_LABELS'][selected_class] not in targets:\n",
        "            continue\n",
        "\n",
        "        y_onehot = torch.nn.functional.one_hot(targets.long(), num_classes=config_dict['N_CLASSES']).float()\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            if config_dict['TASK_TYPE'] =='simple_test':\n",
        "                starter.record()\n",
        "            outputs = model_test(ids,\n",
        "                        attention_mask=mask,\n",
        "                        return_dict=True,\n",
        "                        output_attentions=(config_dict['TASK_TYPE']=='attention_analysis' or config_dict['TASK_TYPE']=='attention_flow'),\n",
        "                        output_hidden_states=(config_dict['TASK_TYPE']=='eigenvalues_analysis'\n",
        "                                              or config_dict['TASK_TYPE']=='singularvalues_ratio_analysis'\n",
        "                                              or config_dict['TASK_TYPE']=='check_normality'\n",
        "                                              or config_dict['TASK_TYPE']=='distance_cones_analysis'\n",
        "                                              or config_dict['TASK_TYPE']=='cone_index_sing_vals_histograms'\n",
        "                                              or config_dict['TASK_TYPE']=='layer_output_analysis'\n",
        "                                              or config_dict['TASK_TYPE']=='print_layer_output'\n",
        "                                              or config_dict['TASK_TYPE']=='clustering'\n",
        "                                              or config_dict['TASK_TYPE']=='violin_plots'))\n",
        "            if config_dict['TASK_TYPE'] =='simple_test':\n",
        "                ender.record()\n",
        "                torch.cuda.synchronize() # WAIT FOR GPU SYNC\n",
        "                curr_time = starter.elapsed_time(ender)\n",
        "                timings.append(curr_time)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = targets.to('cpu').numpy()\n",
        "\n",
        "        # # Store predictions and true labels\n",
        "        # predictions.append(logits)\n",
        "        # true_labels.append(label_ids)\n",
        "\n",
        "\n",
        "        if config_dict['TASK_TYPE'] !='clustering':\n",
        "            batch_accuracy = flat_accuracy(logits, label_ids)\n",
        "            total_test_accuracy += batch_accuracy\n",
        "            test_accuracies.append(batch_accuracy)\n",
        "\n",
        "            final_data_test['seq_ids'].extend(seq_ids)\n",
        "            final_data_test['positions'].extend(positions)\n",
        "            final_data_test['targets'].extend(y_onehot.cpu().detach().numpy().tolist())\n",
        "            final_data_test['logits'].extend(outputs.logits.cpu().detach().numpy().tolist())\n",
        "            # final_data_test['outputs'].extend(torch.softmax(outputs.logits).cpu().detach().numpy().tolist())\n",
        "            if config_dict['N_CLASSES'] > 2:\n",
        "                final_data_test['outputs'].extend(torch.softmax(outputs.logits, dim=1).cpu().detach().numpy().tolist())\n",
        "            else:\n",
        "                final_data_test['outputs'].extend(torch.sigmoid(outputs.logits).cpu().detach().numpy().tolist())\n",
        "\n",
        "        #############################################\n",
        "        if config_dict['TASK_TYPE']=='print_layer_output':\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "\n",
        "                if target_label == config_dict['CLASS_LABELS']['omicron'] and target_label == np.argmax(logits[sample_idx]): # correct prediction\n",
        "                    input_embs = outputs.hidden_states[0][sample_idx].cpu().detach().numpy() # X = imput to current layer (output of previous layer)\n",
        "                                                                                                    # NB: hidden_states of attn layers shifted by +1 because hidden_states[0] is embedding layer\n",
        "                    mat_save_csv(input_embs, f'input_embeddings', math_interpret_dir)\n",
        "                    return final_data_test, test_accuracies, attentions, timings\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='clustering':\n",
        "            sel_layers = [1,2,3,4,5,6,7,8,9,10,11]\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "                count_class_samples[target_label] += 1\n",
        "                if count_class_samples[target_label] <= 300:\n",
        "                    final_data_test['targets'].append(y_onehot[sample_idx].cpu().detach().numpy())\n",
        "                    # input_embeddings = outputs.hidden_states[0][sample_idx].cpu().detach().numpy()\n",
        "                    # output_embeddings = outputs.hidden_states[-1][sample_idx].cpu().detach().numpy()\n",
        "                    # final_data_test['input_embeddings'].append(input_embeddings)\n",
        "                    # final_data_test['output_embeddings'].append(output_embeddings)\n",
        "                    if config_dict['N_CLASSES'] > 2:\n",
        "                        final_data_test['outputs'].append(torch.softmax(outputs.logits[sample_idx], dim=0).cpu().detach().numpy().tolist())\n",
        "                    else:\n",
        "                        final_data_test['outputs'].append(torch.sigmoid(outputs.logits[sample_idx]).cpu().detach().numpy().tolist())\n",
        "\n",
        "                    CLS_embs_dict = {\n",
        "                        # \"CLS_input_embeddings\" : input_embeddings[0],\n",
        "                        # \"CLS_Y_output_L12\" : output_embeddings[0],\n",
        "                        \"CLS_pre_softmax\" : logits[sample_idx],\n",
        "                        \"CLS_final_output\" : torch.softmax(outputs.logits[sample_idx], dim=0).cpu().detach().numpy().tolist()\n",
        "                    }\n",
        "                    for k,v in CLS_embs_dict.items():\n",
        "                        with open(Path(CLS_embeddings_dir) / f\"{k}.csv\", 'a') as fp:\n",
        "                            writer = csv.writer(fp)\n",
        "                            writer.writerow(v)\n",
        "\n",
        "                    for l in sel_layers:\n",
        "                        with open(Path(CLS_embeddings_dir) / f\"CLS_Y_output_L{l}.csv\", 'a') as fp:\n",
        "                            CLS_l_embeddings = outputs.hidden_states[l][sample_idx][0].cpu().detach().numpy()\n",
        "                            writer = csv.writer(fp)\n",
        "                            writer.writerow(CLS_l_embeddings)\n",
        "\n",
        "        # get attention matrices\n",
        "        # NB:   outputs.attentions is a tuple containing 12 elements, i.e. attentions of the 12 layers of Bert\n",
        "        #       eg. if we consider first layer from the bottom (closest to the input):\n",
        "        #           print(outputs.attentions[0].shape)\n",
        "        #               -> torch.Size([4, 12, 512, 512]) -> [batch_size , num_heads, seq_len, seq_len]\n",
        "        elif config_dict['TASK_TYPE']=='attention_analysis' or config_dict['TASK_TYPE']=='attention_flow':\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "\n",
        "                if target_label == np.argmax(logits[sample_idx]) or not do_finetuning: # correct prediction\n",
        "\n",
        "                    count_class_samples[target_label] += 1\n",
        "\n",
        "                    # get attention scores of sample from all heads in all layers\n",
        "                    sample_attentions = []\n",
        "                    sample_attentions_thresh = []\n",
        "                    for layer in range(n_layers):\n",
        "                        layer_attentions = []\n",
        "                        masked_layer_attentions = []\n",
        "                        for head in range(n_heads):\n",
        "                            # Source: \"Bertology meets Biology...\": We exclude attention to the [SEP] delimiter token, as it has been shown to\n",
        "                            # be a “no-op” attention token (Clark et al., 2019), as well as attention to\n",
        "                            # the [CLS] token, which is not explicitly used in language modeling.\n",
        "                            # Additionally, exclude attention on [PAD] tokens.\n",
        "                            head_attentions = get_attentions(outputs.attentions, sample_idx_in_batch=sample_idx, layer=layer, attention_head=head, sum=False)\n",
        "                            sep_token_idx = ids[sample_idx].cpu().detach().numpy().tolist().index(next(id for id in ids[sample_idx].cpu().detach().numpy().tolist() if tokenizer.convert_ids_to_tokens(id)==\"[SEP]\"))\n",
        "                            head_attentions[:,sep_token_idx] = 0 # SEP col\n",
        "                            head_attentions[sep_token_idx,:] = 0 # SEP row\n",
        "                            head_attentions[:,sep_token_idx:] = 0 # PAD cols\n",
        "                            head_attentions[sep_token_idx:,:] = 0 # PAD rows\n",
        "                            layer_attentions.append(head_attentions)\n",
        "                            mask_att_below_thresh = head_attentions < theta\n",
        "                            masked_head_attentions = np.ma.masked_array(head_attentions, mask=mask_att_below_thresh, fill_value=0)\n",
        "                            masked_layer_attentions.append(masked_head_attentions.filled())\n",
        "                        sample_attentions.append(layer_attentions)\n",
        "                        sample_attentions_thresh.append(masked_layer_attentions)\n",
        "\n",
        "                    # update sum of class attention scores of all heads of all layers\n",
        "                    if target_label not in attentions_all_layers:\n",
        "                        attentions_all_layers[target_label] = torch.zeros(np.asarray(sample_attentions).shape, dtype=torch.double)\n",
        "                        repr_token_base_positions_axis [target_label] = []\n",
        "                        for n,id in enumerate(ids[sample_idx].cpu().detach().numpy().tolist()):\n",
        "                            if n==0:\n",
        "                                repr_token_base_positions_axis[target_label].append('[CLS]')\n",
        "                            else:\n",
        "                                start_p = int(((n*config_dict['STRIDE']-config_dict['STRIDE'])/3) +1)\n",
        "                                end_p = int(start_p + (config_dict['K']/3) - 1)\n",
        "                                repr_token_base_positions_axis[target_label].append(f\"{start_p}-{end_p}\")\n",
        "                        # repr_token_base_positions_axis [target_label] = [f\"{n*config_dict['STRIDE']-config_dict['STRIDE']}_{tokenizer.convert_ids_to_tokens(id)}\" for n,id in enumerate(ids[sample_idx].cpu().detach().numpy().tolist())]\n",
        "\n",
        "                    if target_label not in attentions_all_layers_thresh:\n",
        "                        attentions_all_layers_thresh[target_label] = torch.zeros(np.asarray(sample_attentions_thresh).shape, dtype=torch.double)\n",
        "\n",
        "                    for layer in range(n_layers):\n",
        "                        for head in range(n_heads):\n",
        "                            attentions_all_layers[target_label][layer][head] = attentions_all_layers[target_label][layer][head] + np.asarray(sample_attentions[layer][head])\n",
        "                            attentions_all_layers_thresh[target_label][layer][head] = attentions_all_layers_thresh[target_label][layer][head] + np.asarray(sample_attentions_thresh[layer][head])\n",
        "\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='violin_plots':\n",
        "            print(\"Computing violin plots...\")\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "                if target_label != config_dict['CLASS_LABELS'][selected_class]:\n",
        "                    continue\n",
        "\n",
        "                if target_label not in violin_plots_data:\n",
        "                    violin_plots_data[target_label] = [[[] for _ in range(n_heads)] for _ in range(n_layers)]\n",
        "\n",
        "                for layer in range(n_layers):\n",
        "                    for head in range(n_heads):\n",
        "                        X = outputs.hidden_states[layer+1][sample_idx].cpu().detach().numpy() # token embeddings shape=(n_tokens_in_seq, emb_dim)=(512, 768)\n",
        "                        emb_dim = X.shape[1]\n",
        "                        d_k = int(emb_dim / n_heads)\n",
        "                        W_q = model_params[layer_query_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                        W_k = model_params[layer_key_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                        W_q_i = W_q[:, head*d_k : head*d_k + d_k]\n",
        "                        W_k_i = W_k[:, head*d_k : head*d_k + d_k]\n",
        "\n",
        "                        Q_i = np.matmul(X, W_q_i)\n",
        "                        K_i = np.matmul(X, W_k_i)\n",
        "\n",
        "                        softmax_i = torch.softmax(torch.tensor(np.matmul(Q_i, K_i.T) / math.sqrt(d_k)), dim=1).cpu().detach().numpy()\n",
        "\n",
        "                        violin_plots_data[target_label][layer][head].extend([find_min_n_elem_sum_greater_thresh(sorted(row), 0.90) for row in softmax_i])\n",
        "            #     break\n",
        "            # break\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='check_normality':\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "                name_seq=seq_ids[sample_idx]\n",
        "                if target_label == np.argmax(logits[sample_idx]): # correct prediction\n",
        "                    X = outputs.hidden_states[0][sample_idx].cpu().detach().numpy()  # X = imput to current layer (output of previous layer)\n",
        "                                                                                            # NB: hidden_states of attn layers shifted by +1 because hidden_states[0] is embedding layer\n",
        "                                                                                            # token embeddings shape=(n_tokens_in_seq, emb_dim)=(512, 768)\n",
        "\n",
        "                    for i, row in enumerate(X):\n",
        "                        print(f'Mean row {i}: {np.mean(row)}')\n",
        "                break\n",
        "\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='distance_cones_analysis':\n",
        "\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "                name_seq=seq_ids[sample_idx]\n",
        "                if target_label == config_dict['CLASS_LABELS']['omicron'] and target_label == np.argmax(logits[sample_idx]): # correct prediction\n",
        "\n",
        "\n",
        "                    for layer in set(np.asarray(selected_layer_head_list)[:,0]):\n",
        "                        X = outputs.hidden_states[layer][sample_idx].cpu().detach().numpy()  # X = imput to current layer (output of previous layer)\n",
        "                                                                                            # NB: hidden_states of attn layers shifted by +1 because hidden_states[0] is embedding layer\n",
        "                                                                                            # token embeddings shape=(n_tokens_in_seq, emb_dim)=(512, 768)\n",
        "                        # print(f'-------NORMA X[0]: {np.linalg.norm(X[0])}')\n",
        "                        rows_sum_X = np.sum(X, axis=0)\n",
        "                        # norm = np.linalg.norm(rows_sum_X, ord=2)\n",
        "                        # print(f'-------NORMA X: {norm}')\n",
        "                        # model_params = dict(model.named_parameters())\n",
        "                        # print(model_params[\"bert.encoder.layer.0.attention.self.value.weight\"].data)\n",
        "                        W_v = model_params[f\"bert.encoder.layer.{layer}.attention.self.value.weight\"].data.cpu().detach().numpy()\n",
        "                        W_q = model_params[f\"bert.encoder.layer.{layer}.attention.self.query.weight\"].data.cpu().detach().numpy()\n",
        "                        W_k = model_params[f\"bert.encoder.layer.{layer}.attention.self.key.weight\"].data.cpu().detach().numpy()\n",
        "                        emb_dim = X.shape[1]\n",
        "                        d_k = int(emb_dim / n_heads)\n",
        "                        multihead_output = None\n",
        "\n",
        "                        for head in np.sort([h for l, h in selected_layer_head_list if l==layer]):\n",
        "                            W_v_i = W_v[:, head*d_k : head*d_k + d_k]\n",
        "                            W_q_i = W_q[:, head*d_k : head*d_k + d_k]\n",
        "                            W_k_i = W_k[:, head*d_k : head*d_k + d_k]\n",
        "                            V_i = np.matmul(X, W_v_i)\n",
        "                            Q_i = np.matmul(X, W_q_i)\n",
        "                            K_i = np.matmul(X, W_k_i)\n",
        "\n",
        "                            softmax_i = torch.softmax(torch.tensor(np.matmul(Q_i, K_i.T) / math.sqrt(d_k)), dim=1).cpu().detach().numpy()\n",
        "                            output_head_i = np.matmul(softmax_i, V_i)\n",
        "\n",
        "                            if head == 5-1:\n",
        "                                output_head_5 = np.copy(output_head_i)\n",
        "\n",
        "                            if multihead_output is None:\n",
        "                                multihead_output = np.copy(output_head_i)\n",
        "                            else:\n",
        "                                multihead_output = np.hstack((multihead_output, output_head_i))\n",
        "\n",
        "                        layerNorm = nn.LayerNorm(model_test.config.hidden_size, eps=model_test.config.layer_norm_eps)\n",
        "                        multihead_output = layerNorm(torch.from_numpy(X + multihead_output)).cpu().detach().numpy()\n",
        "\n",
        "                        layer_output = outputs.hidden_states[layer+1][sample_idx].cpu().detach().numpy() # output of current layer\n",
        "\n",
        "                    #     print(f'Saving mat L{layer+1}')\n",
        "                    #     dirpath = Path(math_interpret_dir) / 'Y_outputs'\n",
        "                    #     mat_save_csv(multihead_output, f'Y_output_L{layer+1}', dirpath)\n",
        "\n",
        "                    # return final_data_test, test_accuracies, attentions, timings\n",
        "\n",
        "                        if layer not in distance_cones:\n",
        "                            distance_cones[layer] = {}\n",
        "\n",
        "                        dc_X = distance_cone(X)\n",
        "                        # print(f'-------({name_seq}) DISTANCE CONE X: {dc_X}')\n",
        "                        if 'Layer input' not in distance_cones[layer]:\n",
        "                            distance_cones[layer]['Layer input'] = []\n",
        "                        distance_cones[layer]['Layer input'].append(dc_X)\n",
        "\n",
        "                        dc_output_head_5 = distance_cone(output_head_5)\n",
        "                        # print(f'-------({name_seq}) DISTANCE CONE output_head_5: {dc_output_head_5}')\n",
        "                        if 'Head output' not in distance_cones[layer]:\n",
        "                            distance_cones[layer]['Head output'] = []\n",
        "                        distance_cones[layer]['Head output'].append(dc_output_head_5)\n",
        "\n",
        "                        dc_multihead_output = distance_cone(multihead_output)\n",
        "                        # print(f'-------({name_seq}) DISTANCE CONE multihead_i: {dc_multihead_output}')\n",
        "                        if 'Multihead output' not in distance_cones[layer]:\n",
        "                            distance_cones[layer]['Multihead output'] = []\n",
        "                        distance_cones[layer]['Multihead output'].append(dc_multihead_output)\n",
        "\n",
        "                        dc_layer_output = distance_cone(layer_output)\n",
        "                        # print(f'-------({name_seq}) DISTANCE CONE layer_output: {layer_output}')\n",
        "                        if 'Layer output' not in distance_cones[layer]:\n",
        "                            distance_cones[layer]['Layer output'] = []\n",
        "                        distance_cones[layer]['Layer output'].append(dc_layer_output)\n",
        "\n",
        "                        if count_layer[layer] == 0:\n",
        "                            if layer == 0:\n",
        "                                distance_cones_1_sample[-1] = dc_X\n",
        "                            distance_cones_1_sample[layer] = dc_layer_output\n",
        "                break\n",
        "            break\n",
        "\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='cone_index_sing_vals_histograms':\n",
        "\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "\n",
        "                target_label = label_ids[sample_idx]\n",
        "                if target_label == config_dict['CLASS_LABELS']['omicron'] and target_label == np.argmax(logits[sample_idx]): # correct prediction\n",
        "\n",
        "                    layer = 10\n",
        "                    head = 0\n",
        "\n",
        "                    X = outputs.hidden_states[layer][sample_idx].cpu().detach().numpy()  # X = imput to current layer (output of previous layer)\n",
        "                                                                                            # NB: hidden_states of attn layers shifted by +1 because hidden_states[0] is embedding layer\n",
        "                                                                                            # token embeddings shape=(n_tokens_in_seq, emb_dim)=(512, 768)\n",
        "                    W_v = model_params[f\"bert.encoder.layer.{layer}.attention.self.value.weight\"].data.cpu().detach().numpy()\n",
        "                    W_q = model_params[f\"bert.encoder.layer.{layer}.attention.self.query.weight\"].data.cpu().detach().numpy()\n",
        "                    W_k = model_params[f\"bert.encoder.layer.{layer}.attention.self.key.weight\"].data.cpu().detach().numpy()\n",
        "                    emb_dim = X.shape[1]\n",
        "                    d_k = int(emb_dim / n_heads)\n",
        "                    multihead_output = None\n",
        "\n",
        "                    W_v_i = W_v[:, head*d_k : head*d_k + d_k]\n",
        "                    W_q_i = W_q[:, head*d_k : head*d_k + d_k]\n",
        "                    W_k_i = W_k[:, head*d_k : head*d_k + d_k]\n",
        "                    V_i = np.matmul(X, W_v_i)\n",
        "                    Q_i = np.matmul(X, W_q_i)\n",
        "                    K_i = np.matmul(X, W_k_i)\n",
        "\n",
        "                    softmax_i = torch.softmax(torch.tensor(np.matmul(Q_i, K_i.T) / math.sqrt(d_k)), dim=1).cpu().detach().numpy()\n",
        "\n",
        "                    cone_indices_at_softmax.append(distance_cone(softmax_i))\n",
        "                    sing_vals_at_softmax.append((singular_values(softmax_i) < 0.001).sum())\n",
        "\n",
        "\n",
        "                    count_class_samples[int(label)] += 1\n",
        "\n",
        "                if count_class_samples[int(label)] > 30:\n",
        "                    break\n",
        "\n",
        "            if count_class_samples[int(label)] > 30:\n",
        "                break\n",
        "\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='layer_output_analysis':\n",
        "            output_layers_dir = Path(math_interpret_dir) / \"output_layer_histograms\"\n",
        "            if not os.path.exists(output_layers_dir):\n",
        "                os.makedirs(output_layers_dir)\n",
        "                print(f\"Directory '{output_layers_dir}' created\")\n",
        "\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "                name_seq=seq_ids[sample_idx]\n",
        "\n",
        "                if target_label == config_dict['CLASS_LABELS']['omicron'] and target_label == np.argmax(logits[sample_idx]): # correct prediction\n",
        "\n",
        "                    # output layer histogram:\n",
        "                    for layer in set(np.asarray(selected_layer_head_list)[:,0]):\n",
        "                        layer_output = outputs.hidden_states[layer+1][sample_idx].cpu().detach().numpy()\n",
        "                        layer_ouput_sum_rows = np.sum(layer_output, axis=0)\n",
        "                        fig = plt.figure(figsize=(8,8))\n",
        "                        plt.hist(layer_ouput_sum_rows, bins=27)\n",
        "                        plt.title(f\"L{layer+1} output\")\n",
        "                        plt.grid(linestyle = '--')\n",
        "                        plt.yticks(list(plt.yticks()[0]) + [1])\n",
        "                        plt.show()\n",
        "                        fig_path = Path(output_layers_dir) / f'output_layer_{layer+1}.jpg'\n",
        "                        fig.savefig(fig_path)\n",
        "                return final_data_test, test_accuracies, attentions, timings\n",
        "\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='singularvalues_ratio_analysis':\n",
        "            mat_ratio_max_min_sv_softmax_i = np.zeros((n_layers,n_heads))\n",
        "            mat_ratio_2ndmin_min_sv_softmax_i = np.zeros((n_layers,n_heads))\n",
        "            mat_ratio_max_min_sv_W_v_i = np.zeros((n_layers,n_heads))\n",
        "            mat_ratio_2ndmin_min_sv_W_v_i = np.zeros((n_layers,n_heads))\n",
        "            rank_softmax_i = np.zeros((n_layers,n_heads))\n",
        "            rank_output_head_i = np.zeros((n_layers,n_heads))\n",
        "            rank_W_v_i = np.zeros((n_layers,n_heads))\n",
        "            rank_V_i = np.zeros((n_layers,n_heads))\n",
        "            rank_layer_output = np.zeros((n_layers,1))\n",
        "            histograms = {}\n",
        "\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "\n",
        "                if target_label == config_dict['CLASS_LABELS'][selected_class] and target_label == np.argmax(logits[sample_idx]): # correct prediction\n",
        "                    dir_path_sv = Path(math_interpret_dir) / 'sing_vals'\n",
        "                    if not os.path.exists(dir_path_sv):\n",
        "                        os.makedirs(dir_path_sv)\n",
        "                    singvals_ratio_file = Path(dir_path_sv) / \"singvals_ratio.txt\"\n",
        "\n",
        "                    for layer, head in selected_layer_head_list:\n",
        "\n",
        "                        X = outputs.hidden_states[layer+1][sample_idx].cpu().detach().numpy() # token embeddings shape=(n_tokens_in_seq, emb_dim)=(512, 768)\n",
        "                        emb_dim = X.shape[1]\n",
        "                        d_k = int(emb_dim / n_heads)\n",
        "                        W_q = model_params[layer_query_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                        W_k = model_params[layer_key_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                        W_v = model_params[layer_value_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                        W_q_i = W_q[:, head*d_k : head*d_k + d_k]\n",
        "                        W_k_i = W_k[:, head*d_k : head*d_k + d_k]\n",
        "                        W_v_i = W_v[:, head*d_k : head*d_k + d_k]\n",
        "\n",
        "                        Q_i = np.matmul(X, W_q_i)\n",
        "                        K_i = np.matmul(X, W_k_i)\n",
        "                        V_i = np.matmul(X, W_v_i)\n",
        "\n",
        "                        softmax_i = torch.softmax(torch.tensor(np.matmul(Q_i, K_i.T) / math.sqrt(d_k)), dim=1).cpu().detach().numpy()\n",
        "                        # singular_values_softmax_i = singular_values(softmax_i)\n",
        "                        # mat_ratio_max_min_sv_softmax_i[layer][head] = max(singular_values_softmax_i) / min(singular_values_softmax_i)\n",
        "                        # secondmin_sv_softmax_i = min(np.delete(singular_values_softmax_i, singular_values_softmax_i.argmin()))\n",
        "                        # mat_ratio_2ndmin_min_sv_softmax_i[layer][head] = secondmin_sv_softmax_i / min(singular_values_softmax_i)\n",
        "                        # mat_count_zero_sv_softmax_i[layer][head] = (singular_values_softmax_i < 0.01).sum()/len(singular_values_softmax_i)\n",
        "                        rank_softmax_i[layer][head] = matrix_rank(softmax_i)\n",
        "\n",
        "                        # singular_values_W_v_i = singular_values(W_v_i)\n",
        "                        # mat_ratio_max_min_sv_W_v_i[layer][head] = max(singular_values_W_v_i) / min(singular_values_W_v_i)\n",
        "                        # secondmin_sv_W_v_i = min(np.delete(singular_values_W_v_i, singular_values_W_v_i.argmin()))\n",
        "                        # mat_ratio_2ndmin_min_sv_W_v_i[layer][head] = secondmin_sv_W_v_i / min(singular_values_W_v_i)\n",
        "                        # mat_count_zero_sv_W_v_i[layer][head] = (singular_values_W_v_i < 0.5).sum()/len(singular_values_W_v_i)\n",
        "                        rank_W_v_i[layer][head] = matrix_rank(W_v_i)\n",
        "\n",
        "                        output_head_i = np.matmul(softmax_i, V_i)\n",
        "                        # singular_values_output_head_i = singular_values(output_head_i)\n",
        "                        # mat_abs_max_sv_output_head_i[layer][head] = abs(max(singular_values_output_head_i))\n",
        "                        # mat_count_zero_sv_output_head_i[layer][head] = (singular_values_output_head_i < 0.1).sum()/len(singular_values_output_head_i)\n",
        "                        rank_output_head_i[layer][head] = matrix_rank(output_head_i)\n",
        "\n",
        "                        # singular_values_V_i = singular_values(V_i)\n",
        "                        # mat_abs_max_sv_V_i[layer][head] = abs(max(singular_values_V_i))\n",
        "                        # mat_count_zero_sv_V_i[layer][head] = (singular_values_V_i < 0.1).sum()/len(singular_values_V_i)\n",
        "                        rank_V_i[layer][head] = matrix_rank(V_i)\n",
        "\n",
        "                        layer_output = outputs.hidden_states[layer+1][sample_idx].cpu().detach().numpy() # X = imput to current layer (output of previous layer)\n",
        "                                                                                                            # NB: hidden_states of attn layers shifted by +1 because hidden_states[0] is embedding layer\n",
        "                        # singular_values_layer_output = singular_values(layer_output)\n",
        "                        # mat_abs_max_sv_layer_output[layer] = abs(max(singular_values_layer_output))\n",
        "                        # mat_count_zero_sv_layer_output[layer] = (singular_values_layer_output < 0.1).sum()/len(singular_values_layer_output)\n",
        "                        rank_layer_output[layer] = matrix_rank(layer_output)\n",
        "\n",
        "                        def generate_hist_sv(sv, h, l, title, histograms):\n",
        "                            counts, bins = np.histogram(sv, 10)\n",
        "                            if title not in histograms.keys():\n",
        "                                histograms[title] = {}\n",
        "                            if h not in histograms[title].keys():\n",
        "                                histograms[title][h] = []\n",
        "                            histograms[title][h].append({\n",
        "                                \"label\": f'Layer {l+1}\\n(max SV: {max(sv)})',\n",
        "                                \"counts\": counts,\n",
        "                                \"bins\": bins,\n",
        "                            })\n",
        "                            return histograms\n",
        "\n",
        "\n",
        "                        # if head in [0,4,8,9]:\n",
        "                        #     histograms = generate_hist_sv(singular_values_softmax_i, head, layer, \"softmax\", histograms)\n",
        "                        #     histograms = generate_hist_sv(singular_values_W_v_i, head, layer, \"W_v\", histograms)\n",
        "\n",
        "                    def save_to_file(mat, fp, title, short_title, fmt, path_fig):\n",
        "                        df_cols = [f\"H{x+1}\" for x in range(n_heads)]\n",
        "                        df_idx = [f\"L{x+1}\" for x in range(n_layers)]\n",
        "                        df = pd.DataFrame(mat, columns=df_cols)\n",
        "                        df.insert(0, \"Layer\\Head\", df_idx)\n",
        "                        df_tabulate = tabulate(df, headers=df.columns, showindex=False, stralign='right', floatfmt=fmt)\n",
        "                        print(title)\n",
        "                        print(\"=================================================================\")\n",
        "                        print(df_tabulate)\n",
        "                        print()\n",
        "                        fp.write(f'{title}\\n=================================================================\\n')\n",
        "                        fp.write(df_tabulate)\n",
        "                        fp.write(\"\\n\\n\")\n",
        "                        df.set_index(\"Layer\\Head\")\n",
        "                        fig = plt.figure(figsize=(15,15))\n",
        "                        plt.title(title)\n",
        "                        ax = sns.heatmap(mat, annot=True, fmt=fmt, yticklabels=df_idx, xticklabels=df_cols)\n",
        "                        ax.invert_yaxis()\n",
        "                        plt.show()\n",
        "                        fig.savefig(Path(path_fig) / f'heatmap_{short_title}.jpg', bbox_inches='tight')\n",
        "                        plt.close()\n",
        "\n",
        "                    def save_to_file_array(a, fp, title, short_title, fmt, path_fig):\n",
        "                        a = np.squeeze(a, axis=0)\n",
        "                        print(title)\n",
        "                        print(\"=================================================================\")\n",
        "                        print(a)\n",
        "                        print()\n",
        "                        idx = [f\"L{x+1}\" for x in range(n_layers)]\n",
        "                        fig = plt.figure(figsize=(15,15))\n",
        "                        plt.title(title)\n",
        "                        ax = sns.heatmap(a, annot=True, fmt=fmt, yticklabels=idx, xticklabels='', )\n",
        "                        ax.set_aspect(\"equal\")\n",
        "                        ax.invert_yaxis()\n",
        "                        plt.show()\n",
        "                        fig.savefig(Path(path_fig) / f'heatmap_{short_title}.jpg', bbox_inches='tight')\n",
        "                        plt.close()\n",
        "\n",
        "\n",
        "\n",
        "                    with open(singvals_ratio_file, 'w') as singvals_ratio_fp:\n",
        "                        # save_to_file(mat_ratio_max_min_sv_softmax_i, singvals_ratio_fp, \"Ratio between the highest and the lowest singular value of matrix softmax((QK^T)/d_k)\", \"ratio_max_min_sv_softmax\", '.2e', dir_path_sv)\n",
        "                        # save_to_file(mat_ratio_2ndmin_min_sv_softmax_i, singvals_ratio_fp, \"Ratio between the second lowest and the lowest singular value of matrix softmax((QK^T)/d_k)\", \"ratio_2ndmin_min_sv_softmax\", '.2f', dir_path_sv)\n",
        "                        # save_to_file(mat_ratio_max_min_sv_W_v_i, singvals_ratio_fp, \"Ratio between the highest and the lowest singular value of matrix W_v\", \"ratio_max_min_sv_Wv\", '.2f', dir_path_sv)\n",
        "                        # save_to_file(mat_ratio_2ndmin_min_sv_W_v_i, singvals_ratio_fp, \"Ratio between the second lowest and the lowest singular value of matrix W_v\", \"ratio_2ndmin_min_sv_Wv\", '.2f', dir_path_sv)\n",
        "                        # save_to_file(mat_count_zero_sv_softmax_i, singvals_ratio_fp, \"Percentage of singular values < 0.01 of matrix softmax((QK^T)/d_k)\", \"count_zero_sv_softmax\", \".2%\", dir_path_sv)\n",
        "                        # save_to_file(mat_count_zero_sv_W_v_i, singvals_ratio_fp, \"Percentage of singular values < 0.5 of matrix W_v\", \"count_zero_sv_Wv\", \".2%\", dir_path_sv)\n",
        "                        # save_to_file(mat_abs_max_sv_output_head_i, singvals_ratio_fp, \"Absolute value of the highest singular value of each head output Z\", \"abs_max_sv_output_head\", '.2f', dir_path_sv)\n",
        "                        # save_to_file(mat_abs_max_sv_V_i, singvals_ratio_fp, \"Absolute value of the highest singular value of matrix V\", \"abs_max_sv_V_i\", '.2f', dir_path_sv)\n",
        "                        # save_to_file_array(np.asarray([mat_abs_max_sv_layer_output]), singvals_ratio_fp, \"Absolute value of the highest singular value of each layer output Y\", \"abs_max_sv_output_layer\", '.2f', dir_path_sv)\n",
        "                        # save_to_file(mat_count_zero_sv_output_head_i, singvals_ratio_fp, \"Percentage of singular values < 0.1 of each head output Z\", \"count_zero_sv_output_head\", \".2%\", dir_path_sv)\n",
        "                        # save_to_file(mat_count_zero_sv_V_i, singvals_ratio_fp, \"Percentage of singular values < 0.1 of matrix V\", \"count_zero_sv_V\", \".2%\", dir_path_sv)\n",
        "                        # save_to_file_array(np.asarray([mat_count_zero_sv_layer_output]), singvals_ratio_fp, \"Percentage of singular values < 0.1 of each layer output Y\", \"count_zero_sv_output_layer\", \".2%\", dir_path_sv)\n",
        "                        save_to_file(rank_softmax_i, singvals_ratio_fp, \"Rank of matrix softmax((QK^T)/d_k)\", \"rank_softmax\", \".2f\", dir_path_sv)\n",
        "                        save_to_file(rank_W_v_i, singvals_ratio_fp, \"Rank of matrix W_v\", \"rank_W_v\", \".2f\", dir_path_sv)\n",
        "                        save_to_file(rank_output_head_i, singvals_ratio_fp, \"Rank of each head output Z\", \"rank_output_head\", \".2f\", dir_path_sv)\n",
        "                        save_to_file(rank_V_i, singvals_ratio_fp, \"Rank of matrix V\", \"rank_V\", \".2f\", dir_path_sv)\n",
        "                        save_to_file_array(np.asarray([rank_layer_output]), singvals_ratio_fp, \"Rank of each layer output Y\", \"rank_layer_output\", \".2f\", dir_path_sv)\n",
        "\n",
        "                    def plot_hist_sv(hist_dict, title, path):\n",
        "                        for h, hist_list_h in hist_dict.items():\n",
        "                            fig,ax = plt.subplots(figsize=(8,8))\n",
        "                            plt.title(f\"Histogram of Singular Values of {title} for head {h+1}\")\n",
        "                            ax.set_xlabel(\"Singular Value\")\n",
        "                            ax.set_ylabel(\"Frequency\")\n",
        "                            plt.grid()\n",
        "                            colors = [cm.get_cmap(\"nipy_spectral\")(i) for i in np.linspace(0, 1, n_layers)]\n",
        "                            ax.set_prop_cycle(cycler('color', colors))\n",
        "                            for hist in hist_list_h:\n",
        "                                ax.step(hist['bins'][:-1], hist['counts'], label=hist['label'])\n",
        "                            plt.legend(bbox_to_anchor=(1.04, 1),loc='upper left')\n",
        "                            plt.show()\n",
        "                            fig_path = Path(path) / f'sing_val_{title}_{h+1}.jpg'\n",
        "                            fig.savefig(fig_path, bbox_inches='tight')\n",
        "                            plt.close()\n",
        "\n",
        "                    # plot_hist_sv(histograms[\"softmax\"], \"softmax\", dir_path_sv)\n",
        "                    # plot_hist_sv(histograms[\"W_v\"], \"W_v\", dir_path_sv)\n",
        "\n",
        "                    return None, None, attentions, None\n",
        "\n",
        "        elif config_dict['TASK_TYPE']=='eigenvalues_analysis':\n",
        "\n",
        "            for sample_idx in range(0,len(seq_ids)):\n",
        "                target_label = label_ids[sample_idx]\n",
        "\n",
        "                if target_label == config_dict['CLASS_LABELS'][selected_class] and target_label == np.argmax(logits[sample_idx]): # correct prediction\n",
        "                    Qi_KiT_eigvals_file = Path(math_interpret_dir) / \"von_neumann_entropy_Qi_KiT.txt\"\n",
        "                    symm_comp_eigvals_file = Path(math_interpret_dir) / \"von_neumann_entropy_symm_comp.txt\"\n",
        "\n",
        "                    with open(Qi_KiT_eigvals_file, 'w') as Qi_KiT_eigvals_fp, open(symm_comp_eigvals_file, 'w') as symm_comp_eigvals_fp:\n",
        "                        # get input embedding of sample for selected heads (shape=(n_layers, batch_dim, n_tokens_in_seq, emb_dim))\n",
        "                        VN_entropy_Qi_KiT_layer_sum = np.zeros(n_layers)\n",
        "                        VN_entropy_symm_comp_layer_sum = np.zeros(n_layers)\n",
        "                        Sh_entropy_Qi_KiT_layer_sum = np.zeros(n_layers)\n",
        "                        Sh_entropy_symm_comp_layer_sum = np.zeros(n_layers)\n",
        "                        count_heads_layer = np.zeros(n_layers)\n",
        "\n",
        "                        for layer, head in selected_layer_head_list:\n",
        "\n",
        "                            X = outputs.hidden_states[layer+1][sample_idx].cpu().detach().numpy() # token embeddings shape=(n_tokens_in_seq, emb_dim)=(512, 768)\n",
        "                            emb_dim = X.shape[1]\n",
        "                            d_k = int(emb_dim / n_heads)\n",
        "                            W_q = model_params[layer_query_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                            W_k = model_params[layer_key_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                            W_v = model_params[layer_value_weight_names[layer]].data.cpu().detach().numpy()\n",
        "                            W_q_i = W_q[:, head*d_k : head*d_k + d_k]\n",
        "                            W_k_i = W_k[:, head*d_k : head*d_k + d_k]\n",
        "                            W_v_i = W_v[:, head*d_k : head*d_k + d_k]\n",
        "\n",
        "                            Q_i = np.matmul(X, W_q_i)\n",
        "                            K_i = np.matmul(X, W_k_i)\n",
        "                            V_i = np.matmul(X, W_v_i)\n",
        "\n",
        "                            norm_Q_i = np.linalg.norm(Q_i, axis=1)\n",
        "                            norm_K_i = np.linalg.norm(K_i, axis=1)\n",
        "                            norm_V_i = np.linalg.norm(V_i, axis=1)\n",
        "\n",
        "                            singular_values_W_q_i = abs(np.linalg.svd(W_q_i, compute_uv=False))\n",
        "                            singular_values_W_k_i = abs(np.linalg.svd(W_k_i, compute_uv=False))\n",
        "                            singular_values_W_v_i = abs(np.linalg.svd(W_v_i, compute_uv=False))\n",
        "                            max_sv_W_q_i = max(singular_values_W_q_i)\n",
        "                            max_sv_W_k_i = max(singular_values_W_k_i)\n",
        "                            max_sv_W_v_i = max(singular_values_W_v_i)\n",
        "\n",
        "\n",
        "                            softmax_i = torch.softmax(torch.tensor(np.matmul(Q_i, K_i.T) / math.sqrt(d_k)), dim=1).cpu().detach().numpy()\n",
        "                            output_head_i = np.matmul(softmax_i, V_i)\n",
        "                            norm_output_head_i = np.linalg.norm(output_head_i, axis=1)\n",
        "                            singular_values_output_head_i = abs(np.linalg.svd(output_head_i, compute_uv=False))\n",
        "                            max_sv_output_head_i = max(singular_values_output_head_i)\n",
        "\n",
        "                            dit_path_head = Path(dir_path) / 'norm_Qi_Ki_Vi' / f'{layer+1}_{head+1}'\n",
        "                            if not os.path.exists(dit_path_head):\n",
        "                                os.makedirs(dit_path_head)\n",
        "\n",
        "                            # # histograms of singular values\n",
        "                            # fig = plt.figure(figsize=(8,8))\n",
        "                            # plt.hist(singular_values_W_k_i)\n",
        "                            # plt.title(f\"Histogram of singular values of W_k_i for head {head+1} in layer {layer+1}\")\n",
        "                            # plt.grid(linestyle = '--')\n",
        "                            # plt.yticks(list(plt.yticks()[0]) + [1])\n",
        "                            # plt.show()\n",
        "                            # dit_path_head = Path(dir_path) / 'hist_sing_val' / f'{layer+1}_{head+1}'\n",
        "                            # if not os.path.exists(dit_path_head):\n",
        "                            #     os.makedirs(dit_path_head)\n",
        "                            # fig_path = Path(dit_path_head) / f'{layer+1}_{head+1}.jpg'\n",
        "                            # fig.savefig(fig_path, bbox_inches='tight')\n",
        "                            # plt.close()\n",
        "\n",
        "                            # # histograms of norms of Q_i\n",
        "                            # fig,ax = plt.subplots(figsize=(8,8))\n",
        "                            # plt.hist(norm_Q_i)\n",
        "                            # plt.title(f\"Histogram of the norms of Q_i for head {head+1} in layer {layer+1}\\n(max SV of W_q_i: {max_sv_W_q_i})\")\n",
        "                            # plt.grid(linestyle = '--')\n",
        "                            # ax.set_xlabel(\"Norm of Q_i\")\n",
        "                            # ax.set_ylabel(\"Frequency\")\n",
        "                            # plt.show()\n",
        "                            # fig_path = Path(dit_path_head) / f'norm_Qi_{layer+1}_{head+1}.jpg'\n",
        "                            # fig.savefig(fig_path, bbox_inches='tight')\n",
        "                            # plt.close()\n",
        "\n",
        "                            # # histograms of norms of K_i\n",
        "                            # fig,ax = plt.subplots(figsize=(8,8))\n",
        "                            # plt.hist(norm_K_i)\n",
        "                            # plt.title(f\"Histogram of the norms of K_i for head {head+1} in layer {layer+1}\\n(max SV of W_k_i: {max_sv_W_k_i})\")\n",
        "                            # plt.grid(linestyle = '--')\n",
        "                            # ax.set_xlabel(\"Norm of K_i\")\n",
        "                            # ax.set_ylabel(\"Frequency\")\n",
        "                            # plt.show()\n",
        "                            # fig_path = Path(dit_path_head) / f'norm_Ki_{layer+1}_{head+1}.jpg'\n",
        "                            # fig.savefig(fig_path, bbox_inches='tight')\n",
        "                            # plt.close()\n",
        "\n",
        "                            # # histograms of norms of V_i\n",
        "                            # fig,ax = plt.subplots(figsize=(8,8))\n",
        "                            # plt.hist(norm_V_i)\n",
        "                            # plt.title(f\"Histogram of the norms of V_i for head {head+1} in layer {layer+1}\\n(max SV of W_v_i: {max_sv_W_v_i})\")\n",
        "                            # plt.grid(linestyle = '--')\n",
        "                            # ax.set_xlabel(\"Norm of V_i\")\n",
        "                            # ax.set_ylabel(\"Frequency\")\n",
        "                            # plt.show()\n",
        "                            # fig_path = Path(dit_path_head) / f'norm_Vi_{layer+1}_{head+1}.jpg'\n",
        "                            # fig.savefig(fig_path, bbox_inches='tight')\n",
        "                            # plt.close()\n",
        "\n",
        "\n",
        "                            # # histograms of norms of output_head_i\n",
        "                            # fig,ax = plt.subplots(figsize=(8,8))\n",
        "                            # plt.hist(norm_output_head_i)\n",
        "                            # plt.title(f\"Histogram of the norms of Y_i output for head {head+1} in layer {layer+1}\\n(max SV of Y_i: {max_sv_output_head_i})\")\n",
        "                            # plt.grid(linestyle = '--')\n",
        "                            # ax.set_xlabel(\"Norm of Y_i\")\n",
        "                            # ax.set_ylabel(\"Frequency\")\n",
        "                            # plt.show()\n",
        "                            # fig_path = Path(dit_path_head) / f'norm_Yi_{layer+1}_{head+1}.jpg'\n",
        "                            # fig.savefig(fig_path, bbox_inches='tight')\n",
        "                            # plt.close()\n",
        "\n",
        "\n",
        "                            # histogram of singular values of output_head_i\n",
        "                            fig,ax = plt.subplots(figsize=(8,8))\n",
        "                            plt.hist(singular_values_output_head_i)\n",
        "                            plt.title(f\"Histogram of Singular Values of Y_i output for head {head+1} in layer {layer+1}\\n(max SV of Y_i: {max_sv_output_head_i})\")\n",
        "                            plt.grid(linestyle = '--')\n",
        "                            ax.set_xlabel(\"Norm of Y_i\")\n",
        "                            ax.set_ylabel(\"Frequency\")\n",
        "                            plt.show()\n",
        "                            fig_path = Path(dit_path_head) / f'sing_val_Yi_{layer+1}_{head+1}.jpg'\n",
        "                            fig.savefig(fig_path, bbox_inches='tight')\n",
        "                            plt.close()\n",
        "\n",
        "\n",
        "                        #     # Von Neumann norms\n",
        "                        #     count_heads_layer[layer] += 1\n",
        "                        #     # Eigenvalues of QK^T\n",
        "                        #     Qi_KiT = np.matmul(Q_i, K_i.T)\n",
        "                        #     eigvals_Qi_KiT = np.linalg.eigvals(Qi_KiT)\n",
        "                        #     n_eigvals_zero_Qi_KiT = f\"{len(eigvals_Qi_KiT) - np.count_nonzero(eigvals_Qi_KiT)}/{len(eigvals_Qi_KiT)}\"\n",
        "                        #     # von neumann entropy and shannon entropy\n",
        "                        #     VN_entropy_Qi_KiT = compute_Von_Neumann_entropy_eigvals(Qi_KiT, f\"head{head+1}_layer_{layer+1}\")\n",
        "                        #     Sh_entropy_Qi_KiT = compute_Shannon_entropy(Qi_KiT, f\"head{head+1}_layer_{layer+1}\")\n",
        "                        #     Qi_KiT_eigvals_fp.write(f\"\\thead_{head+1}_layer_{layer+1} | n_eigv_zero={n_eigvals_zero_Qi_KiT} | VN_entropy_={VN_entropy_Qi_KiT}\\n\")\n",
        "                        #     VN_entropy_Qi_KiT_layer_sum[layer] += VN_entropy_Qi_KiT\n",
        "                        #     Sh_entropy_Qi_KiT_layer_sum[layer] += Sh_entropy_Qi_KiT\n",
        "\n",
        "                        #     # Eigenvalues of symmetric component\n",
        "                        #     symm_comp_mat = (Qi_KiT + Qi_KiT.T) / 2\n",
        "                        #     eigvals_symm_comp_mat = np.linalg.eigvals(symm_comp_mat)\n",
        "                        #     n_eigvals_zero_symm_comp_mat = f\"{len(eigvals_symm_comp_mat) - np.count_nonzero(eigvals_symm_comp_mat)}/{len(eigvals_symm_comp_mat)}\"\n",
        "                        #     # von neumann entropy\n",
        "                        #     VN_entropy_symm_comp_mat = compute_Von_Neumann_entropy_eigvals(symm_comp_mat, f\"head{head+1}_layer_{layer+1}\")\n",
        "                        #     Sh_entropy_symm_comp_mat = compute_Shannon_entropy(symm_comp_mat, f\"head{head+1}_layer_{layer+1}\")\n",
        "                        #     symm_comp_eigvals_fp.write(f\"\\thead_{head+1}_layer_{layer+1} | n_eigv_zero={n_eigvals_zero_symm_comp_mat} | VN_entropy_head_{head+1}layer_{layer+1} = {VN_entropy_symm_comp_mat}\\n\")\n",
        "                        #     VN_entropy_symm_comp_layer_sum[layer] += VN_entropy_symm_comp_mat\n",
        "                        #     Sh_entropy_symm_comp_layer_sum[layer] += Sh_entropy_symm_comp_mat\n",
        "\n",
        "                        # VN_entropy_Qi_KiT_layer_mean = VN_entropy_Qi_KiT_layer_sum / count_heads_layer\n",
        "                        # VN_entropy_symm_comp_layer_mean = VN_entropy_symm_comp_layer_sum / count_heads_layer\n",
        "                        # Sh_entropy_Qi_KiT_layer_mean = Sh_entropy_Qi_KiT_layer_sum / count_heads_layer\n",
        "                        # Sh_entropy_symm_comp_layer_mean = Sh_entropy_symm_comp_layer_sum / count_heads_layer\n",
        "                        # fig = plt.figure(figsize=(10,10))\n",
        "                        # x = [i+1 for i in range(n_layers)]\n",
        "                        # fig, ax = plt.subplots()\n",
        "                        # ax.plot(x, VN_entropy_Qi_KiT_layer_mean, 'b', label='VN entropy of QiKi^T')\n",
        "                        # ax.plot(x, VN_entropy_symm_comp_layer_mean, 'r', label='VN entropy of QiKi^T symm. comp.')\n",
        "                        # ax.axis('equal')\n",
        "                        # leg = ax.legend()\n",
        "                        # plt.title(f\"Average Von Neumann entropy for each layer\")\n",
        "                        # plt.grid(linestyle = '--')\n",
        "                        # ax.set_xticks(x)\n",
        "                        # #fig.yticks(list(plt.yticks()[0]) + [1])\n",
        "                        # ax.set_xlabel(\"Layer\")\n",
        "                        # ax.set_ylabel(\"VN entropy\")\n",
        "                        # plt.show()\n",
        "                        # fig_path = Path(dir_path) / f'VN_entropy_plot.jpg'\n",
        "                        # fig.savefig(fig_path, bbox_inches='tight')\n",
        "                        # plt.close()\n",
        "\n",
        "                        # fig = plt.figure(figsize=(10,10))\n",
        "                        # x = [i+1 for i in range(n_layers)]\n",
        "                        # fig, ax = plt.subplots()\n",
        "                        # ax.plot(x, Sh_entropy_Qi_KiT_layer_mean, 'b', label='Shannon entropy of QiKi^T')\n",
        "                        # ax.plot(x, Sh_entropy_symm_comp_layer_mean, 'r', label='Shannon entropy of QiKi^T symm. comp.')\n",
        "                        # ax.axis('equal')\n",
        "                        # leg = ax.legend()\n",
        "                        # plt.title(f\"Average Shannon entropy for each layer\")\n",
        "                        # plt.grid(linestyle = '--')\n",
        "                        # ax.set_xticks(x)\n",
        "                        # #fig.yticks(list(plt.yticks()[0]) + [1])\n",
        "                        # ax.set_xlabel(\"Layer\")\n",
        "                        # ax.set_ylabel(\"Shannon entropy\")\n",
        "                        # plt.show()\n",
        "                        # fig_path = Path(dir_path) / f'Shannon_entropy_plot.jpg'\n",
        "                        # fig.savefig(fig_path, bbox_inches='tight')\n",
        "                        # plt.close()\n",
        "\n",
        "                    return final_data_test, test_accuracies, attentions, timings\n",
        "\n",
        "    if config_dict['TASK_TYPE']=='attention_analysis' or config_dict['TASK_TYPE']=='attention_flow':\n",
        "        # calculate mean of attention matrices:\n",
        "        for target_label in config_dict['CLASS_LABELS'].values():\n",
        "            for layer in range(len(attentions_all_layers[target_label])):\n",
        "                for head in range(len(attentions_all_layers[target_label][layer])):\n",
        "                    attentions_all_layers[target_label][layer][head] = attentions_all_layers[target_label][layer][head] / count_class_samples[target_label]\n",
        "\n",
        "\n",
        "    if config_dict['TASK_TYPE']=='violin_plots':\n",
        "        for target_label in violin_plots_data.keys():\n",
        "            layer_head_data = np.asarray(violin_plots_data[target_label])\n",
        "            print(np.asarray(layer_head_data).shape)\n",
        "            fig, axs = plt.subplots(layer_head_data.shape[0], layer_head_data.shape[1], figsize=(20, 20), layout=\"constrained\")\n",
        "            tot_median_k_list = []\n",
        "            for i in range(layer_head_data.shape[0]):\n",
        "                for j in range(layer_head_data.shape[1]):\n",
        "                    ax = axs[i][j]\n",
        "                    sns.violinplot(x=layer_head_data[i][j], ax=ax, color='lightgrey')\n",
        "                    median_k = np.median(layer_head_data[i][j])\n",
        "                    stddev_k = np.std(layer_head_data[i][j])\n",
        "                    min_k = np.min(layer_head_data[i][j])\n",
        "                    ax.text(0.25, 0.9, f'[{i+1},{j+1}] {median_k} ({\"%.2f\" % min_k})', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
        "                    ax.set_xlim(100,500)\n",
        "                    tot_median_k_list.append(median_k)\n",
        "                    if median_k <= 300:\n",
        "                        ax.set_facecolor('palegreen')\n",
        "                    elif 300 < median_k <= 350:\n",
        "                        ax.set_facecolor('lightblue')\n",
        "                    # elif 400 < median_k <= 450 and stddev_k <= 25:\n",
        "                    #     ax.set_facecolor('palegreen')\n",
        "                    elif 350 < median_k <= 375:\n",
        "                        ax.set_facecolor('moccasin')\n",
        "                    elif median_k > 375:\n",
        "                        ax.set_facecolor('pink')\n",
        "            plt.show()\n",
        "            bins = np.linspace(math.ceil(min(tot_median_k_list)),\n",
        "                   math.floor(max(tot_median_k_list)),\n",
        "                   30) # fixed number of bins\n",
        "            plt.xlim([min(tot_median_k_list)-5, max(tot_median_k_list)+5])\n",
        "            plt.hist(tot_median_k_list, bins=bins, alpha=0.5)\n",
        "            plt.title('Histogram of median K')\n",
        "            plt.xlabel('Median K')\n",
        "            plt.ylabel('Count')\n",
        "            plt.show()\n",
        "\n",
        "        return None, None, None, None\n",
        "\n",
        "\n",
        "    if config_dict['TASK_TYPE']=='distance_cones_analysis':\n",
        "        dist_cones_path = Path(math_interpret_dir) / 'distance_cones'\n",
        "        if not os.path.exists(dist_cones_path):\n",
        "            os.makedirs(dist_cones_path)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=((7,4)))\n",
        "        ax.plot(list(distance_cones_1_sample.keys()), list(distance_cones_1_sample.values()), '-o')\n",
        "        ax.set_title('Cone index of one sequence of class Omicron')\n",
        "        ax.set_xlabel(\"Layer\")\n",
        "        ax.set_ylabel(\"Index\")\n",
        "        ax.set_xticks(list(distance_cones_1_sample.keys()))\n",
        "        ax.set_xticklabels(np.asarray(list(distance_cones_1_sample.keys()))+1)\n",
        "        ax.grid()\n",
        "        plt.show()\n",
        "        fig_path = Path(dist_cones_path) / f'distance_cones_1_sample.jpg'\n",
        "        fig.savefig(fig_path)\n",
        "        fig.clear()\n",
        "\n",
        "        for layer, layer_distance_cones in distance_cones.items():\n",
        "            n_bins=20\n",
        "            distance_cones_1 = {k:v for k,v in layer_distance_cones.items() if k in ['Layer input', 'Multihead output', 'Layer output']}\n",
        "            fig, ax = plt.subplots(figsize=((7,4)))\n",
        "            ax.hist(distance_cones_1.values(), n_bins, histtype='step', stacked=True, fill=False, label=layer_distance_cones.keys())\n",
        "            ax.set_title('L1 cone index')\n",
        "            ax.legend(prop={'size': 10})\n",
        "            ax.set_xlabel(\"Cone index\")\n",
        "            ax.set_ylabel(\"Frequency\")\n",
        "            box = ax.get_position()\n",
        "            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "            ax.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "            plt.show()\n",
        "            fig_path = Path(dist_cones_path) / f'distance_cones_L{layer}_1.jpg'\n",
        "            fig.savefig(fig_path)\n",
        "            fig.clear()\n",
        "\n",
        "            distance_cones_2 = {k:v for k,v in layer_distance_cones.items() if k in ['Layer input', 'Head output']}\n",
        "            fig, ax = plt.subplots(figsize=((7,4)))\n",
        "            ax.hist(distance_cones_2.values(), n_bins, histtype='step', stacked=True, fill=False, label=layer_distance_cones.keys())\n",
        "            ax.set_title('L1 cone index')\n",
        "            ax.legend(prop={'size': 10})\n",
        "            ax.set_xlabel(\"Cone index\")\n",
        "            ax.set_ylabel(\"Frequency\")\n",
        "            box = ax.get_position()\n",
        "            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "            ax.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "            plt.show()\n",
        "            fig_path = Path(dist_cones_path) / f'distance_cones_L{layer}_2.jpg'\n",
        "            fig.savefig(fig_path)\n",
        "            fig.clear()\n",
        "\n",
        "    if config_dict['TASK_TYPE']=='simple_test':\n",
        "        # timing\n",
        "        mean_syn = np.sum(timings) / len(timings)\n",
        "        std_syn = np.std(timings)\n",
        "\n",
        "    if config_dict['TASK_TYPE']=='cone_index_sing_vals_histograms':\n",
        "        fig_dir = Path(math_interpret_dir) / 'cone_index_sing_vals_histograms_layer11_head0'\n",
        "        if not os.path.exists(fig_dir):\n",
        "            os.makedirs(fig_dir)\n",
        "        n_bins=15\n",
        "        fig, ax = plt.subplots(figsize=((15,9)))\n",
        "        ax.hist(cone_indices_at_softmax, n_bins, histtype='step', stacked=True, fill=False, label='Histogram of cone indices')\n",
        "        ax.set_title('Histogram of cone indices of head 1 in layer 11, considering 30 random samples of class Omicron')\n",
        "        ax.legend(prop={'size': 10})\n",
        "        ax.set_xlabel(\"Cone index\")\n",
        "        ax.set_ylabel(\"Frequency\")\n",
        "        box = ax.get_position()\n",
        "        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "        ax.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "        plt.show()\n",
        "        fig_path = Path(fig_dir) / f'cone_indices_histogram_L11_H1.jpg'\n",
        "        fig.savefig(fig_path)\n",
        "        fig.clear()\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=((15,9)))\n",
        "        ax.hist(sing_vals_at_softmax, n_bins, histtype='step', stacked=True, fill=False, label='Histogram of n. singular values < 0.001')\n",
        "        ax.set_title('Histogram of n. singular values < 0.001 of head 1 in layer 11, considering 30 random samples of class Omicron')\n",
        "        ax.legend(prop={'size': 10})\n",
        "        ax.set_xlabel(\"N. singular values < 0.001\")\n",
        "        ax.set_ylabel(\"Frequency\")\n",
        "        box = ax.get_position()\n",
        "        ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "        ax.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "        plt.show()\n",
        "        fig_path = Path(fig_dir) / f'n_sing_vals_zero_histogram_L11_H1.jpg'\n",
        "        fig.savefig(fig_path)\n",
        "        fig.clear()\n",
        "\n",
        "\n",
        "    print('DONE.')\n",
        "\n",
        "    # token_base_positions_axis = [f\"{i*config_dict['STRIDE']-config_dict['STRIDE']+spike_gene_start}\" for i in range(0,config_dict['MAX_LENGTH'])]\n",
        "\n",
        "    attentions = {'attentions_all_layers' : attentions_all_layers,\n",
        "                  'attentions_all_layers_thresh' : attentions_all_layers_thresh,\n",
        "                  'repr_token_base_positions_axis' : repr_token_base_positions_axis\n",
        "                  }\n",
        "\n",
        "    #print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))\n",
        "    # Report the final accuracy for this validation run.\n",
        "    # avg_test_accuracy = total_test_acuracy / len(test_dataloader)\n",
        "    # print(\"  Global Test Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
        "    # log_fp_test.write(\"  Global Test Accuracy: {0:.2f}\\n\".format(avg_test_accuracy))\n",
        "\n",
        "    return final_data_test, test_accuracies, attentions, timings\n",
        "\n",
        "def show_test_plots(accuracies):\n",
        "    # Create a barplot showing the accuracy score for each batch of test samples.\n",
        "    fig = plt.figure(figsize=(12,6))\n",
        "    ax = sns.lineplot(x=list(range(len(accuracies))), y=accuracies, ci=None)\n",
        "\n",
        "    plt.title('Accuracy per Batch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Batch #')\n",
        "\n",
        "    plt.show()\n",
        "    fig_path = Path(outputs_dir) / 'test.jpg'\n",
        "    fig.savefig(fig_path)\n",
        "\n",
        "\n",
        "def plot_color_gradients(cmap_list, labels=None, title=''):\n",
        "    # Create figure and adjust figure height to number of colormaps\n",
        "    nrows = len(cmap_list)\n",
        "    figh = 0.35 + 0.15 + (nrows + (nrows-1)*0.1)*0.22\n",
        "    fig, axs = plt.subplots(nrows=nrows, figsize=(6.4, figh))\n",
        "    fig.subplots_adjust(top=1-.35/figh, bottom=.15/figh, left=0.2, right=0.99)\n",
        "\n",
        "    axs[0].set_title(title, fontsize=14)\n",
        "    gradient = np.linspace(0, 1, 256)\n",
        "    gradient = np.vstack((gradient, gradient))\n",
        "\n",
        "    i=0\n",
        "    for ax, cmap_name in zip(axs, cmap_list):\n",
        "        if labels == None:\n",
        "            ax.imshow(gradient, aspect='auto', cmap=cmap_name)\n",
        "            ax.text(-.01, .5, f'{i}', va='center', ha='right', fontsize=10,\n",
        "                transform=ax.transAxes)\n",
        "        else:\n",
        "            if i not in labels:\n",
        "                break\n",
        "            ax.imshow(gradient, aspect='auto', cmap=cmap_name)\n",
        "            ax.text(-.01, .5, f'{labels[i]}', va='center', ha='right', fontsize=10,\n",
        "                transform=ax.transAxes)\n",
        "        i+=1\n",
        "\n",
        "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
        "    for ax in axs:\n",
        "        ax.set_axis_off()\n",
        "\n",
        "    plt.show()\n",
        "    fig_path = Path(attention_matrices_dir) / f'{title}.jpg'\n",
        "    fig.savefig(fig_path, bbox_inches='tight', pad_inches=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c5m5d7vkPJg"
      },
      "outputs": [],
      "source": [
        "# bins = pd.cut(mat.ravel(), bins=20)\n",
        "#     shannon_entropy=0\n",
        "#     tot_count = len(mat.ravel())\n",
        "#     pjs = pd.value_counts(bins) / tot_countdicta = {'a': [1,2,3,4], 'b': [1,3,5,7], \"c\": [5,12,13]}\n",
        "# lista = []\n",
        "# for k, v in dicta.items():\n",
        "#     lista.extend(v)\n",
        "# unique, count = np.unique(lista, return_counts= True)\n",
        "# list1occ = [x for idx, x in enumerate(unique) if count[idx]==1]\n",
        "# dictaf = {k:[x for x in v if x in list1occ] for k,v in dicta.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxcgT9ogVW7U"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Om9LG4uSEfy"
      },
      "outputs": [],
      "source": [
        "if do_test:\n",
        "    try:\n",
        "        test_data_size = sizes_info['test_data_size_seqs'] #-1 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
        "    except NameError:\n",
        "        test_data_size = sum(1 for line in open(test_file))\n",
        "\n",
        "    print(test_data_size)\n",
        "\n",
        "    with open(test_file) as test_fp, open(log_file, 'a') as log_fp:\n",
        "        test_reader = csv.reader(test_fp, delimiter=',')\n",
        "        test_metadata = {'len':test_data_size}\n",
        "        X_test_generator = DatasetGenerator(test_reader, test_fp, test_metadata)\n",
        "        test_dataloader = DataLoader(\n",
        "                X_test_generator, # The validation samples.\n",
        "                sampler = SequentialSampler(X_test_generator), # Pull out batches sequentially.\n",
        "                batch_size = config_dict['EVAL_BATCH_SIZE'], # Evaluate with this batch size.\n",
        "                num_workers = 0\n",
        "        )\n",
        "\n",
        "        if config_dict['TASK_TYPE']=='eigenvalues_analysis' \\\n",
        "            or config_dict['TASK_TYPE']=='violin_plots' \\\n",
        "            or config_dict['TASK_TYPE']=='singularvalues_ratio_analysis' \\\n",
        "            or config_dict['TASK_TYPE']=='distance_cones_analysis' \\\n",
        "            or config_dict['TASK_TYPE']=='layer_output_analysis':\n",
        "            selected_layer_head_list = config_dict['SELECTED_LAYER_HEAD_LIST']\n",
        "            selected_class = config_dict['SELECTED_CLASS']\n",
        "        else:\n",
        "            selected_layer_head_list = None\n",
        "            selected_class = None\n",
        "\n",
        "        if config_dict['TASK_TYPE']=='singularvalues_ratio_analysis':\n",
        "            model_base = BertForSequenceClassification.from_pretrained(\n",
        "                \"bert-base-cased\",\n",
        "                num_labels=config_dict['N_CLASSES'],\n",
        "                output_attentions = True, # Whether the model returns attentions weights.\n",
        "                output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "            )\n",
        "            model_base.resize_token_embeddings(len(tokenizer))\n",
        "            model_base.cuda()\n",
        "\n",
        "            model_finetuned = model\n",
        "\n",
        "            print(\"\\n------- Base model -------\")\n",
        "            # supervisedBertClassifierTest(\n",
        "            #     model_base,\n",
        "            #     test_dataloader,\n",
        "            #     log_fp,\n",
        "            #     config_dict['THETA'],\n",
        "            #     selected_layer_head_list=selected_layer_head_list,\n",
        "            #     selected_class=selected_class,\n",
        "            #     dir_path=math_interpret_dir,\n",
        "            #     title=\"covid_embeddings_base_model\"\n",
        "            #     )\n",
        "\n",
        "            print(\"\\n------- Fine-tuned model -------\")\n",
        "            supervisedBertClassifierTest(\n",
        "                model_finetuned,\n",
        "                test_dataloader,\n",
        "                log_fp,\n",
        "                config_dict['THETA'],\n",
        "                selected_layer_head_list=selected_layer_head_list,\n",
        "                selected_class=selected_class,\n",
        "                dir_path=math_interpret_dir,\n",
        "                title=\"covid_embeddings_finetuned_model\"\n",
        "                )\n",
        "        else:\n",
        "            final_data_test, test_accuracies, attentions, timings = supervisedBertClassifierTest(\n",
        "                model,\n",
        "                test_dataloader,\n",
        "                log_fp,\n",
        "                config_dict['THETA'],\n",
        "                selected_layer_head_list=selected_layer_head_list,\n",
        "                selected_class=selected_class,\n",
        "                dir_path=math_interpret_dir\n",
        "                )\n",
        "        # show_attention_plots(attentions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqzgsVaGE658"
      },
      "outputs": [],
      "source": [
        "if do_test and config_dict['TASK_TYPE']=='simple_test':\n",
        "    mean_syn = np.sum(timings) / len(timings)\n",
        "    std_syn = np.std(timings)\n",
        "    print(f\"Inference time: {mean_syn} ({std_syn})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6xP5KjUG-v3"
      },
      "outputs": [],
      "source": [
        "# if config_dict['TASK_TYPE']=='clustering':\n",
        "#     del final_data_test['seq_ids']\n",
        "#     del final_data_test['positions']\n",
        "#     del final_data_test['outputs']\n",
        "#     del final_data_test['logits']\n",
        "#     del final_data_test['output_embeddings']\n",
        "#     gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXfhkYowiTDG"
      },
      "outputs": [],
      "source": [
        "if do_test or config_dict['TASK_TYPE']=='simple_test' or config_dict['TASK_TYPE']=='clustering':\n",
        "    # save or load test accuracies:\n",
        "    if os.path.exists(test_accuracies_file):\n",
        "        test_accuracies = pickle.load(open(test_accuracies_file, 'rb'))\n",
        "        #test_results = test_results[0]\n",
        "    else:\n",
        "        # save output data on file for furure computation:\n",
        "        pickle.dump(test_accuracies, open(test_accuracies_file, 'wb'))\n",
        "\n",
        "    # save or load test data:\n",
        "    if os.path.exists(final_data_test_file):\n",
        "        final_data_test = pickle.load(open(final_data_test_file, 'rb'))\n",
        "    else:\n",
        "        # save output data on file for furure computation:\n",
        "        pickle.dump(final_data_test, open(final_data_test_file, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weights heatmaps and histograms\n"
      ],
      "metadata": {
        "id": "D6gE2pYYgDDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "if do_finetuning:\n",
        "    mod_f = model_file_finetuned\n",
        "else:\n",
        "    mod_f = model_file\n",
        "if os.path.exists(mod_f):\n",
        "    # model.load_state_dict(torch.load(mod_f, map_location=torch.device('cpu')))\n",
        "    # device = torch.device(\"cpu\")\n",
        "    model.load_state_dict(torch.load(mod_f, map_location=torch.device('cpu')))\n",
        "model_params = dict(model.named_parameters())\n",
        "# print(model_params[f\"bert.encoder.layer.{layer}.attention.self.value.weight\"].data.cpu().detach().numpy().shape)\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name, param.requires_grad)\n",
        "# pattern1 = re.compile(\"bert.encoder.layer.([0-9]|1[0-1]).attention.output.dense.weight\")\n",
        "# pattern2 = re.compile(\"bert.encoder.layer.([0-9]|1[0-1]).attention.output.dense.bias\")\n",
        "\n",
        "\n",
        "for layer_name in ['intermediate.dense', 'output.dense']:   #attention.output.dense\n",
        "    # pattern = re.compile(f\"bert.encoder.layer.([0-9]|1[0-1]).{layer_name}.dense.weight\")\n",
        "    # model_params = {name: param.detach().numpy() for name, param in model.named_parameters() if pattern.match(name)}\n",
        "    weights_heatmaps_dir = Path(math_interpret_dir) / \"weights_heatmaps_dir\"\n",
        "    if not os.path.exists(weights_heatmaps_dir):\n",
        "        os.makedirs(weights_heatmaps_dir)\n",
        "        print(f\"Directory '{weights_heatmaps_dir}' created\")\n",
        "    weights_histograms_dir = Path(weights_heatmaps_dir) / \"weights_histograms\"\n",
        "    if not os.path.exists(weights_histograms_dir):\n",
        "        os.makedirs(weights_histograms_dir)\n",
        "        print(f\"Directory '{weights_histograms_dir}' created\")\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        params = model_params[f'bert.encoder.layer.{i}.{layer_name}.weight'].detach().numpy()\n",
        "        params /= np.max(np.abs(params))\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(20,20))\n",
        "        hm = sns.heatmap(params*100, ax=ax, cmap='bwr')\n",
        "        ax.set_title(f\"Weight matrix of {layer_name.replace('.', ' ')} at layer {i+1}\")\n",
        "        plt.show()\n",
        "        fig_path = Path(weights_heatmaps_dir) / f\"weights_{layer_name.replace('.', '_')}_{i+1}.png\"\n",
        "        fig.savefig(fig_path, bbox_inches='tight')\n",
        "\n",
        "        # fig,ax = plt.subplots(figsize=(10,10))\n",
        "        # plt.hist(params)\n",
        "        # plt.title(f\"Histogram of weights of {layer_name.replace('.', ' ')} at layer {i+1}\")\n",
        "        # plt.grid(linestyle = '--')\n",
        "        # ax.set_xlabel(\"Value\")\n",
        "        # ax.set_ylabel(\"Frequency\")\n",
        "        # plt.show()\n",
        "        # fig_path = Path(weights_histograms_dir) / f\"histogram_weights_{layer_name.replace('.', '_')}_{i+1}.png\"\n",
        "        # fig.savefig(fig_path, bbox_inches='tight')\n",
        "        # plt.close()"
      ],
      "metadata": {
        "id": "yBzq2MmNgHkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgUoO9QSnf9t"
      },
      "source": [
        "#### Test statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LljujgjIcLYF"
      },
      "outputs": [],
      "source": [
        "if do_test or config_dict['TASK_TYPE']=='simple_test':\n",
        "    show_test_plots(test_accuracies)\n",
        "\n",
        "    output_labels_test = np.argmax(final_data_test['outputs'],axis=1)\n",
        "    target_labels_test = np.argmax(final_data_test['targets'],axis=1)\n",
        "    output_logits_test = final_data_test['outputs']\n",
        "    final_statistics(target_labels_test, output_labels_test, output_logits_test, log_file, 'Test') #, logits=final_data_test['logits'])\n",
        "\n",
        "    if config_dict['SPLIT_DATA_IN_CHUNKS']:\n",
        "        #per_sample_result_computation(final_data_test, best_positions, taskname=\"Test\")\n",
        "        per_sample_result_computation(final_data_test, best_positions, taskname=\"Test\", filter_positions=True, filter_score=True, min_score=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6b_5g2QCs28"
      },
      "source": [
        "###Attention analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWTnELSGgaQI"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='attention_analysis' or config_dict['TASK_TYPE']=='attention_flow' or config_dict['TASK_TYPE']=='von_neumann_entropy_attentions' or config_dict['TASK_TYPE']=='shannon_entropy_attentions':\n",
        "    attention_matrices_file = Path(attention_matrices_dir) / 'attention_matrices_np'\n",
        "    attention_matrices_thresh_file = Path(attention_matrices_dir) / 'attention_matrices_thresh_np'\n",
        "    ticks_file = Path(attention_matrices_dir) / 'ticks_np'\n",
        "    attentions2 = {}\n",
        "\n",
        "    if os.path.exists(attention_matrices_file):\n",
        "        attentions2['attentions_all_layers'] = pickle.load(open(attention_matrices_file, 'rb'))\n",
        "    else:\n",
        "        pickle.dump(attentions['attentions_all_layers'], open(attention_matrices_file, 'wb'))\n",
        "        attentions2['attentions_all_layers'] = attentions['attentions_all_layers']\n",
        "\n",
        "    if os.path.exists(attention_matrices_thresh_file):\n",
        "        attentions2['attentions_all_layers_thresh'] = pickle.load(open(attention_matrices_thresh_file, 'rb'))\n",
        "    else:\n",
        "        pickle.dump(attentions['attentions_all_layers_thresh'], open(attention_matrices_thresh_file, 'wb'))\n",
        "        attentions2['attentions_all_layers_thresh'] = attentions['attentions_all_layers_thresh']\n",
        "\n",
        "    if os.path.exists(ticks_file):\n",
        "        attentions2['repr_token_base_positions_axis'] = pickle.load(open(ticks_file, 'rb'))\n",
        "    else:\n",
        "        pickle.dump(attentions['repr_token_base_positions_axis'], open(ticks_file, 'wb'))\n",
        "        attentions2['repr_token_base_positions_axis'] = attentions['repr_token_base_positions_axis']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbW86Qc0nUGX"
      },
      "outputs": [],
      "source": [
        "# np.asarray(attentions2['attentions_all_layers'][7][9][11]).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzgnLrEvAyI4"
      },
      "outputs": [],
      "source": [
        "def attention_analysis(attentions, log_fp_test, theta=0, selected_classes=None, selected_layers=None, selected_heads=None):\n",
        "    # token_base_positions_axis = attentions['token_base_positions_axis']\n",
        "    # attentions_last_layer = attentions['attentions_last_layer']\n",
        "    attentions_all_layers = attentions['attentions_all_layers'] if 'attentions_all_layers' in attentions.keys() else None\n",
        "    attentions_all_layers_thresh = attentions['attentions_all_layers_thresh']\n",
        "    repr_token_base_positions_axis = attentions['repr_token_base_positions_axis']\n",
        "\n",
        "    attn_last_layers_sum_dir = Path(attention_matrices_dir) / \"last_layers_sum\"\n",
        "\n",
        "    if selected_classes == 'avg':\n",
        "        # attn_mats = np.asarray([attentions_all_layers[cls][layer][head].cpu().detach().numpy()\n",
        "        #                         for cls in attentions_all_layers.keys()\n",
        "        #                         for layer in selected_layers\n",
        "        #                         for head in selected_heads])\n",
        "        # attn_mats = attn_mats.reshape(len(attentions_all_layers.keys()), len(selected_layers), len(selected_heads), attentions_all_layers[0][0][0].shape[0], attentions_all_layers[0][0][0].shape[1])\n",
        "        # attn_mats = attn_mats.transpose(1,2,0)\n",
        "        attn_mats = {}\n",
        "        for cls in attentions_all_layers.keys():\n",
        "            for layer in range(12):\n",
        "                if layer not in attn_mats:\n",
        "                    attn_mats[layer] = {}\n",
        "                for head in range(12):\n",
        "                    if head not in attn_mats[layer]:\n",
        "                        attn_mats[layer][head] = []\n",
        "                    attn_mats[layer][head].append(attentions_all_layers[cls][layer][head].cpu().detach().numpy())\n",
        "        for layer in selected_layers:\n",
        "            for head in selected_heads:\n",
        "                attn_mat = np.asarray(attn_mats[layer][head]).mean(axis=0)\n",
        "                plt_attentions(attn_mat,\n",
        "                                repr_token_base_positions_axis[0],\n",
        "                                attn_last_layers_sum_dir,\n",
        "                                theta=theta,\n",
        "                                filename=f\"avg_{layer+1}_{head+1}\",\n",
        "                                title=f\"Average of attention matrices of head {head+1} of layer {layer+1} over all classes, theta = {theta:.3f}\",\n",
        "                                # cmap=colormaps_layers[layer]\n",
        "                                )\n",
        "    else:\n",
        "        for target_label in selected_classes:\n",
        "            # percentile = None\n",
        "            for layer in selected_layers:\n",
        "                if selected_heads == 'avg':\n",
        "                    #calculate the avg of attention matrices of heads of current layer\n",
        "                    attn_mat = attentions_all_layers[target_label][layer].mean(dim=0).cpu().detach().numpy()\n",
        "                    plt_attentions(attn_mat,\n",
        "                                repr_token_base_positions_axis[target_label],\n",
        "                                attn_last_layers_sum_dir,\n",
        "                                theta=theta,\n",
        "                                filename=f\"{inv_class_labels_dict[int(target_label)]}_{layer+1}\",\n",
        "                                title=f\"Average of attention matrices of heads of layer {layer+1} for class '{inv_class_labels_dict[int(target_label)]}', theta = {theta:.3f}\",\n",
        "                                # cmap=colormaps_layers[layer]\n",
        "                                )\n",
        "                else:\n",
        "                    for head in selected_heads:\n",
        "                        attn_mat = attentions_all_layers[target_label][layer][head].cpu().detach().numpy()\n",
        "                        plt_attentions(attn_mat,\n",
        "                                repr_token_base_positions_axis[target_label],\n",
        "                                attn_last_layers_sum_dir,\n",
        "                                theta=theta,\n",
        "                                filename=f\"{inv_class_labels_dict[int(target_label)]}_{layer+1}_{head+1}\",\n",
        "                                title=f\"Attention matrix of head {head+1} of layer {layer+1} for class '{inv_class_labels_dict[int(target_label)]}', theta = {theta:.3f}\",\n",
        "                                # cmap=colormaps_layers[layer]\n",
        "                                )\n",
        "\n",
        "def attention_analysis_proportions(attentions, log_fp_test, theta=0):\n",
        "    attentions_all_layers = attentions['attentions_all_layers'] if 'attentions_all_layers' in attentions.keys() else None\n",
        "    # attentions_all_layers_thresh = attentions['attentions_all_layers_thresh']\n",
        "    ##### ATTENTIONS BY DOMAIN ######\n",
        "    show_attentions_for_each_domain(attentions_all_layers, proportion_attn_domains_dir, proportion=\"attention\")\n",
        "    # show_attentions_for_each_domain(attentions_all_layers_thresh, proportion_attn_domains_dir, proportion=\"high-attention-tokens-count\", theta=theta)\n",
        "    # attention_pct, pvalues = top_heads_attention_proportions(attentions_all_layers_thresh, proportion=\"high-confidence-attention\", min_total=0, theta=theta)\n",
        "    # print(f\"\\nPercentages of attention of maximally attentive heads:\\n\")\n",
        "    # print(attention_pct.to_markdown())\n",
        "    # print(f\"\\nP-values of attention of maximally attentive heads with respect to background frequencies:\\n\")\n",
        "    # print(pvalues.to_markdown())\n",
        "    # log_fp_test.write(f\"\\nP-values of attention of maximally attentive heads with respect to background frequencies:\\n\")\n",
        "    # log_fp_test.write(f\"{pvalues.to_markdown()}\\n\")\n",
        "    # log_fp_test.write(f\"\\nPercentages of attention of maximally attentive heads:\\n\")\n",
        "    # log_fp_test.write(f\"{attention_pct.to_markdown()}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_HDUJ_T1F8E"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='attention_analysis':\n",
        "    with open(test_file) as test_fp, open(log_file, 'a') as log_fp:\n",
        "        selected_classes = [0] #[1]  #range(config_dict['N_CLASSES']) #'avg'\n",
        "        selected_layers = range(n_layers) #range(n_layers) #[10] #range(n_layers)\n",
        "        selected_heads = range(n_heads) #range(n_heads) #[4] #range(n_heads) #'avg'\n",
        "        theta_plot = 0.01 #0.01\n",
        "        attention_analysis(attentions2, log_fp, theta_plot, selected_classes, selected_layers, selected_heads)\n",
        "        #attention_analysis_proportions(attentions2, log_fp) #theta=0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEAkAWrVv9Nb"
      },
      "source": [
        "### Attention flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BsO_mdUwAGP"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "\n",
        "def get_adjmat(mat, input_tokens):\n",
        "    n_layers, length, _ = mat.shape\n",
        "    adj_mat = np.zeros(((n_layers+1)*length, (n_layers+1)*length))\n",
        "    labels_to_index = {}\n",
        "    for k in np.arange(length):\n",
        "        labels_to_index[str(k)+\"_\"+input_tokens[k]] = k\n",
        "\n",
        "    for i in np.arange(1,n_layers+1):\n",
        "        for k_f in np.arange(length):\n",
        "            index_from = (i)*length+k_f\n",
        "            label = \"L\"+str(i)+\"_\"+str(k_f)\n",
        "            labels_to_index[label] = index_from\n",
        "            for k_t in np.arange(length):\n",
        "                index_to = (i-1)*length+k_t\n",
        "                adj_mat[index_from][index_to] = mat[i-1][k_f][k_t]\n",
        "\n",
        "    return adj_mat, labels_to_index\n",
        "\n",
        "\n",
        "def draw_attention_graph(adjmat, labels_to_index, n_layers, length):\n",
        "    A = adjmat\n",
        "    G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
        "    for i in np.arange(A.shape[0]):\n",
        "        for j in np.arange(A.shape[1]):\n",
        "            nx.set_edge_attributes(G, {(i,j): A[i,j]}, 'capacity')\n",
        "\n",
        "    pos = {}\n",
        "    label_pos = {}\n",
        "    for i in np.arange(n_layers+1):\n",
        "        for k_f in np.arange(length):\n",
        "            pos[i*length+k_f] = ((i+0.5)*2, length - k_f)\n",
        "            label_pos[i*length+k_f] = (i*2, length - k_f)\n",
        "\n",
        "    index_to_labels = {}\n",
        "    for key in labels_to_index:\n",
        "        index_to_labels[labels_to_index[key]] = key.split(\"_\")[-1]\n",
        "        if labels_to_index[key] >= length:\n",
        "            index_to_labels[labels_to_index[key]] = ''\n",
        "\n",
        "    #plt.figure(1,figsize=(20,12))\n",
        "\n",
        "    nx.draw_networkx_nodes(G,pos,node_color='green', node_size=50)\n",
        "    nx.draw_networkx_labels(G,pos=label_pos, labels=index_to_labels, font_size=10)\n",
        "\n",
        "    all_weights = []\n",
        "    #4 a. Iterate through the graph nodes to gather all the weights\n",
        "    for (node1,node2,data) in G.edges(data=True):\n",
        "        all_weights.append(data['weight']) #we'll use this when determining edge thickness\n",
        "\n",
        "    #4 b. Get unique weights\n",
        "    unique_weights = list(set(all_weights))\n",
        "\n",
        "    #4 c. Plot the edges - one by one!\n",
        "    print(\"Plot the edges - one by one!\")\n",
        "    for weight in tqdm(unique_weights):\n",
        "        #4 d. Form a filtered list with just the weight you want to draw\n",
        "        weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n",
        "        #4 e. I think multiplying by [num_nodes/sum(all_weights)] makes the graphs edges look cleaner\n",
        "\n",
        "        w = weight #(weight - min(all_weights))/(max(all_weights) - min(all_weights))\n",
        "        width = w\n",
        "        nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,width=width, edge_color='darkblue')\n",
        "\n",
        "    return G\n",
        "\n",
        "def get_attention_graph(adjmat, labels_to_index, n_layers, length):\n",
        "    A = adjmat\n",
        "    G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
        "    for i in np.arange(A.shape[0]):\n",
        "        for j in np.arange(A.shape[1]):\n",
        "            nx.set_edge_attributes(G, {(i,j): A[i,j]}, 'capacity')\n",
        "\n",
        "    pos = {}\n",
        "    label_pos = {}\n",
        "    for i in np.arange(n_layers+1):\n",
        "        for k_f in np.arange(length):\n",
        "            pos[i*length+k_f] = ((i+0.5)*2, length - k_f)\n",
        "            label_pos[i*length+k_f] = (i*2, length - k_f)\n",
        "\n",
        "    index_to_labels = {}\n",
        "    for key in labels_to_index:\n",
        "        index_to_labels[labels_to_index[key]] = key.split(\"_\")[-1]\n",
        "        if labels_to_index[key] >= length:\n",
        "            index_to_labels[labels_to_index[key]] = ''\n",
        "\n",
        "    #plt.figure(1,figsize=(20,12))\n",
        "\n",
        "    nx.draw_networkx_nodes(G,pos,node_color='green', node_size=50)\n",
        "    nx.draw_networkx_labels(G,pos=label_pos, labels=index_to_labels, font_size=10)\n",
        "\n",
        "    all_weights = []\n",
        "    #4 a. Iterate through the graph nodes to gather all the weights\n",
        "    for (node1,node2,data) in G.edges(data=True):\n",
        "        all_weights.append(data['weight']) #we'll use this when determining edge thickness\n",
        "\n",
        "    #4 b. Get unique weights\n",
        "    unique_weights = list(set(all_weights))\n",
        "\n",
        "    #4 c. Plot the edges - one by one!\n",
        "    for weight in unique_weights:\n",
        "        #4 d. Form a filtered list with just the weight you want to draw\n",
        "        weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n",
        "        #4 e. I think multiplying by [num_nodes/sum(all_weights)] makes the graphs edges look cleaner\n",
        "\n",
        "        w = weight #(weight - min(all_weights))/(max(all_weights) - min(all_weights))\n",
        "        width = w\n",
        "        nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,width=width, edge_color='darkblue')\n",
        "\n",
        "    return G\n",
        "\n",
        "def compute_flows(G, labels_to_index, input_nodes, length):\n",
        "    print(\"Computing flows\")\n",
        "    number_of_nodes = len(labels_to_index)\n",
        "    flow_values=np.zeros((number_of_nodes,number_of_nodes))\n",
        "    for key in tqdm(labels_to_index):\n",
        "        if key not in input_nodes:\n",
        "            current_layer = int(labels_to_index[key] / length)\n",
        "            pre_layer = current_layer - 1\n",
        "            u = labels_to_index[key]\n",
        "            for inp_node_key in input_nodes:\n",
        "                v = labels_to_index[inp_node_key]\n",
        "                flow_value = nx.maximum_flow_value(G,u,v, flow_func=nx.algorithms.flow.edmonds_karp)\n",
        "                flow_values[u][pre_layer*length+v ] = flow_value\n",
        "            flow_values[u] /= flow_values[u].sum()\n",
        "\n",
        "    return flow_values\n",
        "\n",
        "def compute_node_flow(G, labels_to_index, input_nodes, output_nodes,length):\n",
        "    number_of_nodes = len(labels_to_index)\n",
        "    flow_values=np.zeros((number_of_nodes,number_of_nodes))\n",
        "    for key in output_nodes:\n",
        "        if key not in input_nodes:\n",
        "            current_layer = int(labels_to_index[key] / length)\n",
        "            pre_layer = current_layer - 1\n",
        "            u = labels_to_index[key]\n",
        "            for inp_node_key in input_nodes:\n",
        "                v = labels_to_index[inp_node_key]\n",
        "                flow_value = nx.maximum_flow_value(G,u,v, flow_func=nx.algorithms.flow.edmonds_karp)\n",
        "                flow_values[u][pre_layer*length+v ] = flow_value\n",
        "            flow_values[u] /= flow_values[u].sum()\n",
        "\n",
        "    return flow_values\n",
        "\n",
        "def compute_joint_attention(att_mat, add_residual=True):\n",
        "    if add_residual:\n",
        "        residual_att = np.eye(att_mat.shape[1])[None,...]\n",
        "        aug_att_mat = att_mat + residual_att\n",
        "        aug_att_mat = aug_att_mat / aug_att_mat.sum(axis=-1)[...,None]\n",
        "    else:\n",
        "       aug_att_mat =  att_mat\n",
        "\n",
        "    joint_attentions = np.zeros(aug_att_mat.shape)\n",
        "\n",
        "    layers = joint_attentions.shape[0]\n",
        "    joint_attentions[0] = aug_att_mat[0].numpy()\n",
        "    for i in np.arange(1,layers):\n",
        "        joint_attentions[i] = aug_att_mat[i].numpy().dot(joint_attentions[i-1])\n",
        "\n",
        "    return joint_attentions\n",
        "\n",
        "def plot_attention_heatmap(att, s_position, t_positions, sentence):\n",
        "\n",
        "  cls_att = np.flip(att[:,s_position, t_positions], axis=0)\n",
        "  xticklb = input_tokens= list(itertools.compress(['<cls>']+sentence.split(), [i in t_positions for i in np.arange(len(sentence)+1)]))\n",
        "  yticklb = [str(i) if i%2 ==0 else '' for i in np.arange(att.shape[0],0, -1)]\n",
        "  ax = sns.heatmap(cls_att, xticklabels=xticklb, yticklabels=yticklb, cmap=\"YlOrRd\")\n",
        "  return ax\n",
        "\n",
        "\n",
        "def convert_adjmat_tomats(adjmat, n_layers, l):\n",
        "   mats = np.zeros((n_layers,l,l))\n",
        "\n",
        "   for i in np.arange(n_layers):\n",
        "       mats[i] = adjmat[(i+1)*l:(i+2)*l,i*l:(i+1)*l]\n",
        "\n",
        "   return mats\n",
        "\n",
        "def compute_raw_attention_residual_connections(attentions_mat, input_tokens):\n",
        "    print(\"Get raw attention mat + residual coonections\")\n",
        "    res_att_mat = attentions_mat.sum(axis=1)/attentions_mat.shape[1]\n",
        "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
        "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
        "\n",
        "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
        "    return res_adj_mat, res_labels_to_index, res_att_mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ1dCasBBmrt"
      },
      "outputs": [],
      "source": [
        "#title Get raw attention mat + residual coonections\n",
        "def plot_raw_attention_residual_connections(attentions_mat, input_tokens):\n",
        "    print(\"\\n============ RAW ATTENTION MAP + RESIDUAL CONNECTIONS ============\")\n",
        "    res_adj_mat, res_labels_to_index, res_att_mat = compute_raw_attention_residual_connections(attentions_mat=attentions_mat, input_tokens=input_tokens)\n",
        "\n",
        "    plt.figure(figsize=(20,100))\n",
        "    res_G = draw_attention_graph(res_adj_mat,res_labels_to_index, n_layers=res_att_mat.shape[0], length=res_att_mat.shape[-1])\n",
        "    return res_G\n",
        "\n",
        "def plot_attention_rollout(attentions_mat, input_tokens):\n",
        "    print(\"\\n============ ATTENTION ROLLOUT ============\")\n",
        "    res_adj_mat, res_labels_to_index, res_att_mat = compute_raw_attention_residual_connections(attentions_mat=attentions_mat, input_tokens=input_tokens)\n",
        "    joint_attentions = compute_joint_attention(res_att_mat, add_residual=False)\n",
        "    joint_att_adjmat, joint_labels_to_index = get_adjmat(mat=joint_attentions, input_tokens=input_tokens)\n",
        "    plt.figure(figsize=(20,100))\n",
        "    G = draw_attention_graph(joint_att_adjmat,joint_labels_to_index, n_layers=joint_attentions.shape[0], length=joint_attentions.shape[-1])\n",
        "\n",
        "def plot_attention_flow(attentions_mat, input_tokens, res_G):\n",
        "    print(\"\\n============ ATTENTION FLOW ============\")\n",
        "    res_adj_mat, res_labels_to_index, res_att_mat = compute_raw_attention_residual_connections(attentions_mat=attentions_mat, input_tokens=input_tokens)\n",
        "    print(\"Compute attention flow (this will take quite some time to compute)\")\n",
        "    output_nodes = []\n",
        "    input_nodes = []\n",
        "    for key in tqdm(res_labels_to_index):\n",
        "        if 'L24' in key:\n",
        "            output_nodes.append(key)\n",
        "        if res_labels_to_index[key] < attentions_mat.shape[-1]:\n",
        "            input_nodes.append(key)\n",
        "\n",
        "    flow_values = compute_flows(res_G, res_labels_to_index, input_nodes, length=attentions_mat.shape[-1])\n",
        "\n",
        "    plt.figure(figsize=(20,100))\n",
        "    flow_G = draw_attention_graph(flow_values,res_labels_to_index, n_layers=attentions_mat.shape[0], length=attentions_mat.shape[-1])\n",
        "\n",
        "\n",
        "def attention_flow(attentions, log_fp_test, selected_classes=None, selected_layers=None, selected_heads=None):\n",
        "    # token_base_positions_axis = attentions['token_base_positions_axis']\n",
        "    # attentions_last_layer = attentions['attentions_last_layer']\n",
        "\n",
        "    for target_label in selected_classes:\n",
        "        attentions_mat = attentions['attentions_all_layers'][target_label]\n",
        "        tokens = attentions['repr_token_base_positions_axis'][target_label]\n",
        "        # domain_start_idx, domain_end_idx = convert_domain_coords_in_token_indices(\"NTD\")\n",
        "        domain_end_idx = 12\n",
        "        print(domain_end_idx)\n",
        "        attentions_mat = attentions_mat[:,:,:domain_end_idx,:domain_end_idx]\n",
        "        tokens = tokens[:domain_end_idx]\n",
        "        print(attentions_mat.shape)\n",
        "        res_G = plot_raw_attention_residual_connections(attentions_mat, tokens)\n",
        "        plot_attention_rollout(attentions_mat, tokens)\n",
        "        plot_attention_flow(attentions_mat, tokens, res_G)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKEGLHgewBFs"
      },
      "outputs": [],
      "source": [
        "#@title Analysis\n",
        "if config_dict['TASK_TYPE']=='attention_flow':\n",
        "    with open(log_file, 'a') as log_fp:\n",
        "            selected_classes = [3] #[1]  #range(config_dict['N_CLASSES'])\n",
        "            attention_flow(attentions2, log_fp, selected_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94FeRjDoAgxv"
      },
      "source": [
        "###Entropy of attention matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp_5qAwCA4MB"
      },
      "outputs": [],
      "source": [
        "def entropy_attentions(type_entropy, attentions, selected_classes, selected_layers, selected_heads, file_fp):\n",
        "    attentions_all_layers = attentions['attentions_all_layers']\n",
        "\n",
        "    for target_label in selected_classes:\n",
        "        file_fp.write(f\"{type_entropy} entropy {'of eigenvalues ' if type_entropy=='Von-Neumann' else ''}of attention matrices, class '{inv_class_labels_dict[int(target_label)]}'\\n\")\n",
        "\n",
        "        VN_entropy_layer_attn_sum = np.zeros(n_layers)\n",
        "        Sh_entropy_layer_attn_sum = np.zeros(n_layers)\n",
        "\n",
        "        for layer in selected_layers:\n",
        "\n",
        "            if selected_heads == 'avg':\n",
        "                #calculate the avg of attention matrices of heads of current layer\n",
        "                attn_mat_layer = attentions_all_layers[target_label][layer].mean(dim=0).cpu().detach().numpy()\n",
        "                if type_entropy==\"von_neumann_entropy_attentions\":\n",
        "                    result = compute_Von_Neumann_entropy_eigvals(attn_mat_layer, f\"layer_{layer+1}\")\n",
        "                elif type_entropy==\"shannon_entropy_attentions\":\n",
        "                    result = compute_Shannon_entropy(attn_mat_layer, f\"layer_{layer+1}\")\n",
        "                print(f\"\\t{type_entropy}_layer_{layer+1} = {result}\")\n",
        "                file_fp.write(f\"\\t{type_entropy}_layer_{layer+1} = {result}\\n\")\n",
        "\n",
        "            else:\n",
        "                for head in selected_heads:\n",
        "                    attn_mat_head = attentions_all_layers[target_label][layer][head].cpu().detach().numpy()\n",
        "                    if type_entropy==\"von_neumann_entropy_attentions\":\n",
        "                        result = compute_Von_Neumann_entropy_eigvals(attn_mat_head, f\"head_{head+1}_layer_{layer+1}\")\n",
        "                    elif type_entropy==\"shannon_entropy_attentions\":\n",
        "                        result = compute_Shannon_entropy(attn_mat_head, f\"head_{head+1}_layer_{layer+1}\")\n",
        "                    print(f\"\\t{type_entropy}_entropy_head_{head+1}_layer_{layer+1} = {result}\")\n",
        "                    file_fp.write(f\"\\t{type_entropy}_entropy_head_{head+1}_layer_{layer+1} = {result}\\n\")\n",
        "\n",
        "def plot_entropies(attentions, selected_classes, selected_layers, selected_heads):\n",
        "    attentions_all_layers = attentions['attentions_all_layers']\n",
        "    VN_entropy_layer_attn_sum = np.zeros(n_layers)\n",
        "    Sh_entropy_layer_attn_sum = np.zeros(n_layers)\n",
        "\n",
        "    for target_label in selected_classes:\n",
        "        for layer in selected_layers:\n",
        "            for head in selected_heads:\n",
        "\n",
        "                attn_mat_head = attentions_all_layers[target_label][layer][head].cpu().detach().numpy()\n",
        "\n",
        "                VN_entropy_attn = compute_Von_Neumann_entropy_eigvals(attn_mat_head, f\"head{head+1}_layer_{layer+1}\")\n",
        "                Sh_entropy_attn = compute_Shannon_entropy(attn_mat_head, f\"head{head+1}_layer_{layer+1}\")\n",
        "\n",
        "                VN_entropy_layer_attn_sum[layer] += VN_entropy_attn\n",
        "                Sh_entropy_layer_attn_sum[layer] += Sh_entropy_attn\n",
        "\n",
        "        VN_entropy_layer_attn_mean = VN_entropy_layer_attn_sum / len(selected_heads)\n",
        "        Sh_entropy_layer_attn_mean = Sh_entropy_layer_attn_sum / len(selected_heads)\n",
        "\n",
        "\n",
        "        # x = [i+1 for i in range(n_layers)]\n",
        "        fig, ax = plt.subplots(figsize=(6,6))\n",
        "        ax.plot(Sh_entropy_layer_attn_mean, 'b', label='Shannon entropy')\n",
        "        # ax.axis('equal')\n",
        "        leg = ax.legend()\n",
        "        plt.title(f\"Average Shannon entropy of attention matrices of each layer\")\n",
        "        plt.grid(linestyle = '--')\n",
        "        ax.set_xticks(range(n_layers))\n",
        "        ax.set_xticklabels(np.asarray(ax.get_xticks())+1)\n",
        "        #fig.yticks(list(plt.yticks()[0]) + [1])\n",
        "        ax.set_xlabel(\"Layer\")\n",
        "        ax.set_ylabel(\"Shannon entropy\")\n",
        "        plt.show()\n",
        "        fig_path = Path(math_interpret_dir) / f'Shannon_entropy_plot.jpg'\n",
        "        fig.savefig(fig_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        # x = [i+1 for i in range(n_layers)]\n",
        "        fig, ax = plt.subplots(figsize=(6,6))\n",
        "        ax.plot(VN_entropy_layer_attn_mean, 'b', label='VN entropy')\n",
        "        # ax.axis('equal')\n",
        "        leg = ax.legend()\n",
        "        plt.title(f\"Average Von Neumann entropy of attention matrices of each layer\")\n",
        "        plt.grid(linestyle = '--')\n",
        "        ax.set_xticks(range(n_layers))\n",
        "        ax.set_xticklabels(np.asarray(ax.get_xticks())+1)\n",
        "        #fig.yticks(list(plt.yticks()[0]) + [1])\n",
        "        ax.set_xlabel(\"Layer\")\n",
        "        ax.set_ylabel(\"VN entropy\")\n",
        "        plt.show()\n",
        "        fig_path = Path(math_interpret_dir) / f'VN_entropy_plot.jpg'\n",
        "        fig.savefig(fig_path, bbox_inches='tight')\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znhpo1cL_dxG"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='von_neumann_entropy_attentions' or config_dict['TASK_TYPE']=='shannon_entropy_attentions':\n",
        "    attn_eigvals_file = Path(math_interpret_dir) / f\"{config_dict['TASK_TYPE']}.txt\"\n",
        "    with open(attn_eigvals_file, 'w') as attn_eigvals_fp:\n",
        "\n",
        "        selected_classes = [7]  #attentions_all_layers.keys()\n",
        "        selected_layers = range(n_layers)\n",
        "        # selected_heads = 'avg'\n",
        "        # entropy_attentions(config_dict['TASK_TYPE'], attentions2, selected_classes, selected_layers, selected_heads, attn_eigvals_fp)\n",
        "        selected_heads = range(n_heads)\n",
        "        # entropy_attentions(config_dict['TASK_TYPE'], attentions2, selected_classes, selected_layers, selected_heads, attn_eigvals_fp)\n",
        "\n",
        "    plot_entropies(attentions2, selected_classes, selected_layers, selected_heads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGXhgldabuI-"
      },
      "source": [
        "### One-vs-all classification with best tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waM1Lh4_4XVy"
      },
      "outputs": [],
      "source": [
        "def get_tick_pos(tick_str):\n",
        "    ticks_file = Path(attention_matrices_dir) / 'ticks_np'\n",
        "    ticks = pickle.load(open(ticks_file, 'rb'))\n",
        "    selected_class_ticks = ticks[config_dict['CLASS_LABELS'][config_dict['POSITIVE_CLASS_MLP']]]\n",
        "    try:\n",
        "        idx = [i for i,tick in enumerate(selected_class_ticks) if tick.startswith(tick_str)][0]\n",
        "        return idx\n",
        "    except Exception as e:\n",
        "        print(f'Exception: tick not found. {e}')\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHHuEgxthITH"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self, batch_size, input_dim, output_dim, hidden_units_1, hidden_units_2):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "    #   nn.Flatten(),\n",
        "      nn.Linear(input_dim, hidden_units_1),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_units_1, hidden_units_2),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_units_2, output_dim)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WWQX44MhoOY"
      },
      "outputs": [],
      "source": [
        "def mlp_train(mlp_batch, mlp, optimizer, loss_function, current_loss):\n",
        "    # Get inputs\n",
        "    # mlp_inputs = torch.stack(mlp_batch['input_embeddings'])#.to(device, dtype = torch.long)\n",
        "    mlp_inputs = torch.tensor(mlp_batch['input_embeddings']) #.to(device, dtype = torch.long)\n",
        "    mlp_targets = torch.tensor(mlp_batch['targets'])\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    mlp_outputs = mlp(mlp_inputs)\n",
        "    # Compute loss\n",
        "    loss = loss_function(mlp_outputs, mlp_targets)\n",
        "\n",
        "    # Perform backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    current_loss += loss.item()\n",
        "    if i % 1 == 0:\n",
        "        print('Loss after mini-batch: %.3f' %\n",
        "            (current_loss / 10))\n",
        "        current_loss = 0.0\n",
        "\n",
        "def supervisedOneVsAllMLP_train(train_dataloader, val_dataloader, log_fp_test):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    mlp_step = 0\n",
        "\n",
        "    relevant_tokens = [get_tick_pos(tick) for tick in config_dict['RELEVANT_TOKENS_MLP']]\n",
        "\n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Initialize the MLP\n",
        "    mlp = MLP(batch_size=config_dict['TRAIN_BATCH_SIZE_MLP'],\n",
        "              input_dim=config_dict['INPUT_DIM_MLP'],\n",
        "              output_dim=config_dict['OUTPUT_DIM_MLP'],\n",
        "              hidden_units_1=config_dict['HIDDEN_UNITS_1_MLP'],\n",
        "              hidden_units_2=config_dict['HIDDEN_UNITS_2_MLP'])\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "\n",
        "    # Run the training loop\n",
        "    for epoch in range(0, 5): # 5 epochs at maximum\n",
        "        print(f'Starting epoch {epoch+1}')\n",
        "        current_loss = 0.0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            mlp_batch = {\n",
        "                'seq_ids': [],\n",
        "                'targets': [],\n",
        "                'input_embeddings': []\n",
        "                }\n",
        "            # Unpack the inputs from our dataloader\n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.float)\n",
        "            label_ids = targets.to('cpu').numpy()\n",
        "            seq_ids = batch['seq_ids'].to('cpu').numpy()\n",
        "\n",
        "            # Extract input embeddings\n",
        "            with torch.no_grad():\n",
        "                outputs = model(ids,\n",
        "                            attention_mask=mask,\n",
        "                            return_dict=True,\n",
        "                            output_attentions=False,\n",
        "                            output_hidden_states=True)\n",
        "\n",
        "            for sample_idx in range(0, len(seq_ids)):\n",
        "                # if mlp_step % 1 == 0 and not mlp_step == 0:\n",
        "                #     print('  Batch {:>5}.'.format(mlp_step))\n",
        "                mlp_batch['seq_ids'].append(torch.tensor(seq_ids[sample_idx], dtype=torch.int))\n",
        "                mlp_batch['targets'].append(torch.tensor(label_ids[sample_idx], dtype=torch.long))\n",
        "                flattened_tokens_embeddings = []\n",
        "                for token in relevant_tokens:\n",
        "                    flattened_tokens_embeddings.extend(outputs.hidden_states[0][sample_idx][token])\n",
        "                mlp_batch['input_embeddings'].append(flattened_tokens_embeddings)\n",
        "                # print(np.asarray(mlp_batch).shape)\n",
        "                # print(len(mlp_batch['input_embeddings'][0][0]))\n",
        "                # print(outputs.hidden_states[0][sample_idx][71].shape)\n",
        "                if len(mlp_batch) == config_dict['TRAIN_BATCH_SIZE_MLP']:\n",
        "                    mlp_step += 1\n",
        "                    mlp_train(mlp_batch, mlp, optimizer, loss_function, current_loss)\n",
        "                    mlp_batch['seq_ids'].clear()\n",
        "                    mlp_batch['targets'].clear()\n",
        "                    mlp_batch['input_embeddings'].clear()\n",
        "\n",
        "            if mlp_batch['seq_ids']:\n",
        "                mlp_step += 1\n",
        "                mlp_train(mlp_batch, mlp, optimizer, loss_function, current_loss)\n",
        "                mlp_batch['seq_ids'].clear()\n",
        "                mlp_batch['targets'].clear()\n",
        "                mlp_batch['input_embeddings'].clear()\n",
        "\n",
        "        mlp_step=0\n",
        "\n",
        "    print('DONE.')\n",
        "\n",
        "    return mlp, results\n",
        "\n",
        "\n",
        "def supervisedOneVsAllMLP_test(mlp, test_dataloader, log_fp_test):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('Calculating predictions...')\n",
        "    log_fp_test.write(\"\\nTest:\\n\")\n",
        "    log_fp_test.write(\"=====\\n\")\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    relevant_tokens = [get_tick_pos(tick) for tick in config_dict['RELEVANT_TOKENS_MLP']]\n",
        "\n",
        "    final_data_test = {'seq_ids' : [],\n",
        "                'targets' : [],\n",
        "                'y_preds' : []\n",
        "                }\n",
        "\n",
        "    # Set fixed random number seed\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Run the training loop\n",
        "    for batch in test_dataloader:\n",
        "        # Unpack the inputs from our dataloader\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.float)\n",
        "        label_ids = targets.to('cpu').numpy()\n",
        "        seq_ids = batch['seq_ids'].to('cpu').numpy()\n",
        "\n",
        "        mlp_batch = {\n",
        "            'seq_ids': [],\n",
        "            'targets': [],\n",
        "            'input_embeddings': []\n",
        "            }\n",
        "\n",
        "        # Extract input embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(ids,\n",
        "                        attention_mask=mask,\n",
        "                        return_dict=True,\n",
        "                        output_attentions=False,\n",
        "                        output_hidden_states=True)\n",
        "\n",
        "            for sample_idx in range(0, len(seq_ids)):\n",
        "                # if mlp_step % 5 == 0 and not mlp_step == 0:\n",
        "                #     print('  Batch {:>5}.'.format(mlp_step))\n",
        "                mlp_batch['seq_ids'].append(torch.tensor(seq_ids[sample_idx], dtype=torch.int))\n",
        "                mlp_batch['targets'].append(torch.tensor(label_ids[sample_idx], dtype=torch.long))\n",
        "                flattened_tokens_embeddings = []\n",
        "                for token in relevant_tokens:\n",
        "                    flattened_tokens_embeddings.extend(outputs.hidden_states[0][sample_idx][token])\n",
        "                mlp_batch['input_embeddings'].append(flattened_tokens_embeddings)\n",
        "\n",
        "                if len(mlp_batch) == config_dict['EVAL_BATCH_SIZE_MLP']:\n",
        "                    mlp_inputs = torch.tensor(mlp_batch['input_embeddings']) #.to(device, dtype = torch.long)\n",
        "                    y_pred = mlp(mlp_inputs)\n",
        "                    mlp_batch['seq_ids'].clear()\n",
        "                    mlp_batch['targets'].clear()\n",
        "                    mlp_batch['input_embeddings'].clear()\n",
        "\n",
        "            if mlp_batch['seq_ids']:\n",
        "                mlp_inputs = torch.tensor(mlp_batch['input_embeddings']) #.to(device, dtype = torch.long)\n",
        "                y_pred = mlp(mlp_inputs)\n",
        "                mlp_batch['seq_ids'].clear()\n",
        "                mlp_batch['targets'].clear()\n",
        "                mlp_batch['input_embeddings'].clear()\n",
        "\n",
        "        final_data_test['seq_ids'].extend(seq_ids)\n",
        "        final_data_test['targets'].extend(label_ids)\n",
        "        final_data_test['y_preds'].extend(y_pred)\n",
        "\n",
        "    print('DONE.')\n",
        "    return final_data_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewyknr2c1-UV"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqThmtt3YPLV"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='one_vs_all_classification':\n",
        "    selected_class = config_dict['POSITIVE_CLASS_MLP']\n",
        "    relevant_tokens_pos = config_dict['RELEVANT_TOKENS_MLP']\n",
        "\n",
        "    with open(train_file) as train_fp, open(val_file) as val_fp, open(log_file, 'a') as log_fp:\n",
        "\n",
        "        train_reader = csv.reader(train_fp, delimiter=',')\n",
        "        train_metadata = {'len':sizes_info['train_data_size']}\n",
        "        X_train_generator = DatasetGenerator_InputEmbeddings(train_fp, train_metadata)\n",
        "        train_dataloader = DataLoader(\n",
        "                X_train_generator,  # The training samples.\n",
        "                sampler = RandomSampler(X_train_generator), # Select batches randomly\n",
        "                batch_size = config_dict['TRAIN_BATCH_SIZE_MLP'], # Trains with this batch size.\n",
        "                num_workers = 0\n",
        "        )\n",
        "\n",
        "        val_reader = csv.reader(val_fp, delimiter=',')\n",
        "        val_metadata = {'len':sizes_info['val_data_size']}\n",
        "        X_val_generator = DatasetGenerator_InputEmbeddings(val_fp, val_metadata) #.batch(EVAL_BATCH_SIZE)\n",
        "\n",
        "        validation_dataloader = DataLoader(\n",
        "                X_val_generator, # The validation samples.\n",
        "                sampler = SequentialSampler(X_val_generator), # Pull out batches sequentially.\n",
        "                batch_size = config_dict['EVAL_BATCH_SIZE_MLP'], # Evaluate with this batch size.\n",
        "                num_workers = 0\n",
        "        )\n",
        "\n",
        "        mlp, results = supervisedOneVsAllMLP_train(train_dataloader, validation_dataloader, log_fp)\n",
        "\n",
        "        # final_statistics(target_labels, output_labels, log_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFiF7Wvb2BKX"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U692xWGiwVih"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='one_vs_all_classification':\n",
        "    try:\n",
        "        test_data_size = sizes_info['test_data_size_seqs']-1\n",
        "    except NameError:\n",
        "        test_data_size = sum(1 for line in open(test_file))\n",
        "\n",
        "    n_log_files = len(os.listdir(log_dir_bio))\n",
        "    if n_log_files > 0:\n",
        "        log_file_mlp = Path(log_dir_bio) / f'log_mlp({n_log_files}).txt'\n",
        "    else:\n",
        "        log_file_mlp = Path(log_dir_bio) / f'log_mlp.txt'\n",
        "\n",
        "    with open(test_file) as test_fp, open(log_file_mlp, 'a') as log_fp:\n",
        "        test_reader = csv.reader(test_fp, delimiter=',')\n",
        "        test_metadata = {'len':test_data_size}\n",
        "        X_test_generator = DatasetGenerator_InputEmbeddings(test_fp, test_metadata)\n",
        "        test_dataloader = DataLoader(\n",
        "                X_test_generator, # The validation samples.\n",
        "                sampler = SequentialSampler(X_test_generator), # Pull out batches sequentially.\n",
        "                batch_size = config_dict['EVAL_BATCH_SIZE'], # Evaluate with this batch size.\n",
        "                num_workers = 0\n",
        "        )\n",
        "\n",
        "        if config_dict['TASK_TYPE']=='eigenvalues_analysis':\n",
        "            selected_layer_head_list = [[1-1,4-1], [12-1,9-1], [11-1, 5-1]]\n",
        "        else:\n",
        "            selected_layer_head_list = None\n",
        "\n",
        "        final_data_test = supervisedOneVsAllMLP_test(\n",
        "            mlp,\n",
        "            test_dataloader,\n",
        "            log_fp\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyDkDp-tHs9u"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='one_vs_all_classification':\n",
        "    with open(log_file_mlp, 'a') as log_fp:\n",
        "        seq_ids = torch.tensor(final_data_test['seq_ids']).to('cpu')\n",
        "        targets = torch.tensor(final_data_test['targets']).to('cpu').numpy()\n",
        "        y_preds = [np.argmax(prediction.numpy()) for prediction in final_data_test['y_preds']]\n",
        "\n",
        "        print(targets[:10])\n",
        "        print(np.unique(np.asarray(targets[:])))\n",
        "        target_names={'mu':1, 'other':0}\n",
        "\n",
        "    final_statistics(targets, y_preds, log_file_mlp, 'Test', target_names=target_names) #, logits=final_data_test['logits'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Q4i_e995VF"
      },
      "outputs": [],
      "source": [
        "# plot_color_gradients(colormaps, {v:k for k, v in config_dict['CLASS_LABELS'].items()}, 'Attention scores colormaps for each class')\n",
        "# plot_color_gradients(colormaps_layers, range(n_layers), 'Attention scores colormaps for each layer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqZaYmmRrymg"
      },
      "source": [
        "### Analysis of eigenvalues of weights of Qi and Ki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UZhG42H6UMD"
      },
      "outputs": [],
      "source": [
        "# model.parameters\n",
        "print(f'{\"parameter\": <55}: {\"mean\": <20} (std)')\n",
        "print()\n",
        "for name,param in model.named_parameters():\n",
        "    if name.endswith(\"attention.output.LayerNorm.weight\"):\n",
        "        print(f'{name: <55}: {torch.mean(param): <20} ({torch.std(param)})')\n",
        "print()\n",
        "for name,param in model.named_parameters():\n",
        "    if name.endswith(\"attention.output.LayerNorm.bias\"):\n",
        "        print(f'{name: <55}: {torch.mean(param): <20} ({torch.std(param)})')\n",
        "print()\n",
        "for name,param in model.named_parameters():\n",
        "    if name.endswith(\"output.LayerNorm.weight\") and not name.endswith(\"attention.output.LayerNorm.weight\"):\n",
        "        print(f'{name: <55}: {torch.mean(param): <20} ({torch.std(param)})')\n",
        "print()\n",
        "for name,param in model.named_parameters():\n",
        "    if name.endswith(\"output.LayerNorm.bias\") and not name.endswith(\"attention.output.LayerNorm.bias\"):\n",
        "        print(f'{name: <55}: {torch.mean(param): <20} ({torch.std(param)})')\n",
        "# print(model.parameters)\n",
        "\n",
        "# model_params = dict(model.named_parameters())\n",
        "# model_params[\"encoder.layer.0.attention.output.LayerNorm.bias\"].data.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3qGrSgNb6_n"
      },
      "outputs": [],
      "source": [
        "def compute_head_weights_eigenvalues(model, dir_path):\n",
        "    layer_query_weight_names = [f'bert.encoder.layer.{l}.attention.self.query.weight' for l in range(n_layers)]\n",
        "    layer_key_weight_names = [f'bert.encoder.layer.{l}.attention.self.key.weight' for l in range(n_layers)]\n",
        "    model_params = dict(model.named_parameters())\n",
        "    for layer in np.flip(range(n_layers)):\n",
        "        w_q = model_params[layer_query_weight_names[layer]].data\n",
        "        w_k = model_params[layer_key_weight_names[layer]].data\n",
        "        emb_dim = w_q.shape[0]\n",
        "        d_k = int(emb_dim / n_heads)\n",
        "\n",
        "        weights_prod = np.matmul(w_q, w_k.T)\n",
        "\n",
        "        eigenvals = np.linalg.eigvals(weights_prod)\n",
        "        print(eigenvals)\n",
        "        return\n",
        "        eigenvals_count = {}\n",
        "        eigenvals_count['positive'] = sum([1 for x in eigenvals if x>0])\n",
        "        eigenvals_count['negative'] = sum([1 for x in eigenvals if x<0])\n",
        "        eigenvals_count['zero'] = sum([1 for x in eigenvals if x==0])\n",
        "\n",
        "        #eigenvals_count = collections.Counter(eigenvals)\n",
        "        print(f\"n. unique eigenvalues = {len(np.unique(eigenvals))}\")\n",
        "        fig=plt.figure(figsize=(8,8))\n",
        "        # print(np.sort_complex(eigenvals)[:10])\n",
        "        plt.bar(range(len(eigenvals_count.keys())), eigenvals_count.values(), align='center', width=0.8)\n",
        "        plt.xticks(range(len(eigenvals_count.keys())), eigenvals_count.keys(), rotation=45, fontsize=12)\n",
        "        plt.title(f\"Eigenvalues for layer {layer+1}\")\n",
        "        plt.show()\n",
        "        fig_path = Path(dir_path) / f'{layer+1}.jpg'\n",
        "        fig.savefig(fig_path, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "        # for i in range(n_heads):\n",
        "        #     w_q_i = w_q[:, i*d_k : i*d_k + d_k]\n",
        "        #     w_k_i = w_k[:, i*d_k : i*d_k + d_k]\n",
        "        #     weights_prod = np.matmul(w_q_i, w_k_i.T)\n",
        "        #     eigenvals = np.linalg.eigvals(weights_prod)\n",
        "        #     eigenvals_count = {}\n",
        "        #     eigenvals_count['positive'] = sum([1 for x in eigenvals if x>0])\n",
        "        #     eigenvals_count['negative'] = sum([1 for x in eigenvals if x<0])\n",
        "        #     eigenvals_count['zero'] = sum([1 for x in eigenvals if x==0])\n",
        "\n",
        "        #     #eigenvals_count = collections.Counter(eigenvals)\n",
        "        #     print(f\"n. unique eigenvalues = {len(np.unique(eigenvals))}\")\n",
        "        #     plt.figure(figsize=(8,8))\n",
        "        #     # print(np.sort_complex(eigenvals)[:10])\n",
        "        #     plt.bar(range(len(eigenvals_count.keys())), eigenvals_count.values(), align='center', width=0.8)\n",
        "        #     plt.xticks(range(len(eigenvals_count.keys())), eigenvals_count.keys(), rotation=45, fontsize=12)\n",
        "        #     plt.title(f\"Eigenvalues for head {i+1} layer {layer+1}\")\n",
        "        #     plt.show()\n",
        "        #     fig_path = Path(dir_path) / f'{layer+1}_{i+1}.jpg'\n",
        "        #     fig.savefig(fig_path, bbox_inches='tight', pad_inches=0)\n",
        "        #     plt.close()\n",
        "\n",
        "def compute_max_bias(model):\n",
        "    layer_query_weight_names = [f'bert.encoder.layer.{l}.attention.self.query.bias' for l in range(n_layers)]\n",
        "    layer_key_weight_names = [f'bert.encoder.layer.{l}.attention.self.key.bias' for l in range(n_layers)]\n",
        "    model_params = dict(model.named_parameters())\n",
        "    for layer in np.flip(range(n_layers)):\n",
        "            bias_q = model_params[layer_query_weight_names[layer]].data\n",
        "            bias_k = model_params[layer_key_weight_names[layer]].data\n",
        "            print(f\"layer {layer}\")\n",
        "            print(f\"\\tbias_q={len(bias_q)}\")\n",
        "            print(f\"\\tbias_k={len(bias_k)}\")\n",
        "\n",
        "def hist_Wq_WkT(model):\n",
        "    layer_query_weight_names = [f'bert.encoder.layer.{l}.attention.self.query.weight' for l in range(n_layers)]\n",
        "    layer_key_weight_names = [f'bert.encoder.layer.{l}.attention.self.key.weight' for l in range(n_layers)]\n",
        "    model_params = dict(model.named_parameters())\n",
        "\n",
        "    for layer in np.flip(range(n_layers)):\n",
        "        W_q = model_params[layer_query_weight_names[layer]].data.cpu().detach().numpy()\n",
        "        W_k = model_params[layer_key_weight_names[layer]].data.cpu().detach().numpy()\n",
        "        emb_dim = W_q.shape[0]\n",
        "        d_k = int(emb_dim / n_heads)\n",
        "        head = 0\n",
        "        W_q_i = W_q[:, head*d_k : head*d_k + d_k]\n",
        "        W_k_i = W_k[:, head*d_k : head*d_k + d_k]\n",
        "        weights_prod = np.matmul(W_q_i, W_k_i.T)\n",
        "        fig, ax = plt.subplots(figsize=(15,10))\n",
        "        ax.set_aspect('equal')\n",
        "        sns.heatmap(weights_prod)\n",
        "        plt.show()\n",
        "\n",
        "        heat_dir = Path(math_interpret_dir) / \"heatmaps_Wq_WkT\"\n",
        "        if not os.path.exists(heat_dir):\n",
        "            os.makedirs(heat_dir)\n",
        "            print(f\"Directory '{heat_dir}' created\")\n",
        "        fig_path = Path(heat_dir) / f'heatmap_Wq_WkT_L{layer+1}_H1.jpg'\n",
        "        fig.savefig(fig_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W9-tEuKrw1E"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE'] == 'eigenvalues_analysis':\n",
        "    head_weights_eigenvalues = compute_head_weights_eigenvalues(model, math_interpret_dir)\n",
        "    compute_max_bias(model)\n",
        "    hist_Wq_WkT(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJDruhsIhqgn"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aczb1X0mjTy1"
      },
      "outputs": [],
      "source": [
        "def plot_PCA_explained_variance(pca, filepath=None):\n",
        "    fig = plt.figure()\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_), label=\"Cumulative\")\n",
        "    plt.plot(pca.explained_variance_ratio_, label=\"Normal\")\n",
        "    plt.legend()\n",
        "    plt.xlabel('Number of components')\n",
        "    plt.ylabel('Explained variance')\n",
        "    plt.show()\n",
        "    if filepath is not None:\n",
        "        fig.savefig(filepath)\n",
        "\n",
        "def plot_Pareto(pca, title=None, filepath=None):\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_figheight(8)\n",
        "    fig.set_figwidth(12)\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "    plt.title(f\"Pareto chart - {title}\")\n",
        "    ax.set_xticks(range(len(pca.explained_variance_ratio_)))\n",
        "    ax.set_xticklabels(range(1,len(pca.explained_variance_ratio_)+1))\n",
        "    ax.set_xlabel(\"Principal component\")\n",
        "    ax.set_ylabel(\"Explaned variance\")\n",
        "    ax.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_, color=\"C0\")\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(range(len(pca.explained_variance_ratio_)), np.cumsum(pca.explained_variance_ratio_)*100, color=\"C1\", marker=\"D\", ms=10)\n",
        "    ax2.yaxis.set_major_formatter(PercentFormatter())\n",
        "\n",
        "    ax.tick_params(axis=\"y\", colors=\"C0\")\n",
        "    ax2.tick_params(axis=\"y\", colors=\"C1\")\n",
        "    plt.show()\n",
        "    if filepath is not None:\n",
        "        fig.savefig(Path(filepath)/'pareto.jpg')\n",
        "\n",
        "def plot_2D_PCA(X_pca, y, cdict, filepath=None):\n",
        "    Xax = X_pca[:,0]\n",
        "    Yax = X_pca[:,1]\n",
        "\n",
        "    labl = inv_class_labels_dict\n",
        "\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    fig.patch.set_facecolor('white')\n",
        "    for l in np.unique(y):\n",
        "        ix=np.where(y==l)\n",
        "        ax.scatter(Xax[ix], Yax[ix], c=cdict[l], s=40,\n",
        "                label=labl[l], alpha=0.7)\n",
        "\n",
        "    ax.set_xlabel(\"PC1\", fontsize=14)\n",
        "    ax.set_ylabel(\"PC2\", fontsize=14)\n",
        "\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    if filepath is not None:\n",
        "        fig.savefig(filepath)\n",
        "\n",
        "def plot_3D_PCA(pca, X_pca, y, cdict, title=None):\n",
        "    y_labels = [inv_class_labels_dict[i] for i in y]\n",
        "    total_var = pca.explained_variance_ratio_.sum() * 100\n",
        "\n",
        "    fig = px.scatter_3d(\n",
        "        X_pca, x=0, y=1, z=2, color=y_labels,\n",
        "        title=f'PCA - {title} - Total Explained Variance: {total_var:.2f}%',\n",
        "        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n",
        "        width=1000, height=800,\n",
        "    )\n",
        "    fig.update_traces(marker_size=5)\n",
        "    fig.show()\n",
        "\n",
        "# def plot_3D_PCA(X_pca, y, cdict, filepath=None):\n",
        "#     Xax = X_pca[:,0]\n",
        "#     Yax = X_pca[:,1]\n",
        "#     Zax = X_pca[:,2]\n",
        "\n",
        "#     labl = inv_class_labels_dict\n",
        "\n",
        "#     fig = plt.figure(figsize=(10,10))\n",
        "#     ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "#     fig.patch.set_facecolor('white')\n",
        "#     for l in np.unique(y):\n",
        "#         ix=np.where(y==l)\n",
        "#         ax.scatter(Xax[ix], Yax[ix], Zax[ix], c=cdict[l], s=40,\n",
        "#                 label=labl[l], alpha=0.7)\n",
        "#     # for loop ends\n",
        "#     ax.set_xlabel(\"PC1\", fontsize=14)\n",
        "#     ax.set_ylabel(\"PC2\", fontsize=14)\n",
        "#     ax.set_zlabel(\"PC3\", fontsize=14)\n",
        "\n",
        "#     ax.legend()\n",
        "#     plt.show()\n",
        "#     if filepath is not None:\n",
        "#         fig.savefig(filepath)\n",
        "\n",
        "def plot_3D_TSNE(tsne_results, y, cdict, title=None):\n",
        "    y_labels = [inv_class_labels_dict[i] for i in y]\n",
        "    fig = px.scatter_3d(\n",
        "        tsne_results, x=0, y=1, z=2, color=y_labels,\n",
        "        labels=inv_class_labels_dict,\n",
        "        width=1000, height=800,\n",
        "        title=f\"T-SNE - {title}\",\n",
        "    )\n",
        "    fig.update_traces(marker_size=5)\n",
        "    fig.show()\n",
        "\n",
        "def visualize(X, y, selected_classes, title, filepath):\n",
        "    if not os.path.exists(filepath):\n",
        "        os.makedirs(filepath)\n",
        "        print(f\"Directory '{filepath}' created\")\n",
        "\n",
        "    X_df = pd.DataFrame(list(zip(X, y)),\n",
        "               columns =['X', 'y'])\n",
        "    X_filtered = X_df[X_df['y'].isin(selected_classes)]\n",
        "    n_components_tot = min(np.asarray(X).shape[1], len(selected_classes))\n",
        "    n_components = min(3, np.asarray(X).shape[1], len(selected_classes))\n",
        "\n",
        "    X = X_filtered['X'].to_list()\n",
        "    y = X_filtered['y'].to_list()\n",
        "    cdict = {i:c for i, c in enumerate(list(TABLEAU_COLORS.keys()))}\n",
        "    # cdict_tsne = [c for i, c in enumerate(cdict) if i in selected_classes]\n",
        "\n",
        "    pca = PCA(n_components=n_components_tot)\n",
        "    pca.fit(X)\n",
        "    # plot_PCA_explained_variance(pca, filepath)\n",
        "    plot_Pareto(pca, title, filepath)\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    if n_components > 2:\n",
        "        plot_3D_PCA(pca, X_pca, y, cdict, title)\n",
        "    else:\n",
        "        plot_2D_PCA(X_pca, y, cdict, filepath)\n",
        "\n",
        "    tsne = TSNE(n_components=3, verbose=0, perplexity=40, n_iter=300, random_state=42)\n",
        "    tsne_results = tsne.fit_transform(X)\n",
        "\n",
        "    plot_3D_TSNE(tsne_results, y, cdict, title)\n",
        "\n",
        "def cluster(method, X, y, filepath):\n",
        "    y_clusters = method.fit_predict(X)\n",
        "    selected_classes = list(config_dict['CLASS_LABELS'].values())\n",
        "    # visualize(X_softmax, y_clusters, selected_classes, filepath)\n",
        "    print(f'\\nHomogeneity = {metrics.homogeneity_score(y, y_clusters)}')\n",
        "    print(f'Completeness = {metrics.completeness_score(y, y_clusters)}')\n",
        "    print(f'Adjusted Rand Index = {metrics.adjusted_rand_score(y, y_clusters)}')\n",
        "    return method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d79rxMAWfVv3"
      },
      "outputs": [],
      "source": [
        "# dict(model.named_parameters()).keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DCiLeJ7nBdp"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='clustering':\n",
        "    y = np.argmax(final_data_test['targets'],axis=1)\n",
        "    selected_classes = list(config_dict['CLASS_LABELS'].values())\n",
        "    # del final_data_test['positions']\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlgTWOuGeexw"
      },
      "source": [
        "####Input embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-eYdXsGc-K4"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='clustering':\n",
        "    # print(np.asarray(final_data_test[\"input_embeddings\"]).shape)\n",
        "    X_input = np.asarray(final_data_test[\"input_embeddings\"])[:,1:]\n",
        "    X_input = X_input.reshape((X_input.shape[0],X_input.shape[1]*X_input.shape[2]))\n",
        "\n",
        "    filepath = Path(clustering_dir) / 'input_embeddings'\n",
        "\n",
        "    # visualize(X_input, y, selected_classes, \"Input embeddings\", filepath)\n",
        "\n",
        "    dbscan1 = DBSCAN(eps=325, n_jobs=-1)\n",
        "    dbscan = cluster(dbscan1, X_input, y, filepath)\n",
        "    print(set(dbscan.labels_))\n",
        "\n",
        "    del X_input\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW9prc9eRppX"
      },
      "source": [
        "####Encoder embedding outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XHdo4PMRpOb"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='clustering':\n",
        "\n",
        "    X_emb = np.asarray(final_data_test[\"output_embeddings\"])[:, 1:] # dim: n_training_samples x d_model = 1439 x 512 x 768\n",
        "    print(X_emb.shape)\n",
        "    X_emb = X_emb.reshape((X_emb.shape[0],X_emb.shape[1]*X_emb.shape[2]))\n",
        "    print(X_emb.shape)\n",
        "\n",
        "    filepath = Path(clustering_dir) / 'output_embeddings'\n",
        "    selected_classes = list(config_dict['CLASS_LABELS'].values())\n",
        "\n",
        "    # visualize(X_emb, y, selected_classes, \"Encoder embedding outputs\", filepath)\n",
        "\n",
        "    dbscan1 = DBSCAN(eps=325, n_jobs=-1)\n",
        "    dbscan = cluster(dbscan1, X_emb, y, filepath)\n",
        "    print(set(dbscan.labels_))\n",
        "\n",
        "    del X_emb\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp1Z9fSxetOe"
      },
      "source": [
        "#### CLS embedding outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv58nTdoQ_k-"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='clustering':\n",
        "# experiments on encoder output\n",
        "    X_CLS_emb = np.asarray(final_data_test[\"output_embeddings\"])[:, 0] # dim: n_training_samples x d_model = 1439 x 512 x 768\n",
        "    print(X_CLS_emb.shape)\n",
        "    # X_CLS_emb = X_CLS_emb.reshape((X_CLS_emb.shape[0],X_CLS_emb.shape[1]*X_CLS_emb.shape[2]))\n",
        "    # print(X_CLS_emb.shape)\n",
        "\n",
        "    filepath = Path(clustering_dir) / 'output_embedding_CLS'\n",
        "\n",
        "    visualize(X_CLS_emb, y, selected_classes, \"CLS embedding output\", filepath)\n",
        "\n",
        "    # dbscan1 = DBSCAN(eps=10, n_jobs=-1)\n",
        "    # dbscan = cluster(dbscan1, X_CLS_emb, y, filepath)\n",
        "    # print(set(dbscan.labels_))\n",
        "\n",
        "    del X_CLS_emb\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDbGb_BUe8hI"
      },
      "source": [
        "#### Pre-softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBlfnpHTBmTO"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='clustering':\n",
        "# experiments on encoder output\n",
        "    X_pre_softmax = np.asarray(final_data_test[\"logits\"]) # dim: n_training_samples x d_model = 1439 x 512 x 768\n",
        "    print(X_pre_softmax.shape)\n",
        "\n",
        "    filepath = Path(clustering_dir) / 'pre_softmax'\n",
        "\n",
        "    visualize(X_pre_softmax, y, selected_classes, \"Pre-Softmax\", filepath)\n",
        "\n",
        "    # dbscan1 = DBSCAN(eps=5, n_jobs=-1)\n",
        "    # dbscan = cluster(dbscan1, X_pre_softmax, y, filepath)\n",
        "    # print(set(dbscan.labels_))\n",
        "\n",
        "    del X_pre_softmax\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wahcvotoekVB"
      },
      "source": [
        "####Post-softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJYlgo1yj3zK"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='clustering':\n",
        "\n",
        "    # experiments on softmax output\n",
        "    X_softmax = np.asarray(final_data_test[\"outputs\"]) # dim: n_training_samples x n_classes = 1439 x 8\n",
        "    print(X_softmax.shape)\n",
        "    filepath = Path(clustering_dir) / 'post_softmax'\n",
        "\n",
        "    visualize(X_softmax, y, selected_classes, \"Post-Softmax\", filepath)\n",
        "\n",
        "    # dbscan1 = DBSCAN(eps=0.35, n_jobs=-1)\n",
        "    # dbscan = cluster(dbscan1, X_softmax, y, filepath)\n",
        "    # print(set(dbscan.labels_))\n",
        "\n",
        "    del X_softmax\n",
        "    gc.collect()\n",
        "\n",
        "    # selected_classes = [config_dict['CLASS_LABELS']['beta'], config_dict['CLASS_LABELS']['mu'], config_dict['CLASS_LABELS']['gh']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7khGgwCVY_M5"
      },
      "source": [
        "#### CLS layers embedding outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFVz6VtrY7oQ"
      },
      "outputs": [],
      "source": [
        "if config_dict['TASK_TYPE']=='clustering':\n",
        "    sel_layers = [1,3,5,7,9,11]\n",
        "    # sel_layers = [11]\n",
        "    X_CLS_l_emb = []\n",
        "\n",
        "    for l in sel_layers:\n",
        "        X_CLS_l_emb = np.loadtxt(open(Path(CLS_embeddings_dir) / f\"CLS_Y_output_L{l}.csv\"), delimiter=\",\")\n",
        "        print(X_CLS_l_emb.shape)\n",
        "        filepath = Path(clustering_dir) / f'CLS_L{l}'\n",
        "\n",
        "        visualize(X_CLS_l_emb, y, selected_classes, f\"L{l}_embeddings_CLS\", filepath)\n",
        "\n",
        "        # dbscan1 = DBSCAN(eps=13, n_jobs=-1)\n",
        "        # dbscan = cluster(dbscan1, X_CLS_l_emb, y, filepath)\n",
        "        # print(set(dbscan.labels_))\n",
        "\n",
        "        del X_CLS_l_emb\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEfnGZcmJPyB"
      },
      "outputs": [],
      "source": [
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('...')\n",
        "a=[1,2,3,4,5,6,7,8,9,10,11]\n",
        "b = [11,10,9,8,7,6,5,4,3,2,1]\n",
        "c = sorted(b)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "WAmTQYi_MgDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrfUZVaoMf84"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62528069eafb4a3ebab007c2336f191c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5af04a567ce46a4bbb1221859e02606",
              "IPY_MODEL_d1501459dc9447b6bde94a8787f8d2b4",
              "IPY_MODEL_9404ca4525914ab1ba1e8d3fbcf38fdf"
            ],
            "layout": "IPY_MODEL_3763841c7c7a4423abce8d6f9422859d"
          }
        },
        "a5af04a567ce46a4bbb1221859e02606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bad7f9042234a46be8d1bf4a6a48ce9",
            "placeholder": "​",
            "style": "IPY_MODEL_ded6037026014b129d66794be49215ec",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "d1501459dc9447b6bde94a8787f8d2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa2952c078c946bda7c777c71832b81a",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_374d63e815da463693d682480d7330a9",
            "value": 213450
          }
        },
        "9404ca4525914ab1ba1e8d3fbcf38fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9655efd73eb4075b3d022541f27c752",
            "placeholder": "​",
            "style": "IPY_MODEL_3a3c227272ef4f17add45b22fd5c150f",
            "value": " 213k/213k [00:00&lt;00:00, 3.68MB/s]"
          }
        },
        "3763841c7c7a4423abce8d6f9422859d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bad7f9042234a46be8d1bf4a6a48ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded6037026014b129d66794be49215ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa2952c078c946bda7c777c71832b81a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374d63e815da463693d682480d7330a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9655efd73eb4075b3d022541f27c752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3c227272ef4f17add45b22fd5c150f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87beeb9bb46849dc84c96d437fe01e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90949c703fd145d289ab9a2a65cb2e38",
              "IPY_MODEL_abc575db4a6c4a059fe049ac6b2f0068",
              "IPY_MODEL_70cba967949343ca8212fb7871bc00e7"
            ],
            "layout": "IPY_MODEL_0314e07920094c5b9d54d20f06465c68"
          }
        },
        "90949c703fd145d289ab9a2a65cb2e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba71b3f2eaea49a698ad2da3fda8bbba",
            "placeholder": "​",
            "style": "IPY_MODEL_c9327eb469a6438b8762b43cefcfdf6e",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "abc575db4a6c4a059fe049ac6b2f0068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3d8c9ab43e6426c970c4b4481eb0fb1",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b35be6fa41b649e28fc896c9a780623c",
            "value": 29
          }
        },
        "70cba967949343ca8212fb7871bc00e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_358ce13f267f41669ff59754652c497a",
            "placeholder": "​",
            "style": "IPY_MODEL_4e7baa4e005541b587c80e51fced9871",
            "value": " 29.0/29.0 [00:00&lt;00:00, 2.28kB/s]"
          }
        },
        "0314e07920094c5b9d54d20f06465c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba71b3f2eaea49a698ad2da3fda8bbba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9327eb469a6438b8762b43cefcfdf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3d8c9ab43e6426c970c4b4481eb0fb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b35be6fa41b649e28fc896c9a780623c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "358ce13f267f41669ff59754652c497a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7baa4e005541b587c80e51fced9871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aefdf3e259984e67ba533399c025a82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75fa909d80134cedae0650bb30e67343",
              "IPY_MODEL_348397f18a924890bd5c46ea096eac71",
              "IPY_MODEL_f3f1773b9cf0452a984ca05f105aaefe"
            ],
            "layout": "IPY_MODEL_be9e04b9f433436eabd83065b022686f"
          }
        },
        "75fa909d80134cedae0650bb30e67343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4250f994d6704ba5b1cce670910213fb",
            "placeholder": "​",
            "style": "IPY_MODEL_a32e4f9f34094459922dce0f77c402c3",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "348397f18a924890bd5c46ea096eac71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7562f30ffbe4e08b7ed71f9197ddc40",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2082d8728c61488989bef409df7623e1",
            "value": 570
          }
        },
        "f3f1773b9cf0452a984ca05f105aaefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b6bef13ca54977ae7c536fcb227e98",
            "placeholder": "​",
            "style": "IPY_MODEL_9f2ec9ef728248afb04ee78178bf6d96",
            "value": " 570/570 [00:00&lt;00:00, 43.7kB/s]"
          }
        },
        "be9e04b9f433436eabd83065b022686f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4250f994d6704ba5b1cce670910213fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32e4f9f34094459922dce0f77c402c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7562f30ffbe4e08b7ed71f9197ddc40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2082d8728c61488989bef409df7623e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33b6bef13ca54977ae7c536fcb227e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f2ec9ef728248afb04ee78178bf6d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b79625aa919c4911a8dbd8031a610f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0f1a0c2b87f4ea0b85077352630c402",
              "IPY_MODEL_fda825da18154a6b8ed86d3efab8b7c0",
              "IPY_MODEL_3a4c089868b6434b9d4c60ab134a1d28"
            ],
            "layout": "IPY_MODEL_64854179264644a2a7fc7b174913f228"
          }
        },
        "e0f1a0c2b87f4ea0b85077352630c402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4c1feb958b3405481cb99dbe6fa8011",
            "placeholder": "​",
            "style": "IPY_MODEL_ca40ac2376034c29a080138817dfcbe6",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "fda825da18154a6b8ed86d3efab8b7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a83268fc86304dc7baa5766f386543cc",
            "max": 435755784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dab372e1788448d9726988d1274c7c6",
            "value": 435755784
          }
        },
        "3a4c089868b6434b9d4c60ab134a1d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e42654f417974163855a8d93ed78fe15",
            "placeholder": "​",
            "style": "IPY_MODEL_9e4ed3604134497982de13511e26d04b",
            "value": " 436M/436M [00:03&lt;00:00, 127MB/s]"
          }
        },
        "64854179264644a2a7fc7b174913f228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c1feb958b3405481cb99dbe6fa8011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca40ac2376034c29a080138817dfcbe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a83268fc86304dc7baa5766f386543cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dab372e1788448d9726988d1274c7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e42654f417974163855a8d93ed78fe15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e4ed3604134497982de13511e26d04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}